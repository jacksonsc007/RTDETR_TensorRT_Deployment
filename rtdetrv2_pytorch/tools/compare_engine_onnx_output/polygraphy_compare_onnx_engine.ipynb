{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3ddf97",
   "metadata": {},
   "source": [
    "# Logger Severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "474a08ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Accessing the `severity` property of G_LOGGER is deprecated and will be removed in v0.50.0. Use `module_severity` instead\n",
      "Warning: Accessing the `severity` property of G_LOGGER is deprecated and will be removed in v0.50.0. Use `module_severity` instead\n"
     ]
    }
   ],
   "source": [
    "from polygraphy.logger import G_LOGGER\n",
    "\n",
    "# Set verbosity level (choose one):\n",
    "G_LOGGER.severity = G_LOGGER.VERBOSE  # Basic verbose output\n",
    "G_LOGGER.severity = G_LOGGER.EXTRA_VERBOSE  # More detailed output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e91d1",
   "metadata": {},
   "source": [
    "# Construct input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c38d108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading image 0 : /root/workspace/coco_calib/COCO_train2014_000000556709.jpg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Copyright(c) 2023 lyuwenyu. All Rights Reserved.\n",
    "\"\"\"\n",
    "\n",
    "import os \n",
    "import sys\n",
    "\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.utils.data.dataloader \n",
    "# sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "# from src.core import YAMLConfig\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CalibrationDateset(Dataset):\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_names = [f for f in os.listdir(self.img_dir) if (os.path.isfile(\n",
    "            os.path.join(self.img_dir, f)) and (f.endswith('jpg')))\n",
    "        ]\n",
    "    \n",
    "    def __len__(self, ):\n",
    "        return len(self.img_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
    "        # We manually pick one with objects\n",
    "        img_path = \"/root/workspace/coco_calib/COCO_train2014_000000556709.jpg\"\n",
    "        print(\"[INFO] Loading image %d : %s\" % (idx, img_path))\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        width, height = img.width, img.height\n",
    "        # img = read_image(img_path)\n",
    "        img = self.transform(img)\n",
    "        # get width and height of the image\n",
    "        size = (width, height)\n",
    "        size = np.array(size)\n",
    "        size = size[np.newaxis, :]\n",
    "        return img, size\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "pre_transforms = T.Compose([\n",
    "    T.Resize(( 640,640 )),\n",
    "    T.ToTensor()\n",
    "])\n",
    "cali_set = \"/root/workspace/coco_calib\"\n",
    "cali_dataset = CalibrationDateset(cali_set, transform=pre_transforms)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    cali_dataset, batch_size=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iterator = iter(dataloader)\n",
    "\n",
    "# size = torch.tensor([[640, 640]]).numpy()\n",
    "    \n",
    "    \n",
    "def load_data():\n",
    "    for _ in range(1):\n",
    "        image, size = next(data_iterator)\n",
    "        yield {\n",
    "            \"images\": image.numpy(),\n",
    "            'orig_target_sizes': size\n",
    "        }  # Still totally real data\n",
    "        \n",
    "input_data = list(load_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5137d",
   "metadata": {},
   "source": [
    "# Compare onnx model and engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509dfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\n",
    "from polygraphy.backend.trt import EngineFromNetwork, NetworkFromOnnxPath, TrtRunner, SaveEngine\n",
    "from polygraphy.comparator import Comparator, CompareFunc\n",
    "import tensorrt as trt\n",
    "\n",
    "\n",
    "# The OnnxrtRunner requires an ONNX-RT session.\n",
    "# We can use the SessionFromOnnx lazy loader to construct one easily:\n",
    "\n",
    "def compare(onnx_model_path: str, engine_path: str, dataloader, use_fp16):\n",
    "    build_onnxrt_session = SessionFromOnnx(onnx_model_path)\n",
    "\n",
    "    # optimization config for build tensorrt engine\n",
    "    logger = trt.Logger(trt.Logger.VERBOSE)\n",
    "\n",
    "    builder = trt.Builder(logger)\n",
    "    config = builder.create_builder_config()\n",
    "    config.profiling_verbosity = trt.ProfilingVerbosity.DETAILED\n",
    "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30) # 1 GiB\n",
    "    if use_fp16:\n",
    "        print(\"use fp16\")\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "    build_engine = SaveEngine(\n",
    "        EngineFromNetwork(\n",
    "            NetworkFromOnnxPath(onnx_model_path),\n",
    "            config=config\n",
    "        ), \n",
    "        path=engine_path\n",
    "    )\n",
    "    # save log into local file\n",
    "\n",
    "\n",
    "\n",
    "    runners = [\n",
    "        TrtRunner(build_engine),\n",
    "        OnnxrtRunner(build_onnxrt_session),\n",
    "    ]\n",
    "\n",
    "    # `Comparator.run()` will run each runner separately using synthetic input data and\n",
    "    #   return a `RunResults` instance. See `polygraphy/comparator/struct.py` for details.\n",
    "    #\n",
    "    # TIP: To use custom input data, you can set the `data_loader` parameter in `Comparator.run()``\n",
    "    #   to a generator or iterable that yields `Dict[str, np.ndarray]`.\n",
    "    run_results = Comparator.run(runners, data_loader=dataloader)\n",
    "\n",
    "    # `Comparator.compare_accuracy()` checks that outputs match between runners.\n",
    "    #\n",
    "    # TIP: The `compare_func` parameter can be used to control how outputs are compared (see API reference for details).\n",
    "    #   The default comparison function is created by `CompareFunc.simple()`, but we can construct it\n",
    "    #   explicitly if we want to change the default parameters, such as tolerance.\n",
    "\n",
    "    # We can use `RunResults.save()` method to save the inference results to a JSON file.\n",
    "    # This can be useful if you want to generate and compare results separately.\n",
    "    # run_results.save(\"inference_results.json\")\n",
    "\n",
    "    # assert bool(\n",
    "    #     Comparator.compare_accuracy(\n",
    "    #         run_results, compare_func=CompareFunc.simple(atol=1e-8)\n",
    "    #     )\n",
    "    # )\n",
    "    compare_result = Comparator.compare_accuracy(\n",
    "        run_results, compare_func=CompareFunc.simple(atol=1e-8)\n",
    "    )\n",
    "    return run_results, compare_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461dfa71",
   "metadata": {},
   "source": [
    "## Mark outputs to be compared by modifying the output nodes of onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c216df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from polygraphy.backend.onnx import modify_outputs\n",
    "from polygraphy import constants\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "\n",
    "def modify_model(MODEL_PATH: str, modified_model_name: str, expected_outputs: list = []):\n",
    "    model = onnx.load(MODEL_PATH)\n",
    "    # model = onnx.load()\n",
    "    # NOTE: we record the original meta info of output nodes, since we observe that polygrpahy\n",
    "    # corrupts this part when we modify the outputs\n",
    "    original_outputs_meta = {output.name: output.type for output in model.graph.output}\n",
    "    original_output_name = list(original_outputs_meta.keys())\n",
    "    print(\"\\033[96m[INFO] Origianl output \\033[0m\")\n",
    "    print(original_output_name)\n",
    "    # onnx.save(model, 'out.onnx')\n",
    "\n",
    "    model = modify_outputs(model, outputs=constants.MARK_ALL)\n",
    "    # get all outputs name\n",
    "    all_node_outputs = model.graph.output\n",
    "    all_node_output_name = [_.name for _ in all_node_outputs]\n",
    "\n",
    "    add_original_output = True\n",
    "    if add_original_output:\n",
    "        expected_outputs = expected_outputs + original_output_name\n",
    "    print(\"\\033[96m[INFO] output to be compared \\033[0m\")\n",
    "    pprint.pprint(expected_outputs)\n",
    "    model = onnx.load(MODEL_PATH)\n",
    "    model = modify_outputs(model, outputs=expected_outputs)\n",
    "    \n",
    "    # NOTE: restore the corrput output info, as the original info is lost when we modify the outputs\n",
    "    # NOTE: Maybe a bug, spent a lot of patience on it.\n",
    "    for output in model.graph.output:\n",
    "        if output.name in original_output_name:\n",
    "            output.type.CopyFrom(original_outputs_meta[output.name])\n",
    "    \n",
    "    onnx.checker.check_model(model)\n",
    "    onnx.save(model, modified_model_name)\n",
    "\n",
    "def modify_and_compare(setting, onnx_model, input_data, if_modify: bool, use_custome_data: bool, expected_outputs: list, use_fp16: bool):\n",
    "    onnx_path = onnx_model\n",
    "    # get model basename\n",
    "    model_name = os.path.splitext(os.path.basename(onnx_path))[0]\n",
    "    modified_onnx_path = model_name + setting + \"-output_modified.onnx\"\n",
    "    saved_engine_name = model_name + setting + \"-output_modified.engine\"\n",
    "    # remove existing modified onnx model and engine\n",
    "    if os.path.exists(modified_onnx_path):\n",
    "        print(\"\\033[96m[INFO] remove existing onnx file \\033[0m\")\n",
    "        os.remove(modified_onnx_path)\n",
    "    if os.path.exists(saved_engine_name):\n",
    "        print(\"\\033[96m[INFO] remove existing engine file \\033[0m\")\n",
    "        os.remove(saved_engine_name)\n",
    "    if if_modify:\n",
    "        modify_model(onnx_path, modified_onnx_path, expected_outputs)\n",
    "    else:\n",
    "        modified_onnx_path = onnx_path\n",
    "    if use_custome_data:\n",
    "        run_result, compare_result = compare(modified_onnx_path, saved_engine_name, input_data, use_fp16)\n",
    "    else:\n",
    "        run_result, compare_result = compare(modified_onnx_path, saved_engine_name, None, use_fp16)\n",
    "    return run_result, compare_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d678b",
   "metadata": {},
   "source": [
    "# Comparison Results of Original Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd2ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m[INFO] log file: polygraphy_results/baseline.log\u001b[0m\n",
      "[05/19/2025-15:03:55] [TRT] [I] [MemUsageChange] Init CUDA: CPU -18, GPU +0, now: CPU 435, GPU 393 (MiB)\n",
      "[05/19/2025-15:03:55] [TRT] [V] Trying to load shared library libnvinfer_builder_resource.so.10.7.0\n",
      "[05/19/2025-15:03:55] [TRT] [V] Loaded shared library libnvinfer_builder_resource.so.10.7.0\n",
      "[05/19/2025-15:03:57] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +2283, GPU +440, now: CPU 2594, GPU 833 (MiB)\n",
      "[05/19/2025-15:03:57] [TRT] [V] CUDA lazy loading is enabled.\n",
      "[05/19/2025-15:03:57] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "# onnx_model = \"../benchmark_models/default_mtq_int8_q_qint8.onnx\"\n",
    "onnx_model = \"../../benchmark_models/default_mtq_int8_q_qint8.onnx\"\n",
    "# only allow fp32\n",
    "use_fp16 = False\n",
    "\n",
    "# we first try the original output, then we manually mark some outputs to \n",
    "# prevent automatic fusion operations in the building process of engine\n",
    "setting = 'baseline'\n",
    "expected_outputs = []\n",
    "output_dir = \"polygraphy_results\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_file = f'{setting}.log'\n",
    "log_file = os.path.join(output_dir, log_file)\n",
    "print(f\"\\033[96m[INFO] log file: {log_file}\\033[0m\")\n",
    "with open(log_file, \"w\") as f:\n",
    "    with redirect_stdout(f), redirect_stderr(f):\n",
    "        run_result, compare_result = modify_and_compare(setting, onnx_model, input_data, True, True, expected_outputs, use_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a3002",
   "metadata": {},
   "source": [
    "# Comparison Results of Problematic Nodes\n",
    "Here are the problematic nodes that get fused during the conversion process.\n",
    "```python\n",
    "fusion\n",
    "0.00511952 ms\n",
    "__myl_FcMulAdd_myl85_40\n",
    "[ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul]\n",
    "[ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear]\n",
    "[ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear]\n",
    "[ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add]\n",
    "[ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul]\n",
    "[ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear]\n",
    "[ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add]\n",
    "```\n",
    "\n",
    "The following nodes are directly connected with those problematic fused node:\n",
    "```python\n",
    "# sampling_offsets part\n",
    "'/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0',\n",
    "'/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0',\n",
    "'/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0',\n",
    "\n",
    "# attn weights part\n",
    "'/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0',\n",
    "'/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0',\n",
    "'/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0',\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd677a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_file: polygraphy_results/mark_outputs_of_fused_nodes.log\n",
      "[05/19/2025-15:35:50] [TRT] [V] Trying to load shared library libnvinfer_builder_resource.so.10.7.0\n",
      "[05/19/2025-15:35:50] [TRT] [V] Loaded shared library libnvinfer_builder_resource.so.10.7.0\n",
      "[05/19/2025-15:35:51] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -2275, GPU +432, now: CPU 7124, GPU 3109 (MiB)\n",
      "[05/19/2025-15:35:51] [TRT] [V] CUDA lazy loading is enabled.\n",
      "[05/19/2025-15:35:51] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "# onnx_model = \"../benchmark_models/default_mtq_int8_q_qint8.onnx\"\n",
    "onnx_model = \"../../benchmark_models/default_mtq_int8_q_qint8.onnx\"\n",
    "use_fp16 = False\n",
    "expected_outputs = [\n",
    "# sampling_offsets part\n",
    "'/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0',\n",
    "'/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0',\n",
    "'/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0',\n",
    "\n",
    "# attn weights part\n",
    "'/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0',\n",
    "'/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0',\n",
    "'/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0',\n",
    "]\n",
    "# we first try the original output, then we manually mark some outputs to \n",
    "# prevent automatic fusion operations in the building process of engine\n",
    "setting = 'mark_outputs_of_fused_nodes'\n",
    "log_file = f\"{setting}.log\"\n",
    "log_file = os.path.join(output_dir, log_file)\n",
    "print(f\"log_file: {log_file}\")\n",
    "with open(log_file, \"w\") as f:\n",
    "    with redirect_stdout(f), redirect_stderr(f):\n",
    "        run_result, compare_result = modify_and_compare(setting, onnx_model, input_data, True, True, expected_outputs, use_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98b75d",
   "metadata": {},
   "source": [
    "# Forcibly  Break the Fusion \n",
    "Directly mark the outputs of some fused nodes as final model output break the original fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99dfea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_file: polygraphy_results/break_fusion.log\n",
      "[05/21/2025-15:15:03] [TRT] [V] Trying to load shared library libnvinfer_builder_resource.so.10.7.0\n",
      "[05/21/2025-15:15:03] [TRT] [V] Loaded shared library libnvinfer_builder_resource.so.10.7.0\n",
      "[05/21/2025-15:15:04] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -2274, GPU +424, now: CPU 10114, GPU 3121 (MiB)\n",
      "[05/21/2025-15:15:04] [TRT] [V] CUDA lazy loading is enabled.\n",
      "[05/21/2025-15:15:04] [TRT] [W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "onnx_model = \"../../benchmark_models/default_mtq_int8_q_qint8.onnx\"\n",
    "use_fp16 = False\n",
    "expected_outputs = [\n",
    "# sampling_offsets part\n",
    "'/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0',\n",
    "'/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0',\n",
    "'/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0',\n",
    "\n",
    "# attn weights part\n",
    "'/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0',\n",
    "'/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0',\n",
    "'/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0',\n",
    "\n",
    "# this break the origianl fusion\n",
    "'/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0',\n",
    "'/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0',\n",
    "'/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0',\n",
    "\n",
    "'/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0',\n",
    "'/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0',\n",
    "'/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0',\n",
    "]\n",
    "# we first try the original output, then we manually mark some outputs to \n",
    "# prevent automatic fusion operations in the building process of engine\n",
    "setting = 'break_fusion'\n",
    "output_dir = \"polygraphy_results\"\n",
    "log_file = f\"{setting}.log\"\n",
    "log_file = os.path.join(output_dir, log_file)\n",
    "print(f\"log_file: {log_file}\")\n",
    "with open(log_file, \"w\") as f:\n",
    "    with redirect_stdout(f), redirect_stderr(f):\n",
    "        run_result, compare_result = modify_and_compare(setting, onnx_model, input_data, True, True, expected_outputs, use_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c40cabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
