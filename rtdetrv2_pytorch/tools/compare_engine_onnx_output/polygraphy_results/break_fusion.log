[96m[INFO] Origianl output [0m
['labels', 'boxes', 'scores']
[V] Marking all ONNX tensors as outputs
[96m[INFO] output to be compared [0m
['/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0',
 'labels',
 'boxes',
 'scores']
[I] trt-runner-N8-07/03/25-18:26:23     | Activating and starting inference
[V] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.
[X] CUDA lazy loading is enabled.
[X] Plugin creator already registered - ::ROIAlign_TRT version 2
[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1
[X] Plugin creator already registered - ::BatchedNMS_TRT version 1
[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1
[X] Plugin creator already registered - ::Clip_TRT version 1
[X] Plugin creator already registered - ::CoordConvAC version 1
[X] Plugin creator already registered - ::CropAndResizeDynamic version 1
[X] Plugin creator already registered - ::CropAndResize version 1
[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1
[X] Plugin creator already registered - ::DetectionLayer_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_TRT version 1
[X] Plugin creator already registered - ::FlattenConcat_TRT version 1
[X] Plugin creator already registered - ::GenerateDetection_TRT version 1
[X] Plugin creator already registered - ::GridAnchor_TRT version 1
[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 3
[X] Plugin creator already registered - ::LReLU_TRT version 1
[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1
[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1
[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1
[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1
[X] Plugin creator already registered - ::NMSDynamic_TRT version 1
[X] Plugin creator already registered - ::NMS_TRT version 1
[X] Plugin creator already registered - ::Normalize_TRT version 1
[X] Plugin creator already registered - ::PillarScatterPlugin version 1
[X] Plugin creator already registered - ::PriorBox_TRT version 1
[X] Plugin creator already registered - ::ProposalDynamic version 1
[X] Plugin creator already registered - ::ProposalLayer_TRT version 1
[X] Plugin creator already registered - ::Proposal version 1
[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1
[X] Plugin creator already registered - ::Region_TRT version 1
[X] Plugin creator already registered - ::Reorg_TRT version 2
[X] Plugin creator already registered - ::Reorg_TRT version 1
[X] Plugin creator already registered - ::ResizeNearest_TRT version 1
[X] Plugin creator already registered - ::ROIAlign_TRT version 1
[X] Plugin creator already registered - ::RPROI_TRT version 1
[X] Plugin creator already registered - ::ScatterElements version 1
[X] Plugin creator already registered - ::ScatterElements version 2
[X] Plugin creator already registered - ::ScatterND version 1
[X] Plugin creator already registered - ::SpecialSlice_TRT version 1
[X] Plugin creator already registered - ::Split version 1
[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1
[V] ----------------------------------------------------------------
[V] Input filename:   q_config_3break_fusion-output_modified.onnx
[V] ONNX IR version:  0.0.8
[V] Opset version:    17
[V] Producer name:    pytorch
[V] Producer version: 2.5.0
[V] Domain:           
[V] Model version:    0
[V] Doc string:       
[V] ----------------------------------------------------------------
[X] Adding network input: images with dtype: float32, dimensions: (1, 3, 640, 640)
[X] Registering tensor: images for ONNX tensor: images
[X] Adding network input: orig_target_sizes with dtype: int64, dimensions: (1, 2)
[X] Registering tensor: orig_target_sizes for ONNX tensor: orig_target_sizes
[X] Importing initializer: model.backbone.conv1.conv1_1.conv.weight
[X] Importing initializer: model.backbone.conv1.conv1_1.norm.weight
[X] Importing initializer: model.backbone.conv1.conv1_1.norm.bias
[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_mean
[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_var
[X] Importing initializer: model.backbone.conv1.conv1_2.conv.weight
[X] Importing initializer: model.backbone.conv1.conv1_2.norm.weight
[X] Importing initializer: model.backbone.conv1.conv1_2.norm.bias
[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_mean
[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_var
[X] Importing initializer: model.backbone.conv1.conv1_3.conv.weight
[X] Importing initializer: model.backbone.conv1.conv1_3.norm.weight
[X] Importing initializer: model.backbone.conv1.conv1_3.norm.bias
[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_mean
[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.bias
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean
[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var
[X] Importing initializer: model.decoder.anchors
[X] Importing initializer: model.decoder.input_proj.0.conv.weight
[X] Importing initializer: model.decoder.input_proj.0.norm.weight
[X] Importing initializer: model.decoder.input_proj.0.norm.bias
[X] Importing initializer: model.decoder.input_proj.0.norm.running_mean
[X] Importing initializer: model.decoder.input_proj.0.norm.running_var
[X] Importing initializer: model.decoder.input_proj.1.conv.weight
[X] Importing initializer: model.decoder.input_proj.1.norm.weight
[X] Importing initializer: model.decoder.input_proj.1.norm.bias
[X] Importing initializer: model.decoder.input_proj.1.norm.running_mean
[X] Importing initializer: model.decoder.input_proj.1.norm.running_var
[X] Importing initializer: model.decoder.input_proj.2.conv.weight
[X] Importing initializer: model.decoder.input_proj.2.norm.weight
[X] Importing initializer: model.decoder.input_proj.2.norm.bias
[X] Importing initializer: model.decoder.input_proj.2.norm.running_mean
[X] Importing initializer: model.decoder.input_proj.2.norm.running_var
[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.0.norm1.weight
[X] Importing initializer: model.decoder.decoder.layers.0.norm1.bias
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.0.norm2.weight
[X] Importing initializer: model.decoder.decoder.layers.0.norm2.bias
[X] Importing initializer: model.decoder.decoder.layers.0.linear1.weight
[X] Importing initializer: model.decoder.decoder.layers.0.linear1.bias
[X] Importing initializer: model.decoder.decoder.layers.0.linear2.weight
[X] Importing initializer: model.decoder.decoder.layers.0.linear2.bias
[X] Importing initializer: model.decoder.decoder.layers.0.norm3.weight
[X] Importing initializer: model.decoder.decoder.layers.0.norm3.bias
[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.1.norm1.weight
[X] Importing initializer: model.decoder.decoder.layers.1.norm1.bias
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.1.norm2.weight
[X] Importing initializer: model.decoder.decoder.layers.1.norm2.bias
[X] Importing initializer: model.decoder.decoder.layers.1.linear1.weight
[X] Importing initializer: model.decoder.decoder.layers.1.linear1.bias
[X] Importing initializer: model.decoder.decoder.layers.1.linear2.weight
[X] Importing initializer: model.decoder.decoder.layers.1.linear2.bias
[X] Importing initializer: model.decoder.decoder.layers.1.norm3.weight
[X] Importing initializer: model.decoder.decoder.layers.1.norm3.bias
[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.2.norm1.weight
[X] Importing initializer: model.decoder.decoder.layers.2.norm1.bias
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight
[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias
[X] Importing initializer: model.decoder.decoder.layers.2.norm2.weight
[X] Importing initializer: model.decoder.decoder.layers.2.norm2.bias
[X] Importing initializer: model.decoder.decoder.layers.2.linear1.weight
[X] Importing initializer: model.decoder.decoder.layers.2.linear1.bias
[X] Importing initializer: model.decoder.decoder.layers.2.linear2.weight
[X] Importing initializer: model.decoder.decoder.layers.2.linear2.bias
[X] Importing initializer: model.decoder.decoder.layers.2.norm3.weight
[X] Importing initializer: model.decoder.decoder.layers.2.norm3.bias
[X] Importing initializer: model.decoder.query_pos_head.layers.0.weight
[X] Importing initializer: model.decoder.query_pos_head.layers.0.bias
[X] Importing initializer: model.decoder.query_pos_head.layers.1.weight
[X] Importing initializer: model.decoder.query_pos_head.layers.1.bias
[X] Importing initializer: model.decoder.enc_output.proj.weight
[X] Importing initializer: model.decoder.enc_output.proj.bias
[X] Importing initializer: model.decoder.enc_output.norm.weight
[X] Importing initializer: model.decoder.enc_output.norm.bias
[X] Importing initializer: model.decoder.enc_score_head.weight
[X] Importing initializer: model.decoder.enc_score_head.bias
[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.weight
[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.bias
[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.weight
[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.bias
[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.weight
[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.bias
[X] Importing initializer: model.decoder.dec_score_head.2.weight
[X] Importing initializer: model.decoder.dec_score_head.2.bias
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.weight
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.bias
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.weight
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.bias
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.weight
[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.bias
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.weight
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.bias
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.weight
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.bias
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.weight
[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.bias
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.weight
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.bias
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.weight
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.bias
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.weight
[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.bias
[X] Importing initializer: model.encoder.input_proj.0.conv.weight
[X] Importing initializer: model.encoder.input_proj.0.norm.weight
[X] Importing initializer: model.encoder.input_proj.0.norm.bias
[X] Importing initializer: model.encoder.input_proj.0.norm.running_mean
[X] Importing initializer: model.encoder.input_proj.0.norm.running_var
[X] Importing initializer: model.encoder.input_proj.1.conv.weight
[X] Importing initializer: model.encoder.input_proj.1.norm.weight
[X] Importing initializer: model.encoder.input_proj.1.norm.bias
[X] Importing initializer: model.encoder.input_proj.1.norm.running_mean
[X] Importing initializer: model.encoder.input_proj.1.norm.running_var
[X] Importing initializer: model.encoder.input_proj.2.conv.weight
[X] Importing initializer: model.encoder.input_proj.2.norm.weight
[X] Importing initializer: model.encoder.input_proj.2.norm.bias
[X] Importing initializer: model.encoder.input_proj.2.norm.running_mean
[X] Importing initializer: model.encoder.input_proj.2.norm.running_var
[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias
[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.bias
[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.bias
[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.bias
[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.weight
[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.bias
[X] Importing initializer: model.encoder.lateral_convs.0.conv.weight
[X] Importing initializer: model.encoder.lateral_convs.0.norm.weight
[X] Importing initializer: model.encoder.lateral_convs.0.norm.bias
[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_mean
[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_var
[X] Importing initializer: model.encoder.lateral_convs.1.conv.weight
[X] Importing initializer: model.encoder.lateral_convs.1.norm.weight
[X] Importing initializer: model.encoder.lateral_convs.1.norm.bias
[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_mean
[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_var
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.conv.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.weight
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.bias
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_mean
[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_var
[X] Importing initializer: model.encoder.downsample_convs.0.conv.weight
[X] Importing initializer: model.encoder.downsample_convs.0.norm.weight
[X] Importing initializer: model.encoder.downsample_convs.0.norm.bias
[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_mean
[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_var
[X] Importing initializer: model.encoder.downsample_convs.1.conv.weight
[X] Importing initializer: model.encoder.downsample_convs.1.norm.weight
[X] Importing initializer: model.encoder.downsample_convs.1.norm.bias
[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_mean
[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_var
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.conv.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.weight
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.bias
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_mean
[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_var
[X] Importing initializer: onnx::Add_3614
[X] Importing initializer: onnx::Add_3616
[X] Importing initializer: onnx::Add_3618
[X] Importing initializer: onnx::MatMul_3619
[X] Importing initializer: onnx::MatMul_3620
[X] Importing initializer: onnx::MatMul_3621
[X] Importing initializer: onnx::Mul_3692
[X] Importing initializer: onnx::Add_3731
[X] Importing initializer: onnx::Add_3733
[X] Importing initializer: onnx::Add_3735
[X] Importing initializer: onnx::MatMul_3736
[X] Importing initializer: onnx::MatMul_3737
[X] Importing initializer: onnx::MatMul_3738
[X] Importing initializer: onnx::Mul_3755
[X] Importing initializer: onnx::Add_3803
[X] Importing initializer: onnx::Add_3805
[X] Importing initializer: onnx::Add_3807
[X] Importing initializer: onnx::MatMul_3808
[X] Importing initializer: onnx::MatMul_3809
[X] Importing initializer: onnx::MatMul_3810
[X] Importing initializer: onnx::Add_3875
[X] Importing initializer: onnx::Add_3877
[X] Importing initializer: onnx::Add_3879
[X] Importing initializer: onnx::MatMul_3880
[X] Importing initializer: onnx::MatMul_3881
[X] Importing initializer: onnx::MatMul_3882
[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/Constant_2_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/Constant_output_0
[X] Importing initializer: onnx::Unsqueeze_1255
[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/Constant_7_output_0
[X] Importing initializer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/Constant_9_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/Constant_18_output_0
[X] Importing initializer: /model/decoder/Constant_21_output_0
[X] Importing initializer: /model/decoder/decoder/Constant_output_0
[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0
[X] Importing initializer: onnx::Split_2305
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/Constant_3_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Importing initializer: /postprocessor/Constant_output_0
[X] Importing initializer: onnx::Tile_3498
[X] Importing initializer: /postprocessor/Constant_14_output_0
[X] Importing initializer: _v_4326
[X] Importing initializer: _v_2002
[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0
[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0
[X] Importing initializer: /model/encoder/Concat_1_output_0
[X] Importing initializer: /model/decoder/Concat_5_output_0
[X] Importing initializer: /model/decoder/Concat_7_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0
[X] Importing initializer: _v_1857
[X] Importing initializer: _v_1859
[X] Importing initializer: _v_1861
[X] Importing initializer: _v_1760
[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0
[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0
[X] Importing initializer: _v_1674
[X] Importing initializer: _v_1676
[X] Importing initializer: _v_1680
[X] Importing initializer: _v_1686
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: images
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [images -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight for ONNX node: tmp_weight
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_0 for ONNX node: tmp_weight_0
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.conv1.conv1_1.conv.weight
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_1.conv.weight -> (32, 3, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], 
[X] Registering layer: model.backbone.conv1.conv1_1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1 for ONNX node: tmp_weight_1
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], 
[X] Registering layer: tmp_weight_2 for ONNX node: tmp_weight_2
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv]
[X] Parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv]
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/conv1/conv1_1/conv/Conv for ONNX node: /model/backbone/conv1/conv1_1/conv/Conv
[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0
[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/Conv_output_0
[X] Searching for input: model.backbone.conv1.conv1_1.norm.weight
[X] Searching for input: model.backbone.conv1.conv1_1.norm.bias
[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_mean
[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_var
[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_1.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_var -> (32)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_1/norm/BatchNormalization
[X] Registering tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu]
[X] Parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu]
[X] Searching for input: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Registering layer: /model/backbone/conv1/conv1_1/act/Relu for ONNX node: /model/backbone/conv1/conv1_1/act/Relu
[X] Registering tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0
[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_1/act/Relu_output_0
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_3 for ONNX node: tmp_weight_3
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_4 for ONNX node: tmp_weight_4
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.conv1.conv1_2.conv.weight
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_2.conv.weight -> (32, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], 
[X] Registering layer: model.backbone.conv1.conv1_2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_5 for ONNX node: tmp_weight_5
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], 
[X] Registering layer: tmp_weight_6 for ONNX node: tmp_weight_6
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv]
[X] Parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv]
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/conv1/conv1_2/conv/Conv for ONNX node: /model/backbone/conv1/conv1_2/conv/Conv
[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0
[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/conv1/conv1_2/conv/Conv_output_0
[X] Searching for input: model.backbone.conv1.conv1_2.norm.weight
[X] Searching for input: model.backbone.conv1.conv1_2.norm.bias
[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_mean
[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_var
[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_2.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_var -> (32)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_2/norm/BatchNormalization
[X] Registering tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu]
[X] Parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu]
[X] Searching for input: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Registering layer: /model/backbone/conv1/conv1_2/act/Relu for ONNX node: /model/backbone/conv1/conv1_2/act/Relu
[X] Registering tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0
[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_2/act/Relu_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_7 for ONNX node: tmp_weight_7
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_8 for ONNX node: tmp_weight_8
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.conv1.conv1_3.conv.weight
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_3.conv.weight -> (64, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.conv1.conv1_3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_9 for ONNX node: tmp_weight_9
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_10 for ONNX node: tmp_weight_10
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv]
[X] Parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv]
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/conv1/conv1_3/conv/Conv for ONNX node: /model/backbone/conv1/conv1_3/conv/Conv
[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0
[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/Conv_output_0
[X] Searching for input: model.backbone.conv1.conv1_3.norm.weight
[X] Searching for input: model.backbone.conv1.conv1_3.norm.bias
[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_mean
[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_var
[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_3.norm.weight -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.bias -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_mean -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_3/norm/BatchNormalization
[X] Registering tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu]
[X] Parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu]
[X] Searching for input: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0
[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Registering layer: /model/backbone/conv1/conv1_3/act/Relu for ONNX node: /model/backbone/conv1/conv1_3/act/Relu
[X] Registering tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0
[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/MaxPool [MaxPool]
[X] Parsing node: /model/backbone/MaxPool [MaxPool]
[X] Searching for input: /model/backbone/conv1/conv1_3/act/Relu_output_0
[X] /model/backbone/MaxPool [MaxPool] inputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], 
[X] Registering layer: /model/backbone/MaxPool for ONNX node: /model/backbone/MaxPool
[X] Registering tensor: /model/backbone/MaxPool_output_0 for ONNX tensor: /model/backbone/MaxPool_output_0
[X] /model/backbone/MaxPool [MaxPool] outputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/MaxPool_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_11 for ONNX node: tmp_weight_11
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_12 for ONNX node: tmp_weight_12
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_13 for ONNX node: tmp_weight_13
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_14 for ONNX node: tmp_weight_14
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var
[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_15 for ONNX node: tmp_weight_15
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_16 for ONNX node: tmp_weight_16
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_17 for ONNX node: tmp_weight_17
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_18 for ONNX node: tmp_weight_18
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var
[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.short.conv.weight -> (64, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.0.short.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_19 for ONNX node: tmp_weight_19
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_20 for ONNX node: tmp_weight_20
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_var
[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/Add for ONNX node: /model/backbone/res_layers.0/blocks.0/Add
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0
[X] /model/backbone/res_layers.0/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/Add_output_0
[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/act/Relu
[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_21 for ONNX node: tmp_weight_21
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_22 for ONNX node: tmp_weight_22
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_23 for ONNX node: tmp_weight_23
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_24 for ONNX node: tmp_weight_24
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var
[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_25 for ONNX node: tmp_weight_25
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_26 for ONNX node: tmp_weight_26
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_27 for ONNX node: tmp_weight_27
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], 
[X] Registering layer: tmp_weight_28 for ONNX node: tmp_weight_28
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var
[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var -> (64)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/Add for ONNX node: /model/backbone/res_layers.0/blocks.1/Add
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0
[X] /model/backbone/res_layers.0/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/Add_output_0
[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.0/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/act/Relu
[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0
[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_29 for ONNX node: tmp_weight_29
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_30 for ONNX node: tmp_weight_30
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2a.conv.weight -> (128, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_31 for ONNX node: tmp_weight_31
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_32 for ONNX node: tmp_weight_32
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var
[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_33 for ONNX node: tmp_weight_33
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_34 for ONNX node: tmp_weight_34
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_35 for ONNX node: tmp_weight_35
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_36 for ONNX node: tmp_weight_36
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var
[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_37 for ONNX node: tmp_weight_37
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_38 for ONNX node: tmp_weight_38
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_39 for ONNX node: tmp_weight_39
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_40 for ONNX node: tmp_weight_40
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.short.conv.conv.weight -> (128, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_41 for ONNX node: tmp_weight_41
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_42 for ONNX node: tmp_weight_42
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var
[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/Add for ONNX node: /model/backbone/res_layers.1/blocks.0/Add
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0
[X] /model/backbone/res_layers.1/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/Add_output_0
[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/act/Relu
[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_43 for ONNX node: tmp_weight_43
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_44 for ONNX node: tmp_weight_44
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2a.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_45 for ONNX node: tmp_weight_45
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_46 for ONNX node: tmp_weight_46
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var
[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_47 for ONNX node: tmp_weight_47
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_48 for ONNX node: tmp_weight_48
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_49 for ONNX node: tmp_weight_49
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_50 for ONNX node: tmp_weight_50
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var
[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/Add for ONNX node: /model/backbone/res_layers.1/blocks.1/Add
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0
[X] /model/backbone/res_layers.1/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/Add_output_0
[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.1/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/act/Relu
[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0
[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_51 for ONNX node: tmp_weight_51
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_52 for ONNX node: tmp_weight_52
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2a.conv.weight -> (256, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_53 for ONNX node: tmp_weight_53
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_54 for ONNX node: tmp_weight_54
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var
[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_55 for ONNX node: tmp_weight_55
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_56 for ONNX node: tmp_weight_56
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_57 for ONNX node: tmp_weight_57
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_58 for ONNX node: tmp_weight_58
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var
[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_59 for ONNX node: tmp_weight_59
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_60 for ONNX node: tmp_weight_60
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_61 for ONNX node: tmp_weight_61
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_62 for ONNX node: tmp_weight_62
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.short.conv.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_63 for ONNX node: tmp_weight_63
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_64 for ONNX node: tmp_weight_64
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var
[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/Add for ONNX node: /model/backbone/res_layers.2/blocks.0/Add
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0
[X] /model/backbone/res_layers.2/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/Add_output_0
[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/act/Relu
[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_65 for ONNX node: tmp_weight_65
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_66 for ONNX node: tmp_weight_66
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2a.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_67 for ONNX node: tmp_weight_67
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_68 for ONNX node: tmp_weight_68
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var
[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_69 for ONNX node: tmp_weight_69
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_70 for ONNX node: tmp_weight_70
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_71 for ONNX node: tmp_weight_71
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_72 for ONNX node: tmp_weight_72
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var
[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/Add for ONNX node: /model/backbone/res_layers.2/blocks.1/Add
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0
[X] /model/backbone/res_layers.2/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/Add_output_0
[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.2/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/act/Relu
[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0
[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_73 for ONNX node: tmp_weight_73
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_74 for ONNX node: tmp_weight_74
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2a.conv.weight -> (512, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_75 for ONNX node: tmp_weight_75
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_76 for ONNX node: tmp_weight_76
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var
[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_77 for ONNX node: tmp_weight_77
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_78 for ONNX node: tmp_weight_78
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_79 for ONNX node: tmp_weight_79
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_80 for ONNX node: tmp_weight_80
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var
[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_81 for ONNX node: tmp_weight_81
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_82 for ONNX node: tmp_weight_82
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_83 for ONNX node: tmp_weight_83
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_84 for ONNX node: tmp_weight_84
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.short.conv.conv.weight -> (512, 256, 1, 1)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_85 for ONNX node: tmp_weight_85
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_86 for ONNX node: tmp_weight_86
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var
[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/Add for ONNX node: /model/backbone/res_layers.3/blocks.0/Add
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0
[X] /model/backbone/res_layers.3/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/Add_output_0
[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/act/Relu
[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_87 for ONNX node: tmp_weight_87
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_88 for ONNX node: tmp_weight_88
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2a.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_89 for ONNX node: tmp_weight_89
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_90 for ONNX node: tmp_weight_90
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var
[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_91 for ONNX node: tmp_weight_91
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_92 for ONNX node: tmp_weight_92
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight required by ONNX-TRT
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_93 for ONNX node: tmp_weight_93
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_94 for ONNX node: tmp_weight_94
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean
[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var
[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var -> (512)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/Add for ONNX node: /model/backbone/res_layers.3/blocks.1/Add
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0
[X] /model/backbone/res_layers.3/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu]
[X] Parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/Add_output_0
[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Registering layer: /model/backbone/res_layers.3/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/act/Relu
[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0
[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.input_proj.0.conv.weight
[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.0.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.input_proj.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_95 for ONNX node: tmp_weight_95
[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_96 for ONNX node: tmp_weight_96
[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/input_proj.0/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.0/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/input_proj.0/conv/Conv for ONNX node: /model/encoder/input_proj.0/conv/Conv
[X] Registering tensor: /model/encoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/Conv_output_0
[X] /model/encoder/input_proj.0/conv/Conv [Conv] outputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/input_proj.0/conv/Conv_output_0
[X] Searching for input: model.encoder.input_proj.0.norm.weight
[X] Searching for input: model.encoder.input_proj.0.norm.bias
[X] Searching for input: model.encoder.input_proj.0.norm.running_mean
[X] Searching for input: model.encoder.input_proj.0.norm.running_var
[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.0/norm/BatchNormalization
[X] Registering tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0
[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.input_proj.1.conv.weight
[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.input_proj.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_97 for ONNX node: tmp_weight_97
[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_98 for ONNX node: tmp_weight_98
[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/input_proj.1/conv/Conv [Conv]
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.1/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/input_proj.1/conv/Conv for ONNX node: /model/encoder/input_proj.1/conv/Conv
[X] Registering tensor: /model/encoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/Conv_output_0
[X] /model/encoder/input_proj.1/conv/Conv [Conv] outputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/input_proj.1/conv/Conv_output_0
[X] Searching for input: model.encoder.input_proj.1.norm.weight
[X] Searching for input: model.encoder.input_proj.1.norm.bias
[X] Searching for input: model.encoder.input_proj.1.norm.running_mean
[X] Searching for input: model.encoder.input_proj.1.norm.running_var
[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0
[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0
[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_99 for ONNX node: tmp_weight_99
[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_100 for ONNX node: tmp_weight_100
[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.input_proj.2.conv.weight
[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.2.conv.weight -> (256, 512, 1, 1)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.input_proj.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_101 for ONNX node: tmp_weight_101
[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_102 for ONNX node: tmp_weight_102
[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/input_proj.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/input_proj.2/conv/Conv [Conv] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/input_proj.2/conv/Conv for ONNX node: /model/encoder/input_proj.2/conv/Conv
[X] Registering tensor: /model/encoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/Conv_output_0
[X] /model/encoder/input_proj.2/conv/Conv [Conv] outputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/input_proj.2/conv/Conv_output_0
[X] Searching for input: model.encoder.input_proj.2.norm.weight
[X] Searching for input: model.encoder.input_proj.2.norm.bias
[X] Searching for input: model.encoder.input_proj.2.norm.running_mean
[X] Searching for input: model.encoder.input_proj.2.norm.running_var
[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0
[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Reshape [Reshape]
[X] Parsing node: /model/encoder/Reshape [Reshape]
[X] Searching for input: /model/encoder/input_proj.2/norm/BatchNormalization_output_0
[X] Searching for input: _v_4326
[X] /model/encoder/Reshape [Reshape] inputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle required by ONNX-TRT
[X] Registering layer: /model/encoder/Reshape for ONNX node: /model/encoder/Reshape
[X] Registering tensor: /model/encoder/Reshape_output_0 for ONNX tensor: /model/encoder/Reshape_output_0
[X] /model/encoder/Reshape [Reshape] outputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Transpose [Transpose]
[X] Parsing node: /model/encoder/Transpose [Transpose]
[X] Searching for input: /model/encoder/Reshape_output_0
[X] /model/encoder/Transpose [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Registering layer: /model/encoder/Transpose for ONNX node: /model/encoder/Transpose
[X] Registering tensor: /model/encoder/Transpose_output_0 for ONNX tensor: /model/encoder/Transpose_output_0
[X] /model/encoder/Transpose [Transpose] outputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/Add [Add]
[X] Searching for input: /model/encoder/Transpose_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/Add [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/Constant_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/Constant_output_0 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/Add for ONNX node: /model/encoder/encoder.0/layers.0/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_output_0
[X] /model/encoder/encoder.0/layers.0/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/encoder/Reshape_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3619
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3619 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3619 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_103 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add]
[X] Searching for input: onnx::Add_3614
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3614 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3614 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_104 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_105 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3620
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3620 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3620 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_106 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_107 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add]
[X] Searching for input: onnx::Add_3616
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3616 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3616 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_108 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_109 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0
[X] Searching for input: onnx::MatMul_3621
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3621 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3621 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_110 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_111 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add]
[X] Searching for input: onnx::Add_3618
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3618 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3618 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_112 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_113 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_2
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_114 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_115 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_116 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_117 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_118 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_119 required by ONNX-TRT
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_120 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0
[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight
[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias
[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight required by ONNX-TRT
[X] Using opA: 0 opB: 1
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Gemm
[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_121 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_122 required by ONNX-TRT
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_123 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6
[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0
[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add]
[X] Searching for input: /model/encoder/Transpose_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0
[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/Add_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0
[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_1_output_0
[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.weight
[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.bias
[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.bias -> (256)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.weight required by ONNX-TRT
[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_126 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_127 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_128 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_129 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization
[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0
[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_130 for ONNX node: tmp_weight_130
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_131 for ONNX node: tmp_weight_131
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.weight
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_132 for ONNX node: tmp_weight_132
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: tmp_weight_133 for ONNX node: tmp_weight_133
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Transpose
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_134 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_135 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear1/MatMul
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add]
[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.bias
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_136 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_137 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0
[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_138 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_139 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Div for ONNX node: /model/encoder/encoder.0/layers.0/activation/Div
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] outputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Div_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] inputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Erf for ONNX node: /model/encoder/encoder.0/layers.0/activation/Erf
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] outputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Erf_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] inputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_140 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_141 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Add for ONNX node: /model/encoder/encoder.0/layers.0/activation/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Add_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_142 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_143 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul_1
[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0
[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_144 for ONNX node: tmp_weight_144
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_145 for ONNX node: tmp_weight_145
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.weight
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_146 for ONNX node: tmp_weight_146
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_147 for ONNX node: tmp_weight_147
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Transpose
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul]
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_148 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_149 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear2/MatMul
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add]
[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.bias
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear2.bias -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_150 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_151 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Add
[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0
[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add]
[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add]
[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Add_output_0
[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/Add_2
[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0
[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_2_output_0
[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.weight
[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.bias
[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.bias -> (256)[FLOAT]], 
[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.weight required by ONNX-TRT
[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_154 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_155 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_156 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_157 required by ONNX-TRT
[X] Registering layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization
[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0
[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Transpose_1 [Transpose]
[X] Parsing node: /model/encoder/Transpose_1 [Transpose]
[X] Searching for input: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0
[X] /model/encoder/Transpose_1 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/encoder/Transpose_1 for ONNX node: /model/encoder/Transpose_1
[X] Registering tensor: /model/encoder/Transpose_1_output_0 for ONNX tensor: /model/encoder/Transpose_1_output_0
[X] /model/encoder/Transpose_1 [Transpose] outputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Reshape_1 [Reshape]
[X] Parsing node: /model/encoder/Reshape_1 [Reshape]
[X] Searching for input: /model/encoder/Transpose_1_output_0
[X] Searching for input: /model/encoder/Concat_1_output_0
[X] /model/encoder/Reshape_1 [Reshape] inputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], [/model/encoder/Concat_1_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_158 required by ONNX-TRT
[X] Registering layer: /model/encoder/Reshape_1 for ONNX node: /model/encoder/Reshape_1
[X] Registering tensor: /model/encoder/Reshape_1_output_0 for ONNX tensor: /model/encoder/Reshape_1_output_0
[X] /model/encoder/Reshape_1 [Reshape] outputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Reshape_1_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_159 for ONNX node: tmp_weight_159
[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_160 for ONNX node: tmp_weight_160
[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.lateral_convs.0.conv.weight
[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.lateral_convs.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_161 for ONNX node: tmp_weight_161
[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_162 for ONNX node: tmp_weight_162
[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/lateral_convs.0/conv/Conv for ONNX node: /model/encoder/lateral_convs.0/conv/Conv
[X] Registering tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0
[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/lateral_convs.0/conv/Conv_output_0
[X] Searching for input: model.encoder.lateral_convs.0.norm.weight
[X] Searching for input: model.encoder.lateral_convs.0.norm.bias
[X] Searching for input: model.encoder.lateral_convs.0.norm.running_mean
[X] Searching for input: model.encoder.lateral_convs.0.norm.running_var
[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.lateral_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/lateral_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.0/norm/BatchNormalization
[X] Registering tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0
[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0
[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/lateral_convs.0/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.0/act/Sigmoid
[X] Registering tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0
[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/act/Sigmoid_output_0
[X] /model/encoder/lateral_convs.0/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/lateral_convs.0/act/Mul for ONNX node: /model/encoder/lateral_convs.0/act/Mul
[X] Registering tensor: /model/encoder/lateral_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Mul_output_0
[X] /model/encoder/lateral_convs.0/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Resize [Resize]
[X] Parsing node: /model/encoder/Resize [Resize]
[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0
[X] Searching for input: /model/encoder/Constant_9_output_0
[X] /model/encoder/Resize [Resize] inputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], 
[X] Registering layer: /model/encoder/Resize for ONNX node: /model/encoder/Resize
[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest
[X] Registering tensor: /model/encoder/Resize_output_0 for ONNX tensor: /model/encoder/Resize_output_0
[X] /model/encoder/Resize [Resize] outputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Concat_2 [Concat]
[X] Parsing node: /model/encoder/Concat_2 [Concat]
[X] Searching for input: /model/encoder/Resize_output_0
[X] Searching for input: /model/encoder/input_proj.1/norm/BatchNormalization_output_0
[X] /model/encoder/Concat_2 [Concat] inputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/Concat_2 for ONNX node: /model/encoder/Concat_2
[X] Registering tensor: /model/encoder/Concat_2_output_0 for ONNX tensor: /model/encoder/Concat_2_output_0
[X] /model/encoder/Concat_2 [Concat] outputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Concat_2_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_163 for ONNX node: tmp_weight_163
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_164 for ONNX node: tmp_weight_164
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.conv1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_165 for ONNX node: tmp_weight_165
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_166 for ONNX node: tmp_weight_166
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_var
[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_167 for ONNX node: tmp_weight_167
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_168 for ONNX node: tmp_weight_168
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_169 for ONNX node: tmp_weight_169
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_170 for ONNX node: tmp_weight_170
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_171 for ONNX node: tmp_weight_171
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_172 for ONNX node: tmp_weight_172
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_173 for ONNX node: tmp_weight_173
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_174 for ONNX node: tmp_weight_174
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_175 for ONNX node: tmp_weight_175
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_176 for ONNX node: tmp_weight_176
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_177 for ONNX node: tmp_weight_177
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_178 for ONNX node: tmp_weight_178
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.conv2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_179 for ONNX node: tmp_weight_179
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_180 for ONNX node: tmp_weight_180
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_var
[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/Add [Add]
[X] Parsing node: /model/encoder/fpn_blocks.0/Add [Add]
[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/Add [Add] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/Add for ONNX node: /model/encoder/fpn_blocks.0/Add
[X] Registering tensor: /model/encoder/fpn_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/Add_output_0
[X] /model/encoder/fpn_blocks.0/Add [Add] outputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/Add_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_181 for ONNX node: tmp_weight_181
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_182 for ONNX node: tmp_weight_182
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.0.conv3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_183 for ONNX node: tmp_weight_183
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_184 for ONNX node: tmp_weight_184
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_var
[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0
[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_185 for ONNX node: tmp_weight_185
[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_186 for ONNX node: tmp_weight_186
[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.lateral_convs.1.conv.weight
[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.lateral_convs.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_187 for ONNX node: tmp_weight_187
[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_188 for ONNX node: tmp_weight_188
[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/lateral_convs.1/conv/Conv for ONNX node: /model/encoder/lateral_convs.1/conv/Conv
[X] Registering tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0
[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/lateral_convs.1/conv/Conv_output_0
[X] Searching for input: model.encoder.lateral_convs.1.norm.weight
[X] Searching for input: model.encoder.lateral_convs.1.norm.bias
[X] Searching for input: model.encoder.lateral_convs.1.norm.running_mean
[X] Searching for input: model.encoder.lateral_convs.1.norm.running_var
[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.lateral_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/lateral_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0
[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0
[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/lateral_convs.1/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.1/act/Sigmoid
[X] Registering tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0
[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/act/Sigmoid_output_0
[X] /model/encoder/lateral_convs.1/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/lateral_convs.1/act/Mul for ONNX node: /model/encoder/lateral_convs.1/act/Mul
[X] Registering tensor: /model/encoder/lateral_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Mul_output_0
[X] /model/encoder/lateral_convs.1/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Resize_1 [Resize]
[X] Parsing node: /model/encoder/Resize_1 [Resize]
[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0
[X] Searching for input: /model/encoder/Constant_9_output_0
[X] /model/encoder/Resize_1 [Resize] inputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], 
[X] Registering layer: /model/encoder/Resize_1 for ONNX node: /model/encoder/Resize_1
[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest
[X] Registering tensor: /model/encoder/Resize_1_output_0 for ONNX tensor: /model/encoder/Resize_1_output_0
[X] /model/encoder/Resize_1 [Resize] outputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Concat_3 [Concat]
[X] Parsing node: /model/encoder/Concat_3 [Concat]
[X] Searching for input: /model/encoder/Resize_1_output_0
[X] Searching for input: /model/encoder/input_proj.0/norm/BatchNormalization_output_0
[X] /model/encoder/Concat_3 [Concat] inputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/Concat_3 for ONNX node: /model/encoder/Concat_3
[X] Registering tensor: /model/encoder/Concat_3_output_0 for ONNX tensor: /model/encoder/Concat_3_output_0
[X] /model/encoder/Concat_3 [Concat] outputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Concat_3_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_189 for ONNX node: tmp_weight_189
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_190 for ONNX node: tmp_weight_190
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.conv1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_191 for ONNX node: tmp_weight_191
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_192 for ONNX node: tmp_weight_192
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_var
[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_193 for ONNX node: tmp_weight_193
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_194 for ONNX node: tmp_weight_194
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_195 for ONNX node: tmp_weight_195
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_196 for ONNX node: tmp_weight_196
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_197 for ONNX node: tmp_weight_197
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_198 for ONNX node: tmp_weight_198
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_199 for ONNX node: tmp_weight_199
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_200 for ONNX node: tmp_weight_200
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_201 for ONNX node: tmp_weight_201
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_202 for ONNX node: tmp_weight_202
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_203 for ONNX node: tmp_weight_203
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_204 for ONNX node: tmp_weight_204
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.conv2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_205 for ONNX node: tmp_weight_205
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_206 for ONNX node: tmp_weight_206
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_var
[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/Add [Add]
[X] Parsing node: /model/encoder/fpn_blocks.1/Add [Add]
[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/Add [Add] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/Add for ONNX node: /model/encoder/fpn_blocks.1/Add
[X] Registering tensor: /model/encoder/fpn_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/Add_output_0
[X] /model/encoder/fpn_blocks.1/Add [Add] outputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/Add_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_207 for ONNX node: tmp_weight_207
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_208 for ONNX node: tmp_weight_208
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.conv.weight
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.fpn_blocks.1.conv3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_209 for ONNX node: tmp_weight_209
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_210 for ONNX node: tmp_weight_210
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/Conv
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0
[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.weight
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.bias
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_mean
[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_var
[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul]
[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0
[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Mul
[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0
[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_211 for ONNX node: tmp_weight_211
[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_212 for ONNX node: tmp_weight_212
[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.downsample_convs.0.conv.weight
[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.0.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.downsample_convs.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_213 for ONNX node: tmp_weight_213
[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_214 for ONNX node: tmp_weight_214
[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/downsample_convs.0/conv/Conv for ONNX node: /model/encoder/downsample_convs.0/conv/Conv
[X] Registering tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0
[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/Conv_output_0
[X] Searching for input: model.encoder.downsample_convs.0.norm.weight
[X] Searching for input: model.encoder.downsample_convs.0.norm.bias
[X] Searching for input: model.encoder.downsample_convs.0.norm.running_mean
[X] Searching for input: model.encoder.downsample_convs.0.norm.running_var
[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.downsample_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/downsample_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.0/norm/BatchNormalization
[X] Registering tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0
[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0
[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/downsample_convs.0/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.0/act/Sigmoid
[X] Registering tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0
[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/downsample_convs.0/act/Sigmoid_output_0
[X] /model/encoder/downsample_convs.0/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/downsample_convs.0/act/Mul for ONNX node: /model/encoder/downsample_convs.0/act/Mul
[X] Registering tensor: /model/encoder/downsample_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Mul_output_0
[X] /model/encoder/downsample_convs.0/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Concat_4 [Concat]
[X] Parsing node: /model/encoder/Concat_4 [Concat]
[X] Searching for input: /model/encoder/downsample_convs.0/act/Mul_output_0
[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0
[X] /model/encoder/Concat_4 [Concat] inputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/Concat_4 for ONNX node: /model/encoder/Concat_4
[X] Registering tensor: /model/encoder/Concat_4_output_0 for ONNX tensor: /model/encoder/Concat_4_output_0
[X] /model/encoder/Concat_4 [Concat] outputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Concat_4_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_215 for ONNX node: tmp_weight_215
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_216 for ONNX node: tmp_weight_216
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.conv1.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.conv1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_217 for ONNX node: tmp_weight_217
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_218 for ONNX node: tmp_weight_218
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.weight
[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.bias
[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_var
[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_219 for ONNX node: tmp_weight_219
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_220 for ONNX node: tmp_weight_220
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_221 for ONNX node: tmp_weight_221
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_222 for ONNX node: tmp_weight_222
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_223 for ONNX node: tmp_weight_223
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_224 for ONNX node: tmp_weight_224
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_225 for ONNX node: tmp_weight_225
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_226 for ONNX node: tmp_weight_226
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_227 for ONNX node: tmp_weight_227
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_228 for ONNX node: tmp_weight_228
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_229 for ONNX node: tmp_weight_229
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_230 for ONNX node: tmp_weight_230
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.conv2.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.conv2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_231 for ONNX node: tmp_weight_231
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_232 for ONNX node: tmp_weight_232
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.weight
[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.bias
[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_var
[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/Add [Add]
[X] Parsing node: /model/encoder/pan_blocks.0/Add [Add]
[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/Add [Add] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/Add for ONNX node: /model/encoder/pan_blocks.0/Add
[X] Registering tensor: /model/encoder/pan_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/Add_output_0
[X] /model/encoder/pan_blocks.0/Add [Add] outputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/Add_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_233 for ONNX node: tmp_weight_233
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_234 for ONNX node: tmp_weight_234
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.0.conv3.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.0.conv3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_235 for ONNX node: tmp_weight_235
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_236 for ONNX node: tmp_weight_236
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0
[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.weight
[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.bias
[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_var
[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0
[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_237 for ONNX node: tmp_weight_237
[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_238 for ONNX node: tmp_weight_238
[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.downsample_convs.1.conv.weight
[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.1.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.downsample_convs.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_239 for ONNX node: tmp_weight_239
[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_240 for ONNX node: tmp_weight_240
[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/downsample_convs.1/conv/Conv for ONNX node: /model/encoder/downsample_convs.1/conv/Conv
[X] Registering tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0
[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/Conv_output_0
[X] Searching for input: model.encoder.downsample_convs.1.norm.weight
[X] Searching for input: model.encoder.downsample_convs.1.norm.bias
[X] Searching for input: model.encoder.downsample_convs.1.norm.running_mean
[X] Searching for input: model.encoder.downsample_convs.1.norm.running_var
[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.downsample_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/downsample_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0
[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0
[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/downsample_convs.1/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.1/act/Sigmoid
[X] Registering tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0
[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/downsample_convs.1/act/Sigmoid_output_0
[X] /model/encoder/downsample_convs.1/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/downsample_convs.1/act/Mul for ONNX node: /model/encoder/downsample_convs.1/act/Mul
[X] Registering tensor: /model/encoder/downsample_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Mul_output_0
[X] /model/encoder/downsample_convs.1/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/Concat_5 [Concat]
[X] Parsing node: /model/encoder/Concat_5 [Concat]
[X] Searching for input: /model/encoder/downsample_convs.1/act/Mul_output_0
[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0
[X] /model/encoder/Concat_5 [Concat] inputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/Concat_5 for ONNX node: /model/encoder/Concat_5
[X] Registering tensor: /model/encoder/Concat_5_output_0 for ONNX tensor: /model/encoder/Concat_5_output_0
[X] /model/encoder/Concat_5 [Concat] outputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/Concat_5_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_241 for ONNX node: tmp_weight_241
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_242 for ONNX node: tmp_weight_242
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.conv1.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.conv1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_243 for ONNX node: tmp_weight_243
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_244 for ONNX node: tmp_weight_244
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.weight
[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.bias
[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_var
[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_245 for ONNX node: tmp_weight_245
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_246 for ONNX node: tmp_weight_246
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_247 for ONNX node: tmp_weight_247
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_248 for ONNX node: tmp_weight_248
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_249 for ONNX node: tmp_weight_249
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_250 for ONNX node: tmp_weight_250
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_251 for ONNX node: tmp_weight_251
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_252 for ONNX node: tmp_weight_252
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_253 for ONNX node: tmp_weight_253
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_254 for ONNX node: tmp_weight_254
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_255 for ONNX node: tmp_weight_255
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_256 for ONNX node: tmp_weight_256
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.conv2.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.conv2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_257 for ONNX node: tmp_weight_257
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], 
[X] Registering layer: tmp_weight_258 for ONNX node: tmp_weight_258
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.weight
[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.bias
[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_var
[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/Add [Add]
[X] Parsing node: /model/encoder/pan_blocks.1/Add [Add]
[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/Add [Add] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/Add for ONNX node: /model/encoder/pan_blocks.1/Add
[X] Registering tensor: /model/encoder/pan_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/Add_output_0
[X] /model/encoder/pan_blocks.1/Add [Add] outputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/Add_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_259 for ONNX node: tmp_weight_259
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_260 for ONNX node: tmp_weight_260
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.encoder.pan_blocks.1.conv3.conv.weight
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.encoder.pan_blocks.1.conv3.conv.weight required by ONNX-TRT
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_261 for ONNX node: tmp_weight_261
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_262 for ONNX node: tmp_weight_262
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/Conv
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0
[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0
[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.weight
[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.bias
[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_mean
[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_var
[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0
[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul]
[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0
[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Mul
[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0
[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.input_proj.0.conv.weight
[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.input_proj.0.conv.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_263 for ONNX node: tmp_weight_263
[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_264 for ONNX node: tmp_weight_264
[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.0/conv/Conv [Conv]
[X] Parsing node: /model/decoder/input_proj.0/conv/Conv [Conv]
[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/decoder/input_proj.0/conv/Conv for ONNX node: /model/decoder/input_proj.0/conv/Conv
[X] Registering tensor: /model/decoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/Conv_output_0
[X] /model/decoder/input_proj.0/conv/Conv [Conv] outputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/decoder/input_proj.0/conv/Conv_output_0
[X] Searching for input: model.decoder.input_proj.0.norm.weight
[X] Searching for input: model.decoder.input_proj.0.norm.bias
[X] Searching for input: model.decoder.input_proj.0.norm.running_mean
[X] Searching for input: model.decoder.input_proj.0.norm.running_var
[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.decoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/decoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.0/norm/BatchNormalization
[X] Registering tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0
[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.input_proj.1.conv.weight
[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.input_proj.1.conv.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_265 for ONNX node: tmp_weight_265
[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_266 for ONNX node: tmp_weight_266
[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.1/conv/Conv [Conv]
[X] Parsing node: /model/decoder/input_proj.1/conv/Conv [Conv]
[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/decoder/input_proj.1/conv/Conv for ONNX node: /model/decoder/input_proj.1/conv/Conv
[X] Registering tensor: /model/decoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/Conv_output_0
[X] /model/decoder/input_proj.1/conv/Conv [Conv] outputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/decoder/input_proj.1/conv/Conv_output_0
[X] Searching for input: model.decoder.input_proj.1.norm.weight
[X] Searching for input: model.decoder.input_proj.1.norm.bias
[X] Searching for input: model.decoder.input_proj.1.norm.running_mean
[X] Searching for input: model.decoder.input_proj.1.norm.running_var
[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.decoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/decoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.1/norm/BatchNormalization
[X] Registering tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0
[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0
[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_267 for ONNX node: tmp_weight_267
[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_268 for ONNX node: tmp_weight_268
[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.input_proj.2.conv.weight
[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.2.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.input_proj.2.conv.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_269 for ONNX node: tmp_weight_269
[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_270 for ONNX node: tmp_weight_270
[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/conv/Conv [Conv]
[X] Parsing node: /model/decoder/input_proj.2/conv/Conv [Conv]
[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/input_proj.2/conv/Conv [Conv] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], 
[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.
[X] Registering layer: /model/decoder/input_proj.2/conv/Conv for ONNX node: /model/decoder/input_proj.2/conv/Conv
[X] Registering tensor: /model/decoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/Conv_output_0
[X] /model/decoder/input_proj.2/conv/Conv [Conv] outputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization]
[X] Parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization]
[X] Searching for input: /model/decoder/input_proj.2/conv/Conv_output_0
[X] Searching for input: model.decoder.input_proj.2.norm.weight
[X] Searching for input: model.decoder.input_proj.2.norm.bias
[X] Searching for input: model.decoder.input_proj.2.norm.running_mean
[X] Searching for input: model.decoder.input_proj.2.norm.running_var
[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.decoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_var -> (256)[FLOAT]], 
[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.
[X] Registering layer: /model/decoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.2/norm/BatchNormalization
[X] Registering tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0
[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Reshape [Reshape]
[X] Parsing node: /model/decoder/Reshape [Reshape]
[X] Searching for input: /model/decoder/input_proj.0/norm/BatchNormalization_output_0
[X] Searching for input: _v_4326
[X] /model/decoder/Reshape [Reshape] inputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [_v_4326 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_271 required by ONNX-TRT
[X] Registering layer: /model/decoder/Reshape for ONNX node: /model/decoder/Reshape
[X] Registering tensor: /model/decoder/Reshape_output_0 for ONNX tensor: /model/decoder/Reshape_output_0
[X] /model/decoder/Reshape [Reshape] outputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Transpose [Transpose]
[X] Parsing node: /model/decoder/Transpose [Transpose]
[X] Searching for input: /model/decoder/Reshape_output_0
[X] /model/decoder/Transpose [Transpose] inputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], 
[X] Registering layer: /model/decoder/Transpose for ONNX node: /model/decoder/Transpose
[X] Registering tensor: /model/decoder/Transpose_output_0 for ONNX tensor: /model/decoder/Transpose_output_0
[X] /model/decoder/Transpose [Transpose] outputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/input_proj.1/norm/BatchNormalization_output_0
[X] Searching for input: _v_4326
[X] /model/decoder/Reshape_1 [Reshape] inputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [_v_4326 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_272 required by ONNX-TRT
[X] Registering layer: /model/decoder/Reshape_1 for ONNX node: /model/decoder/Reshape_1
[X] Registering tensor: /model/decoder/Reshape_1_output_0 for ONNX tensor: /model/decoder/Reshape_1_output_0
[X] /model/decoder/Reshape_1 [Reshape] outputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/Reshape_1_output_0
[X] /model/decoder/Transpose_1 [Transpose] inputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], 
[X] Registering layer: /model/decoder/Transpose_1 for ONNX node: /model/decoder/Transpose_1
[X] Registering tensor: /model/decoder/Transpose_1_output_0 for ONNX tensor: /model/decoder/Transpose_1_output_0
[X] /model/decoder/Transpose_1 [Transpose] outputs: [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/input_proj.2/norm/BatchNormalization_output_0
[X] Searching for input: _v_4326
[X] /model/decoder/Reshape_2 [Reshape] inputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_273 required by ONNX-TRT
[X] Registering layer: /model/decoder/Reshape_2 for ONNX node: /model/decoder/Reshape_2
[X] Registering tensor: /model/decoder/Reshape_2_output_0 for ONNX tensor: /model/decoder/Reshape_2_output_0
[X] /model/decoder/Reshape_2 [Reshape] outputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/Reshape_2_output_0
[X] /model/decoder/Transpose_2 [Transpose] inputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], 
[X] Registering layer: /model/decoder/Transpose_2 for ONNX node: /model/decoder/Transpose_2
[X] Registering tensor: /model/decoder/Transpose_2_output_0 for ONNX tensor: /model/decoder/Transpose_2_output_0
[X] /model/decoder/Transpose_2 [Transpose] outputs: [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Concat_3 [Concat]
[X] Parsing node: /model/decoder/Concat_3 [Concat]
[X] Searching for input: /model/decoder/Transpose_output_0
[X] Searching for input: /model/decoder/Transpose_1_output_0
[X] Searching for input: /model/decoder/Transpose_2_output_0
[X] /model/decoder/Concat_3 [Concat] inputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/Concat_3 for ONNX node: /model/decoder/Concat_3
[X] Registering tensor: /model/decoder/Concat_3_output_0 for ONNX tensor: /model/decoder/Concat_3_output_0
[X] /model/decoder/Concat_3 [Concat] outputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Mul [Mul]
[X] Parsing node: /model/decoder/Mul [Mul]
[X] Searching for input: onnx::Mul_3692
[X] Searching for input: /model/decoder/Concat_3_output_0
[X] /model/decoder/Mul [Mul] inputs: [onnx::Mul_3692 -> (1, 8400, 1)[FLOAT]], [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: onnx::Mul_3692 required by ONNX-TRT
[X] Registering layer: /model/decoder/Mul for ONNX node: /model/decoder/Mul
[X] Registering tensor: /model/decoder/Mul_output_0 for ONNX tensor: /model/decoder/Mul_output_0
[X] /model/decoder/Mul [Mul] outputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/Mul_output_0
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_274 for ONNX node: tmp_weight_274
[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_275 for ONNX node: tmp_weight_275
[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_output.proj.weight
[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_output.proj.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.enc_output.proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_276 for ONNX node: tmp_weight_276
[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_277 for ONNX node: tmp_weight_277
[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_output/proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_output/proj/Transpose [Transpose] inputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_output/proj/Transpose for ONNX node: /model/decoder/enc_output/proj/Transpose
[X] Registering tensor: /model/decoder/enc_output/proj/Transpose_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Transpose_output_0
[X] /model/decoder/enc_output/proj/Transpose [Transpose] outputs: [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_output/proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_output/proj/Transpose_output_0
[X] /model/decoder/enc_output/proj/MatMul [MatMul] inputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_278 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_279 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_output/proj/MatMul for ONNX node: /model/decoder/enc_output/proj/MatMul
[X] Registering tensor: /model/decoder/enc_output/proj/MatMul_output_0 for ONNX tensor: /model/decoder/enc_output/proj/MatMul_output_0
[X] /model/decoder/enc_output/proj/MatMul [MatMul] outputs: [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/proj/Add [Add]
[X] Parsing node: /model/decoder/enc_output/proj/Add [Add]
[X] Searching for input: model.decoder.enc_output.proj.bias
[X] Searching for input: /model/decoder/enc_output/proj/MatMul_output_0
[X] /model/decoder/enc_output/proj/Add [Add] inputs: [model.decoder.enc_output.proj.bias -> (256)[FLOAT]], [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.enc_output.proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_280 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_281 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_output/proj/Add for ONNX node: /model/decoder/enc_output/proj/Add
[X] Registering tensor: /model/decoder/enc_output/proj/Add_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Add_output_0
[X] /model/decoder/enc_output/proj/Add [Add] outputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/enc_output/proj/Add_output_0
[X] Searching for input: model.decoder.enc_output.norm.weight
[X] Searching for input: model.decoder.enc_output.norm.bias
[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] inputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [model.decoder.enc_output.norm.weight -> (256)[FLOAT]], [model.decoder.enc_output.norm.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.enc_output.norm.weight required by ONNX-TRT
[X] Registering layer: model.decoder.enc_output.norm.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_284 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_285 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_286 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_287 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_output/norm/LayerNormalization for ONNX node: /model/decoder/enc_output/norm/LayerNormalization
[X] Registering tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0 for ONNX tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0
[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] outputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_288 for ONNX node: tmp_weight_288
[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_289 for ONNX node: tmp_weight_289
[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_score_head.weight
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_score_head.weight -> (80, 256)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], 
[X] Registering layer: model.decoder.enc_score_head.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_290 for ONNX node: tmp_weight_290
[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], 
[X] Registering layer: tmp_weight_291 for ONNX node: tmp_weight_291
[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_score_head/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_score_head/Transpose [Transpose] inputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_score_head/Transpose for ONNX node: /model/decoder/enc_score_head/Transpose
[X] Registering tensor: /model/decoder/enc_score_head/Transpose_output_0 for ONNX tensor: /model/decoder/enc_score_head/Transpose_output_0
[X] /model/decoder/enc_score_head/Transpose [Transpose] outputs: [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_score_head/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_score_head/Transpose_output_0
[X] /model/decoder/enc_score_head/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_292 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_293 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_score_head/MatMul for ONNX node: /model/decoder/enc_score_head/MatMul
[X] Registering tensor: /model/decoder/enc_score_head/MatMul_output_0 for ONNX tensor: /model/decoder/enc_score_head/MatMul_output_0
[X] /model/decoder/enc_score_head/MatMul [MatMul] outputs: [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_score_head/Add [Add]
[X] Parsing node: /model/decoder/enc_score_head/Add [Add]
[X] Searching for input: model.decoder.enc_score_head.bias
[X] Searching for input: /model/decoder/enc_score_head/MatMul_output_0
[X] /model/decoder/enc_score_head/Add [Add] inputs: [model.decoder.enc_score_head.bias -> (80)[FLOAT]], [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], 
[X] Registering layer: model.decoder.enc_score_head.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_294 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_295 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_score_head/Add for ONNX node: /model/decoder/enc_score_head/Add
[X] Registering tensor: /model/decoder/enc_score_head/Add_output_0 for ONNX tensor: /model/decoder/enc_score_head/Add_output_0
[X] /model/decoder/enc_score_head/Add [Add] outputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_bbox_head.layers.0.weight
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_296 for ONNX node: tmp_weight_296
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_297 for ONNX node: tmp_weight_297
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.0/Transpose
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_298 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_299 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.0/MatMul
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add]
[X] Searching for input: model.decoder.enc_bbox_head.layers.0.bias
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.0.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_300 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_301 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Add for ONNX node: /model/decoder/enc_bbox_head/layers.0/Add
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0
[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu]
[X] Parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Add_output_0
[X] /model/decoder/enc_bbox_head/act/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/act/Relu for ONNX node: /model/decoder/enc_bbox_head/act/Relu
[X] Registering tensor: /model/decoder/enc_bbox_head/act/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act/Relu_output_0
[X] /model/decoder/enc_bbox_head/act/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/act/Relu_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_302 for ONNX node: tmp_weight_302
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_303 for ONNX node: tmp_weight_303
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_bbox_head.layers.1.weight
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_304 for ONNX node: tmp_weight_304
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_305 for ONNX node: tmp_weight_305
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.1/Transpose
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_306 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_307 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.1/MatMul
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add]
[X] Searching for input: model.decoder.enc_bbox_head.layers.1.bias
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_308 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_309 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Add for ONNX node: /model/decoder/enc_bbox_head/layers.1/Add
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0
[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Add_output_0
[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/act_1/Relu for ONNX node: /model/decoder/enc_bbox_head/act_1/Relu
[X] Registering tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0
[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/act_1/Relu_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_310 for ONNX node: tmp_weight_310
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_311 for ONNX node: tmp_weight_311
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.enc_bbox_head.layers.2.weight
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_312 for ONNX node: tmp_weight_312
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: tmp_weight_313 for ONNX node: tmp_weight_313
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.2/Transpose
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0
[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_314 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_315 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.2/MatMul
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add]
[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add]
[X] Searching for input: model.decoder.enc_bbox_head.layers.2.bias
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0
[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.2.bias -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], 
[X] Registering layer: model.decoder.enc_bbox_head.layers.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_316 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_317 required by ONNX-TRT
[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Add for ONNX node: /model/decoder/enc_bbox_head/layers.2/Add
[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0
[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Add [Add]
[X] Parsing node: /model/decoder/Add [Add]
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Add_output_0
[X] Searching for input: model.decoder.anchors
[X] /model/decoder/Add [Add] inputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], [model.decoder.anchors -> (1, 8400, 4)[FLOAT]], 
[X] Registering layer: model.decoder.anchors required by ONNX-TRT
[X] Registering layer: /model/decoder/Add for ONNX node: /model/decoder/Add
[X] Registering tensor: /model/decoder/Add_output_0 for ONNX tensor: /model/decoder/Add_output_0
[X] /model/decoder/Add [Add] outputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/ReduceMax [ReduceMax]
[X] Parsing node: /model/decoder/ReduceMax [ReduceMax]
[X] Searching for input: /model/decoder/enc_score_head/Add_output_0
[X] /model/decoder/ReduceMax [ReduceMax] inputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], 
[X] Registering layer: /model/decoder/ReduceMax for ONNX node: /model/decoder/ReduceMax
[X] Registering tensor: /model/decoder/ReduceMax_output_0 for ONNX tensor: /model/decoder/ReduceMax_output_0
[X] /model/decoder/ReduceMax [ReduceMax] outputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/TopK [TopK]
[X] Parsing node: /model/decoder/TopK [TopK]
[X] Searching for input: /model/decoder/ReduceMax_output_0
[X] Searching for input: /model/decoder/Constant_18_output_0
[X] /model/decoder/TopK [TopK] inputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/Constant_18_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_convertToScalar required by ONNX-TRT
[X] Registering layer: /model/decoder/TopK for ONNX node: /model/decoder/TopK
[X] Registering layer: ONNXTRT_castHelper required by ONNX-TRT
[X] Registering tensor: /model/decoder/TopK_output_0 for ONNX tensor: /model/decoder/TopK_output_0
[X] Registering tensor: /model/decoder/TopK_output_1 for ONNX tensor: /model/decoder/TopK_output_1
[X] /model/decoder/TopK [TopK] outputs: [/model/decoder/TopK_output_0 -> (1, 300)[FLOAT]], [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], 
[X] Static check for parsing node: /model/decoder/Unsqueeze [Unsqueeze]
[X] Parsing node: /model/decoder/Unsqueeze [Unsqueeze]
[X] Searching for input: /model/decoder/TopK_output_1
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/Unsqueeze [Unsqueeze] inputs: [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/encoder/Constant_7_output_0 required by ONNX-TRT
[X] Registering layer: /model/decoder/Unsqueeze for ONNX node: /model/decoder/Unsqueeze
[X] Registering tensor: /model/decoder/Unsqueeze_output_0 for ONNX tensor: /model/decoder/Unsqueeze_output_0
[X] /model/decoder/Unsqueeze [Unsqueeze] outputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], 
[X] Static check for parsing node: /model/decoder/Tile [Tile]
[X] Parsing node: /model/decoder/Tile [Tile]
[X] Searching for input: /model/decoder/Unsqueeze_output_0
[X] Searching for input: /model/decoder/Concat_5_output_0
[X] /model/decoder/Tile [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice required by ONNX-TRT
[X] Registering layer: /model/decoder/Tile for ONNX node: /model/decoder/Tile
[X] Registering tensor: /model/decoder/Tile_output_0 for ONNX tensor: /model/decoder/Tile_output_0
[X] /model/decoder/Tile [Tile] outputs: [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], 
[X] Static check for parsing node: /model/decoder/GatherElements [GatherElements]
[X] Parsing node: /model/decoder/GatherElements [GatherElements]
[X] Searching for input: /model/decoder/Add_output_0
[X] Searching for input: /model/decoder/Tile_output_0
[X] /model/decoder/GatherElements [GatherElements] inputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], 
[X] Using Gather axis: 1
[X] Registering layer: ONNXTRT_castHelper_318 required by ONNX-TRT
[X] Registering layer: /model/decoder/GatherElements for ONNX node: /model/decoder/GatherElements
[X] Registering tensor: /model/decoder/GatherElements_output_0 for ONNX tensor: /model/decoder/GatherElements_output_0
[X] /model/decoder/GatherElements [GatherElements] outputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Tile_1 [Tile]
[X] Parsing node: /model/decoder/Tile_1 [Tile]
[X] Searching for input: /model/decoder/Unsqueeze_output_0
[X] Searching for input: /model/decoder/Concat_7_output_0
[X] /model/decoder/Tile_1 [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_7_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_319 required by ONNX-TRT
[X] Registering layer: /model/decoder/Tile_1 for ONNX node: /model/decoder/Tile_1
[X] Registering tensor: /model/decoder/Tile_1_output_0 for ONNX tensor: /model/decoder/Tile_1_output_0
[X] /model/decoder/Tile_1 [Tile] outputs: [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], 
[X] Static check for parsing node: /model/decoder/GatherElements_1 [GatherElements]
[X] Parsing node: /model/decoder/GatherElements_1 [GatherElements]
[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0
[X] Searching for input: /model/decoder/Tile_1_output_0
[X] /model/decoder/GatherElements_1 [GatherElements] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], 
[X] Using Gather axis: 1
[X] Registering layer: ONNXTRT_castHelper_320 required by ONNX-TRT
[X] Registering layer: /model/decoder/GatherElements_1 for ONNX node: /model/decoder/GatherElements_1
[X] Registering tensor: /model/decoder/GatherElements_1_output_0 for ONNX tensor: /model/decoder/GatherElements_1_output_0
[X] /model/decoder/GatherElements_1 [GatherElements] outputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sigmoid [Sigmoid]
[X] Parsing node: /model/decoder/decoder/Sigmoid [Sigmoid]
[X] Searching for input: /model/decoder/GatherElements_output_0
[X] /model/decoder/decoder/Sigmoid [Sigmoid] inputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Sigmoid for ONNX node: /model/decoder/decoder/Sigmoid
[X] Registering tensor: /model/decoder/decoder/Sigmoid_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_output_0
[X] /model/decoder/decoder/Sigmoid [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_321 for ONNX node: tmp_weight_321
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_322 for ONNX node: tmp_weight_322
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.query_pos_head.layers.0.weight
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.0.weight -> (512, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: model.decoder.query_pos_head.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_323 for ONNX node: tmp_weight_323
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], 
[X] Registering layer: tmp_weight_324 for ONNX node: tmp_weight_324
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Transpose
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_325 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_326 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.0.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: model.decoder.query_pos_head.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_327 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_328 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0
[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/act/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act/Relu
[X] Registering tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0
[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/act/Relu_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_329 for ONNX node: tmp_weight_329
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_330 for ONNX node: tmp_weight_330
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.query_pos_head.layers.1.weight
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.1.weight -> (256, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.query_pos_head.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_331 for ONNX node: tmp_weight_331
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_332 for ONNX node: tmp_weight_332
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Transpose
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_333 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_334 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.1.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.query_pos_head.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_335 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_336 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add [Add]
[X] Searching for input: /model/decoder/GatherElements_1_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0
[X] /model/decoder/decoder/layers.0/Add [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add for ONNX node: /model/decoder/decoder/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_output_0
[X] /model/decoder/decoder/layers.0/Add [Add] outputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/GatherElements_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3736
[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3736 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3736 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_337 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_338 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul
[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add]
[X] Searching for input: onnx::Add_3731
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3731 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3731 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_339 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_340 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3737
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3737 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3737 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_341 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_342 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_1
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add]
[X] Searching for input: onnx::Add_3733
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3733 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3733 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_343 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_344 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0
[X] Searching for input: onnx::MatMul_3738
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3738 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3738 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_345 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_346 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_2
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add]
[X] Searching for input: onnx::Add_3735
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3735 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3735 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_347 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_348 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_349 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_350 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_351 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_352 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_353 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_4
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_3
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/self_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_354 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_4
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_5
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0
[X] Searching for input: _v_1857
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1857 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_355 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_3
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0
[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.weight
[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.bias
[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.weight required by ONNX-TRT
[X] Using opA: 0 opB: 1
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.0/self_attn/Gemm
[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_356 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_357 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_358 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_6
[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add_1 [Add]
[X] Searching for input: /model/decoder/GatherElements_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.0/Add_1 [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add_1 for ONNX node: /model/decoder/decoder/layers.0/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_1_output_0
[X] /model/decoder/decoder/layers.0/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_1_output_0
[X] Searching for input: model.decoder.decoder.layers.0.norm1.weight
[X] Searching for input: model.decoder.decoder.layers.0.norm1.bias
[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.norm1.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.0.norm1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_361 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_362 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_363 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_364 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm1/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add_2 [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0
[X] /model/decoder/decoder/layers.0/Add_2 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add_2 for ONNX node: /model/decoder/decoder/layers.0/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_2_output_0
[X] /model/decoder/decoder/layers.0/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/Concat_3_output_0
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_365 for ONNX node: tmp_weight_365
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_366 for ONNX node: tmp_weight_366
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_367 for ONNX node: tmp_weight_367
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_368 for ONNX node: tmp_weight_368
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_369 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_370 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_371 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_372 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0
[X] Searching for input: _v_1859
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1859 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_373 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_374 for ONNX node: tmp_weight_374
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_375 for ONNX node: tmp_weight_375
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_376 for ONNX node: tmp_weight_376
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: tmp_weight_377 for ONNX node: tmp_weight_377
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_378 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_379 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_380 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_381 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0_382 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[X] Searching for input: _v_1674
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1674 -> (5)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_383 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_384 for ONNX node: tmp_weight_384
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: tmp_weight_385 for ONNX node: tmp_weight_385
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_386 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_387 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_388 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_389 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
[X] Searching for input: _v_1676
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1676 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_390 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_391 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0
[X] Searching for input: onnx::Mul_3755
[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], 
[X] Registering layer: onnx::Mul_3755 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_392 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_393 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0
[X] Searching for input: _v_2002
[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [_v_2002 -> (2)[INT64]], 
[X] Registering layer: _v_2002 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/Constant_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_394 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_395 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_398 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_399 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_401 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_402 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_403 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_405 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_406 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_407 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_409 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_410 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_412 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_413 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_414 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_415 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_417 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_418 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_419 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_420 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_422 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_423 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_424 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_425 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_426 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_2
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: onnx::Unsqueeze_1255 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_427 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_428 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_430 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_431 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_433 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_434 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_436 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_437 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_438 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_440 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_441 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_442 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_444 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_445 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_447 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_448 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_449 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_450 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_452 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_453 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_454 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_455 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_457 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_458 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_459 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0
[X] Searching for input: _v_1760
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1760 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_460 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_461 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_462 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_464 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_465 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_467 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_468 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_470 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_471 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_472 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_474 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_475 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_476 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_478 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_479 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_481 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_482 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_483 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_484 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_486 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_487 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_488 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_489 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_491 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_492 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_493 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_4
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_494 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_495 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_497 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_498 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_500 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_501 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_503 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_504 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_505 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_507 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_508 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_509 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_511 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_512 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_514 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_515 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_516 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_517 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_519 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_520 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_521 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_522 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_524 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_525 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_526 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_5
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_527 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_528 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_530 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_531 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_533 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_534 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_536 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_537 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_538 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_540 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_541 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_542 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_544 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_545 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_547 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_548 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_549 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_550 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_552 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_553 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_554 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_555 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_557 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_558 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_559 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_6
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_560 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_561 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_6
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_562 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_563 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Sub
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0
[X] Searching for input: _v_1861
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1861 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_564 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0
[X] Searching for input: onnx::Split_2305
[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_565 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_566 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_567 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_568 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_569 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_570 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_571 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_1
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_572 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_573 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Concat_10
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_8
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_574 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_575 for ONNX node: tmp_weight_575
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_576 for ONNX node: tmp_weight_576
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_577 for ONNX node: tmp_weight_577
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_578 for ONNX node: tmp_weight_578
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_579 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_580 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_581 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_582 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_3 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add_3 [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.0/Add_3 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add_3 for ONNX node: /model/decoder/decoder/layers.0/Add_3
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_3_output_0
[X] /model/decoder/decoder/layers.0/Add_3 [Add] outputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_3_output_0
[X] Searching for input: model.decoder.decoder.layers.0.norm2.weight
[X] Searching for input: model.decoder.decoder.layers.0.norm2.bias
[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.norm2.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.0.norm2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_585 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_586 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_587 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_588 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm2/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_589 for ONNX node: tmp_weight_589
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_590 for ONNX node: tmp_weight_590
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.linear1.weight
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.linear1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_591 for ONNX node: tmp_weight_591
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: tmp_weight_592 for ONNX node: tmp_weight_592
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear1/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_593 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_594 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear1/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.linear1.bias
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.0/linear1/Add [Add] inputs: [model.decoder.decoder.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.linear1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_595 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_596 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Add for ONNX node: /model/decoder/decoder/layers.0/linear1/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0
[X] /model/decoder/decoder/layers.0/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Add_output_0
[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/activation/Relu for ONNX node: /model/decoder/decoder/layers.0/activation/Relu
[X] Registering tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0
[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/activation/Relu_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_597 for ONNX node: tmp_weight_597
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_598 for ONNX node: tmp_weight_598
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.0.linear2.weight
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.0.linear2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_599 for ONNX node: tmp_weight_599
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_600 for ONNX node: tmp_weight_600
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear2/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_601 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_602 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear2/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.0.linear2.bias
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.0/linear2/Add [Add] inputs: [model.decoder.decoder.layers.0.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.linear2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_603 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_604 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Add for ONNX node: /model/decoder/decoder/layers.0/linear2/Add
[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0
[X] /model/decoder/decoder/layers.0/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_4 [Add]
[X] Parsing node: /model/decoder/decoder/layers.0/Add_4 [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Add_output_0
[X] /model/decoder/decoder/layers.0/Add_4 [Add] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.0/Add_4 for ONNX node: /model/decoder/decoder/layers.0/Add_4
[X] Registering tensor: /model/decoder/decoder/layers.0/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_4_output_0
[X] /model/decoder/decoder/layers.0/Add_4 [Add] outputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.0/Add_4_output_0
[X] Searching for input: model.decoder.decoder.layers.0.norm3.weight
[X] Searching for input: model.decoder.decoder.layers.0.norm3.bias
[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.0.norm3.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.0.norm3.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_607 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_608 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_609 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_610 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm3/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_611 for ONNX node: tmp_weight_611
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_612 for ONNX node: tmp_weight_612
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_613 for ONNX node: tmp_weight_613
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_614 for ONNX node: tmp_weight_614
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_615 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_616 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_617 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_618 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_619 for ONNX node: tmp_weight_619
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_620 for ONNX node: tmp_weight_620
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_621 for ONNX node: tmp_weight_621
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_622 for ONNX node: tmp_weight_622
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_623 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_624 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_625 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_626 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_627 for ONNX node: tmp_weight_627
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_628 for ONNX node: tmp_weight_628
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_629 for ONNX node: tmp_weight_629
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: tmp_weight_630 for ONNX node: tmp_weight_630
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_631 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_632 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_633 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_634 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip [Clip]
[X] Parsing node: /model/decoder/decoder/Clip [Clip]
[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0
[X] Searching for input: /model/decoder/decoder/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/Clip [Clip] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Clip for ONNX node: /model/decoder/decoder/Clip
[X] Registering tensor: /model/decoder/decoder/Clip_output_0 for ONNX tensor: /model/decoder/decoder/Clip_output_0
[X] /model/decoder/decoder/Clip [Clip] outputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_1 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_1 [Clip]
[X] Searching for input: /model/decoder/decoder/Clip_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_1 [Clip] inputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: /model/decoder/decoder/Constant_3_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_636 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_637 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_638 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_639 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_1_output_0 for ONNX tensor: /model/decoder/decoder/Clip_1_output_0
[X] /model/decoder/decoder/Clip_1 [Clip] outputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sub [Sub]
[X] Parsing node: /model/decoder/decoder/Sub [Sub]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/Clip_output_0
[X] /model/decoder/decoder/Sub [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_640 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_641 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/Sub for ONNX node: /model/decoder/decoder/Sub
[X] Registering tensor: /model/decoder/decoder/Sub_output_0 for ONNX tensor: /model/decoder/decoder/Sub_output_0
[X] /model/decoder/decoder/Sub [Sub] outputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_2 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_2 [Clip]
[X] Searching for input: /model/decoder/decoder/Sub_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_2 [Clip] inputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_643 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_644 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_645 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_646 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_2_output_0 for ONNX tensor: /model/decoder/decoder/Clip_2_output_0
[X] /model/decoder/decoder/Clip_2 [Clip] outputs: [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Div [Div]
[X] Parsing node: /model/decoder/decoder/Div [Div]
[X] Searching for input: /model/decoder/decoder/Clip_1_output_0
[X] Searching for input: /model/decoder/decoder/Clip_2_output_0
[X] /model/decoder/decoder/Div [Div] inputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Div for ONNX node: /model/decoder/decoder/Div
[X] Registering tensor: /model/decoder/decoder/Div_output_0 for ONNX tensor: /model/decoder/decoder/Div_output_0
[X] /model/decoder/decoder/Div [Div] outputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Log [Log]
[X] Parsing node: /model/decoder/decoder/Log [Log]
[X] Searching for input: /model/decoder/decoder/Div_output_0
[X] /model/decoder/decoder/Log [Log] inputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Log for ONNX node: /model/decoder/decoder/Log
[X] Registering tensor: /model/decoder/decoder/Log_output_0 for ONNX tensor: /model/decoder/decoder/Log_output_0
[X] /model/decoder/decoder/Log [Log] outputs: [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Add [Add]
[X] Parsing node: /model/decoder/decoder/Add [Add]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0
[X] Searching for input: /model/decoder/decoder/Log_output_0
[X] /model/decoder/decoder/Add [Add] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Add for ONNX node: /model/decoder/decoder/Add
[X] Registering tensor: /model/decoder/decoder/Add_output_0 for ONNX tensor: /model/decoder/decoder/Add_output_0
[X] /model/decoder/decoder/Add [Add] outputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid]
[X] Parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid]
[X] Searching for input: /model/decoder/decoder/Add_output_0
[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] inputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Sigmoid_1 for ONNX node: /model/decoder/decoder/Sigmoid_1
[X] Registering tensor: /model/decoder/decoder/Sigmoid_1_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_1_output_0
[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_647 for ONNX node: tmp_weight_647
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_648 for ONNX node: tmp_weight_648
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_649 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_650 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.0.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_651 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_652 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0
[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/act_1/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_1/Relu
[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0
[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_653 for ONNX node: tmp_weight_653
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_654 for ONNX node: tmp_weight_654
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_655 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_656 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.1.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_657 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_658 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0
[X] /model/decoder/decoder/layers.1/Add [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add for ONNX node: /model/decoder/decoder/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_output_0
[X] /model/decoder/decoder/layers.1/Add [Add] outputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3808
[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3808 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3808 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_659 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_660 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul
[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add]
[X] Searching for input: onnx::Add_3803
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] inputs: [onnx::Add_3803 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3803 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_661 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_662 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3809
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3809 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3809 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_663 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_664 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_1
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add]
[X] Searching for input: onnx::Add_3805
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] inputs: [onnx::Add_3805 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3805 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_665 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_666 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0
[X] Searching for input: onnx::MatMul_3810
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3810 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3810 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_667 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_668 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_2
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add]
[X] Searching for input: onnx::Add_3807
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] inputs: [onnx::Add_3807 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3807 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_669 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_670 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_output_0
[X] Searching for input: _v_1680
[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1680 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_671 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0
[X] Searching for input: _v_1680
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1680 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_672 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0
[X] Searching for input: _v_1680
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1680 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_673 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_674 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_675 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_4
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_3
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/self_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_676 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_4
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_5
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0
[X] Searching for input: _v_1857
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1857 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_677 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_3
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0
[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.weight
[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.bias
[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.weight required by ONNX-TRT
[X] Using opA: 0 opB: 1
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.1/self_attn/Gemm
[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_678 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_679 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0
[X] Searching for input: _v_1686
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1686 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_680 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_6
[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add_1 [Add]
[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.1/Add_1 [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add_1 for ONNX node: /model/decoder/decoder/layers.1/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_1_output_0
[X] /model/decoder/decoder/layers.1/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_1_output_0
[X] Searching for input: model.decoder.decoder.layers.1.norm1.weight
[X] Searching for input: model.decoder.decoder.layers.1.norm1.bias
[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.norm1.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.1.norm1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_683 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_684 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_685 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_686 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm1/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add_2 [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0
[X] /model/decoder/decoder/layers.1/Add_2 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add_2 for ONNX node: /model/decoder/decoder/layers.1/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_2_output_0
[X] /model/decoder/decoder/layers.1/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_687 for ONNX node: tmp_weight_687
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_688 for ONNX node: tmp_weight_688
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_689 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_690 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_691 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_692 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0
[X] Searching for input: _v_1859
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1859 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_693 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_694 for ONNX node: tmp_weight_694
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_695 for ONNX node: tmp_weight_695
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_696 for ONNX node: tmp_weight_696
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: tmp_weight_697 for ONNX node: tmp_weight_697
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_698 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_699 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_700 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_701 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0_702 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[X] Searching for input: _v_1674
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1674 -> (5)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_703 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_704 for ONNX node: tmp_weight_704
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: tmp_weight_705 for ONNX node: tmp_weight_705
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_706 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_707 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_708 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_709 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
[X] Searching for input: _v_1676
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1676 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_710 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_711 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0
[X] Searching for input: onnx::Mul_3755
[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_712 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_713 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0
[X] Searching for input: _v_2002
[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [_v_2002 -> (2)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_714 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_715 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_717 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_718 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_720 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_721 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_723 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_724 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_725 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_727 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_728 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_729 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_731 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_732 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_734 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_735 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_736 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_737 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_739 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_740 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_741 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_742 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_744 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_745 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_746 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_747 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_748 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_2
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_749 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_750 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_752 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_753 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_755 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_756 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_758 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_759 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_760 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_762 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_763 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_764 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_766 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_767 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_769 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_770 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_771 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_772 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_774 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_775 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_776 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_777 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_779 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_780 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_781 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0
[X] Searching for input: _v_1760
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1760 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_782 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_783 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_784 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_786 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_787 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_789 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_790 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_792 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_793 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_794 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_796 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_797 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_798 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_800 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_801 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_803 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_804 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_805 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_806 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_808 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_809 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_810 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_811 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_813 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_814 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_815 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_4
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_816 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_817 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_819 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_820 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_822 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_823 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_825 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_826 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_827 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_829 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_830 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_831 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_833 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_834 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_836 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_837 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_838 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_839 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_841 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_842 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_843 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_844 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_846 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_847 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_848 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_5
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_849 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_850 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_852 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_853 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_855 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_856 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_858 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_859 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_860 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_862 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_863 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_864 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_866 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_867 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_869 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_870 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_871 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_872 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_874 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_875 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_876 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_877 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_879 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_880 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_881 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_6
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Add_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_882 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_883 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_3
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_884 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_885 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Sub
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0
[X] Searching for input: _v_1861
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1861 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_886 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0
[X] Searching for input: onnx::Split_2305
[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_887 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_888 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_889 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_890 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_891 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_892 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_893 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_1
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_894 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_895 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Concat_10
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_5
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_896 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_897 for ONNX node: tmp_weight_897
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_898 for ONNX node: tmp_weight_898
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_899 for ONNX node: tmp_weight_899
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_900 for ONNX node: tmp_weight_900
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_901 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_902 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_903 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_904 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_3 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add_3 [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.1/Add_3 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add_3 for ONNX node: /model/decoder/decoder/layers.1/Add_3
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_3_output_0
[X] /model/decoder/decoder/layers.1/Add_3 [Add] outputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_3_output_0
[X] Searching for input: model.decoder.decoder.layers.1.norm2.weight
[X] Searching for input: model.decoder.decoder.layers.1.norm2.bias
[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.norm2.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.1.norm2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_907 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_908 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_909 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_910 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm2/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_911 for ONNX node: tmp_weight_911
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_912 for ONNX node: tmp_weight_912
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.linear1.weight
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.linear1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_913 for ONNX node: tmp_weight_913
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: tmp_weight_914 for ONNX node: tmp_weight_914
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear1/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_915 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_916 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear1/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.linear1.bias
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.1/linear1/Add [Add] inputs: [model.decoder.decoder.layers.1.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.linear1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_917 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_918 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Add for ONNX node: /model/decoder/decoder/layers.1/linear1/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0
[X] /model/decoder/decoder/layers.1/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Add_output_0
[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/activation/Relu for ONNX node: /model/decoder/decoder/layers.1/activation/Relu
[X] Registering tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0
[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/activation/Relu_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_919 for ONNX node: tmp_weight_919
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_920 for ONNX node: tmp_weight_920
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.1.linear2.weight
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.1.linear2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_921 for ONNX node: tmp_weight_921
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_922 for ONNX node: tmp_weight_922
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear2/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_923 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_924 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear2/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.1.linear2.bias
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.1/linear2/Add [Add] inputs: [model.decoder.decoder.layers.1.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.linear2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_925 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_926 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Add for ONNX node: /model/decoder/decoder/layers.1/linear2/Add
[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0
[X] /model/decoder/decoder/layers.1/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_4 [Add]
[X] Parsing node: /model/decoder/decoder/layers.1/Add_4 [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Add_output_0
[X] /model/decoder/decoder/layers.1/Add_4 [Add] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.1/Add_4 for ONNX node: /model/decoder/decoder/layers.1/Add_4
[X] Registering tensor: /model/decoder/decoder/layers.1/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_4_output_0
[X] /model/decoder/decoder/layers.1/Add_4 [Add] outputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.1/Add_4_output_0
[X] Searching for input: model.decoder.decoder.layers.1.norm3.weight
[X] Searching for input: model.decoder.decoder.layers.1.norm3.bias
[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.1.norm3.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.1.norm3.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_929 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_930 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_931 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_932 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm3/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_933 for ONNX node: tmp_weight_933
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_934 for ONNX node: tmp_weight_934
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_935 for ONNX node: tmp_weight_935
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_936 for ONNX node: tmp_weight_936
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_937 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_938 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_939 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_940 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_941 for ONNX node: tmp_weight_941
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_942 for ONNX node: tmp_weight_942
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_943 for ONNX node: tmp_weight_943
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_944 for ONNX node: tmp_weight_944
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_945 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_946 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_947 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_948 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_949 for ONNX node: tmp_weight_949
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_950 for ONNX node: tmp_weight_950
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_951 for ONNX node: tmp_weight_951
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: tmp_weight_952 for ONNX node: tmp_weight_952
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_953 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_954 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_955 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_956 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_3 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_3 [Clip]
[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0
[X] Searching for input: /model/decoder/decoder/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/Clip_3 [Clip] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Clip_3 for ONNX node: /model/decoder/decoder/Clip_3
[X] Registering tensor: /model/decoder/decoder/Clip_3_output_0 for ONNX tensor: /model/decoder/decoder/Clip_3_output_0
[X] /model/decoder/decoder/Clip_3 [Clip] outputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_4 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_4 [Clip]
[X] Searching for input: /model/decoder/decoder/Clip_3_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_4 [Clip] inputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_958 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_959 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_960 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_961 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_4_output_0 for ONNX tensor: /model/decoder/decoder/Clip_4_output_0
[X] /model/decoder/decoder/Clip_4 [Clip] outputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sub_1 [Sub]
[X] Parsing node: /model/decoder/decoder/Sub_1 [Sub]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/Clip_3_output_0
[X] /model/decoder/decoder/Sub_1 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_962 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_963 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/Sub_1 for ONNX node: /model/decoder/decoder/Sub_1
[X] Registering tensor: /model/decoder/decoder/Sub_1_output_0 for ONNX tensor: /model/decoder/decoder/Sub_1_output_0
[X] /model/decoder/decoder/Sub_1 [Sub] outputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_5 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_5 [Clip]
[X] Searching for input: /model/decoder/decoder/Sub_1_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_5 [Clip] inputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_965 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_966 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_967 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_968 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_5_output_0 for ONNX tensor: /model/decoder/decoder/Clip_5_output_0
[X] /model/decoder/decoder/Clip_5 [Clip] outputs: [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Div_1 [Div]
[X] Parsing node: /model/decoder/decoder/Div_1 [Div]
[X] Searching for input: /model/decoder/decoder/Clip_4_output_0
[X] Searching for input: /model/decoder/decoder/Clip_5_output_0
[X] /model/decoder/decoder/Div_1 [Div] inputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Div_1 for ONNX node: /model/decoder/decoder/Div_1
[X] Registering tensor: /model/decoder/decoder/Div_1_output_0 for ONNX tensor: /model/decoder/decoder/Div_1_output_0
[X] /model/decoder/decoder/Div_1 [Div] outputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Log_1 [Log]
[X] Parsing node: /model/decoder/decoder/Log_1 [Log]
[X] Searching for input: /model/decoder/decoder/Div_1_output_0
[X] /model/decoder/decoder/Log_1 [Log] inputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Log_1 for ONNX node: /model/decoder/decoder/Log_1
[X] Registering tensor: /model/decoder/decoder/Log_1_output_0 for ONNX tensor: /model/decoder/decoder/Log_1_output_0
[X] /model/decoder/decoder/Log_1 [Log] outputs: [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/Add_1 [Add]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0
[X] Searching for input: /model/decoder/decoder/Log_1_output_0
[X] /model/decoder/decoder/Add_1 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Add_1 for ONNX node: /model/decoder/decoder/Add_1
[X] Registering tensor: /model/decoder/decoder/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/Add_1_output_0
[X] /model/decoder/decoder/Add_1 [Add] outputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid]
[X] Parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid]
[X] Searching for input: /model/decoder/decoder/Add_1_output_0
[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] inputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Sigmoid_2 for ONNX node: /model/decoder/decoder/Sigmoid_2
[X] Registering tensor: /model/decoder/decoder/Sigmoid_2_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_2_output_0
[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_969 for ONNX node: tmp_weight_969
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_970 for ONNX node: tmp_weight_970
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_971 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_972 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.0.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_973 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_974 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0
[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/query_pos_head/act_2/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_2/Relu
[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0
[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_975 for ONNX node: tmp_weight_975
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_976 for ONNX node: tmp_weight_976
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_977 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_978 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add]
[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add]
[X] Searching for input: model.decoder.query_pos_head.layers.1.bias
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_979 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_980 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/Add
[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0
[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0
[X] /model/decoder/decoder/layers.2/Add [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add for ONNX node: /model/decoder/decoder/layers.2/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_output_0
[X] /model/decoder/decoder/layers.2/Add [Add] outputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3880
[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3880 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3880 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_981 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_982 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul
[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add]
[X] Searching for input: onnx::Add_3875
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] inputs: [onnx::Add_3875 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3875 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_983 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_984 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0
[X] Searching for input: onnx::MatMul_3881
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3881 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3881 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_985 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_986 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_1
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add]
[X] Searching for input: onnx::Add_3877
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] inputs: [onnx::Add_3877 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3877 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_987 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_988 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0
[X] Searching for input: onnx::MatMul_3882
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3882 -> (256, 256)[FLOAT]], 
[X] Registering layer: onnx::MatMul_3882 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_989 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_990 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_2
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add]
[X] Searching for input: onnx::Add_3879
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] inputs: [onnx::Add_3879 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: onnx::Add_3879 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_991 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_992 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_output_0
[X] Searching for input: _v_1680
[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1680 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_993 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0
[X] Searching for input: _v_1680
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1680 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_994 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0
[X] Searching for input: _v_1680
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1680 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_995 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_996 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_997 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_4
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_3
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/self_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_998 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_4
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_5
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0
[X] Searching for input: _v_1857
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1857 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_999 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_3
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0
[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.weight
[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.bias
[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.weight required by ONNX-TRT
[X] Using opA: 0 opB: 1
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.2/self_attn/Gemm
[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1000 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1001 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0
[X] Searching for input: _v_1686
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1686 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1002 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_6
[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_1 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add_1 [Add]
[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0
[X] /model/decoder/decoder/layers.2/Add_1 [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add_1 for ONNX node: /model/decoder/decoder/layers.2/Add_1
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_1_output_0
[X] /model/decoder/decoder/layers.2/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_1_output_0
[X] Searching for input: model.decoder.decoder.layers.2.norm1.weight
[X] Searching for input: model.decoder.decoder.layers.2.norm1.bias
[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.norm1.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.2.norm1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1005 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1006 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1007 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1008 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm1/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add_2 [Add]
[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0
[X] /model/decoder/decoder/layers.2/Add_2 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add_2 for ONNX node: /model/decoder/decoder/layers.2/Add_2
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_2_output_0
[X] /model/decoder/decoder/layers.2/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1009 for ONNX node: tmp_weight_1009
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1010 for ONNX node: tmp_weight_1010
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1011 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1012 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1013 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1014 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0
[X] Searching for input: _v_1859
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1859 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1015 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1016 for ONNX node: tmp_weight_1016
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1017 for ONNX node: tmp_weight_1017
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1018 for ONNX node: tmp_weight_1018
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], 
[X] Registering layer: tmp_weight_1019 for ONNX node: tmp_weight_1019
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1020 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1021 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1022 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1023 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0_1024 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[X] Searching for input: _v_1674
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1674 -> (5)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1025 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1026 for ONNX node: tmp_weight_1026
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], 
[X] Registering layer: tmp_weight_1027 for ONNX node: tmp_weight_1027
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1028 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1029 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1030 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1031 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
[X] Searching for input: _v_1676
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1676 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1032 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Softmax
[X] Registering layer: ONNXTRT_ShapeShuffle_1033 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0
[X] Searching for input: onnx::Mul_3755
[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1034 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1035 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0
[X] Searching for input: _v_2002
[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [_v_2002 -> (2)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1036 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1037 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1039 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1040 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1042 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1043 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1045 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1046 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1047 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1049 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1050 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1051 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1053 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1054 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1056 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1057 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1058 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1059 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1061 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1062 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1063 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1064 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1066 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1067 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1068 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1069 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1070 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_2
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/Constant_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1071 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1072 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1074 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1075 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1077 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1078 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1080 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1081 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1082 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1084 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1085 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1086 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1088 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1089 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1091 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1092 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1093 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1094 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1096 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1097 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1098 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1099 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1101 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1102 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1103 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0
[X] Searching for input: _v_1760
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1760 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1104 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1105 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1106 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1108 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1109 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1111 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1112 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1114 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1115 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1116 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1118 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1119 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1120 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1122 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1123 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1125 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1126 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1127 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1128 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1130 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1131 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1132 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1133 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1135 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1136 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1137 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_4
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1138 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1139 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1141 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1142 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1144 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1145 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1147 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1148 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1149 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1151 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1152 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1153 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1155 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1156 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1158 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1159 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1160 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1161 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1163 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1164 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1165 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1166 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1168 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1169 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1170 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_5
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_castHelper_1171 required by ONNX-TRT
[X] Registering layer: ONNXTRT_castHelper_1172 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1174 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1175 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1177 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1178 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1180 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1181 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1182 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1184 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1185 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1186 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1188 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1189 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1191 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1192 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1193 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1194 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1196 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1197 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1198 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1199 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1201 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeElementWise_1202 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeSlice_1203 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_6
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Add_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1204 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1205 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_3
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1206 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1207 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Sub
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0
[X] Searching for input: _v_1861
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1861 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1208 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0
[X] Searching for input: onnx::Split_2305
[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_1209 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1210 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1211 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1212 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1213 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1214 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1215 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_1
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1216 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_2
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1217 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Concat_10
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_5
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0
[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1218 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1219 for ONNX node: tmp_weight_1219
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1220 for ONNX node: tmp_weight_1220
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.weight
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1221 for ONNX node: tmp_weight_1221
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1222 for ONNX node: tmp_weight_1222
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1223 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1224 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.bias
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1225 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1226 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_3 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add_3 [Add]
[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0
[X] /model/decoder/decoder/layers.2/Add_3 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add_3 for ONNX node: /model/decoder/decoder/layers.2/Add_3
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_3_output_0
[X] /model/decoder/decoder/layers.2/Add_3 [Add] outputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_3_output_0
[X] Searching for input: model.decoder.decoder.layers.2.norm2.weight
[X] Searching for input: model.decoder.decoder.layers.2.norm2.bias
[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.norm2.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.2.norm2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1229 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1230 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1231 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1232 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm2/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1233 for ONNX node: tmp_weight_1233
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1234 for ONNX node: tmp_weight_1234
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.linear1.weight
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.linear1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1235 for ONNX node: tmp_weight_1235
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], 
[X] Registering layer: tmp_weight_1236 for ONNX node: tmp_weight_1236
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear1/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Transpose_output_0
[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1237 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1238 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear1/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.linear1.bias
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/MatMul_output_0
[X] /model/decoder/decoder/layers.2/linear1/Add [Add] inputs: [model.decoder.decoder.layers.2.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.linear1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1239 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1240 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Add for ONNX node: /model/decoder/decoder/layers.2/linear1/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0
[X] /model/decoder/decoder/layers.2/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Add_output_0
[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/activation/Relu for ONNX node: /model/decoder/decoder/layers.2/activation/Relu
[X] Registering tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0
[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/activation/Relu_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1241 for ONNX node: tmp_weight_1241
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1242 for ONNX node: tmp_weight_1242
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.decoder.layers.2.linear2.weight
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.decoder.layers.2.linear2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1243 for ONNX node: tmp_weight_1243
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1244 for ONNX node: tmp_weight_1244
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear2/Transpose
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Transpose_output_0
[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1245 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1246 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear2/MatMul
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add]
[X] Searching for input: model.decoder.decoder.layers.2.linear2.bias
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/MatMul_output_0
[X] /model/decoder/decoder/layers.2/linear2/Add [Add] inputs: [model.decoder.decoder.layers.2.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.linear2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1247 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1248 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Add for ONNX node: /model/decoder/decoder/layers.2/linear2/Add
[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0
[X] /model/decoder/decoder/layers.2/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_4 [Add]
[X] Parsing node: /model/decoder/decoder/layers.2/Add_4 [Add]
[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Add_output_0
[X] /model/decoder/decoder/layers.2/Add_4 [Add] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/layers.2/Add_4 for ONNX node: /model/decoder/decoder/layers.2/Add_4
[X] Registering tensor: /model/decoder/decoder/layers.2/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_4_output_0
[X] /model/decoder/decoder/layers.2/Add_4 [Add] outputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization]
[X] Parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization]
[X] Searching for input: /model/decoder/decoder/layers.2/Add_4_output_0
[X] Searching for input: model.decoder.decoder.layers.2.norm3.weight
[X] Searching for input: model.decoder.decoder.layers.2.norm3.bias
[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.bias -> (256)[FLOAT]], 
[X] Registering layer: model.decoder.decoder.layers.2.norm3.weight required by ONNX-TRT
[X] Registering layer: model.decoder.decoder.layers.2.norm3.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1251 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1252 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1253 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1254 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm3/LayerNormalization
[X] Registering tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0
[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1255 for ONNX node: tmp_weight_1255
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1256 for ONNX node: tmp_weight_1256
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1257 for ONNX node: tmp_weight_1257
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1258 for ONNX node: tmp_weight_1258
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1259 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1260 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1261 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1262 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1263 for ONNX node: tmp_weight_1263
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1264 for ONNX node: tmp_weight_1264
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1265 for ONNX node: tmp_weight_1265
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], 
[X] Registering layer: tmp_weight_1266 for ONNX node: tmp_weight_1266
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1267 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1268 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1269 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1270 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1271 for ONNX node: tmp_weight_1271
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], 
[X] Registering layer: tmp_weight_1272 for ONNX node: tmp_weight_1272
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.weight
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1273 for ONNX node: tmp_weight_1273
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], 
[X] Registering layer: tmp_weight_1274 for ONNX node: tmp_weight_1274
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1275 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1276 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add]
[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.bias
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1277 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1278 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add
[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0
[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_6 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_6 [Clip]
[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0
[X] Searching for input: /model/decoder/decoder/Constant_1_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] /model/decoder/decoder/Clip_6 [Clip] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Clip_6 for ONNX node: /model/decoder/decoder/Clip_6
[X] Registering tensor: /model/decoder/decoder/Clip_6_output_0 for ONNX tensor: /model/decoder/decoder/Clip_6_output_0
[X] /model/decoder/decoder/Clip_6 [Clip] outputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_7 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_7 [Clip]
[X] Searching for input: /model/decoder/decoder/Clip_6_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_7 [Clip] inputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1280 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1281 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1282 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1283 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_7_output_0 for ONNX tensor: /model/decoder/decoder/Clip_7_output_0
[X] /model/decoder/decoder/Clip_7 [Clip] outputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sub_2 [Sub]
[X] Parsing node: /model/decoder/decoder/Sub_2 [Sub]
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0
[X] Searching for input: /model/decoder/decoder/Clip_6_output_0
[X] /model/decoder/decoder/Sub_2 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1284 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1285 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/Sub_2 for ONNX node: /model/decoder/decoder/Sub_2
[X] Registering tensor: /model/decoder/decoder/Sub_2_output_0 for ONNX tensor: /model/decoder/decoder/Sub_2_output_0
[X] /model/decoder/decoder/Sub_2 [Sub] outputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Clip_8 [Clip]
[X] Parsing node: /model/decoder/decoder/Clip_8 [Clip]
[X] Searching for input: /model/decoder/decoder/Sub_2_output_0
[X] Searching for input: /model/decoder/decoder/Constant_3_output_0
[X] /model/decoder/decoder/Clip_8 [Clip] inputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1287 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1288 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1289 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1290 required by ONNX-TRT
[X] Registering tensor: /model/decoder/decoder/Clip_8_output_0 for ONNX tensor: /model/decoder/decoder/Clip_8_output_0
[X] /model/decoder/decoder/Clip_8 [Clip] outputs: [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Div_2 [Div]
[X] Parsing node: /model/decoder/decoder/Div_2 [Div]
[X] Searching for input: /model/decoder/decoder/Clip_7_output_0
[X] Searching for input: /model/decoder/decoder/Clip_8_output_0
[X] /model/decoder/decoder/Div_2 [Div] inputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Div_2 for ONNX node: /model/decoder/decoder/Div_2
[X] Registering tensor: /model/decoder/decoder/Div_2_output_0 for ONNX tensor: /model/decoder/decoder/Div_2_output_0
[X] /model/decoder/decoder/Div_2 [Div] outputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Log_2 [Log]
[X] Parsing node: /model/decoder/decoder/Log_2 [Log]
[X] Searching for input: /model/decoder/decoder/Div_2_output_0
[X] /model/decoder/decoder/Log_2 [Log] inputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Log_2 for ONNX node: /model/decoder/decoder/Log_2
[X] Registering tensor: /model/decoder/decoder/Log_2_output_0 for ONNX tensor: /model/decoder/decoder/Log_2_output_0
[X] /model/decoder/decoder/Log_2 [Log] outputs: [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Add_2 [Add]
[X] Parsing node: /model/decoder/decoder/Add_2 [Add]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0
[X] Searching for input: /model/decoder/decoder/Log_2_output_0
[X] /model/decoder/decoder/Add_2 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Add_2 for ONNX node: /model/decoder/decoder/Add_2
[X] Registering tensor: /model/decoder/decoder/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/Add_2_output_0
[X] /model/decoder/decoder/Add_2 [Add] outputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid]
[X] Parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid]
[X] Searching for input: /model/decoder/decoder/Add_2_output_0
[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] inputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/Sigmoid_3 for ONNX node: /model/decoder/decoder/Sigmoid_3
[X] Registering tensor: /model/decoder/decoder/Sigmoid_3_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_3_output_0
[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear]
[X] Searching for input: model.decoder.dec_score_head.2.weight
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_score_head.2.weight -> (80, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], 
[X] Registering layer: model.decoder.dec_score_head.2.weight required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Registering layer: tmp_weight_1291 for ONNX node: tmp_weight_1291
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0
[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear]
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0
[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], 
[X] Registering layer: tmp_weight_1292 for ONNX node: tmp_weight_1292
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose]
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0
[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], 
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Transpose for ONNX node: /model/decoder/decoder/dec_score_head.2/Transpose
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0
[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul]
[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Transpose_output_0
[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1293 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1294 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/MatMul for ONNX node: /model/decoder/decoder/dec_score_head.2/MatMul
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0
[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add]
[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add]
[X] Searching for input: model.decoder.dec_score_head.2.bias
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/MatMul_output_0
[X] /model/decoder/decoder/dec_score_head.2/Add [Add] inputs: [model.decoder.dec_score_head.2.bias -> (80)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Registering layer: model.decoder.dec_score_head.2.bias required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1295 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1296 required by ONNX-TRT
[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Add for ONNX node: /model/decoder/decoder/dec_score_head.2/Add
[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0
[X] /model/decoder/decoder/dec_score_head.2/Add [Add] outputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/Sigmoid_3_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/Unsqueeze_3 for ONNX node: /model/decoder/decoder/Unsqueeze_3
[X] Registering tensor: /model/decoder/decoder/Unsqueeze_3_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_3_output_0
[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze]
[X] Parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze]
[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Add_output_0
[X] Searching for input: onnx::Unsqueeze_1255
[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] inputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/decoder/Unsqueeze_4 for ONNX node: /model/decoder/decoder/Unsqueeze_4
[X] Registering tensor: /model/decoder/decoder/Unsqueeze_4_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_4_output_0
[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Gather_8 [Gather]
[X] Parsing node: /model/decoder/Gather_8 [Gather]
[X] Searching for input: /model/decoder/decoder/Unsqueeze_4_output_0
[X] Searching for input: /model/encoder/Constant_2_output_0
[X] /model/decoder/Gather_8 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], 
[X] Registering layer: /model/encoder/Constant_2_output_0 required by ONNX-TRT
[X] Using Gather axis: 0
[X] Registering layer: ONNXTRT_castHelper_1297 required by ONNX-TRT
[X] Registering layer: /model/decoder/Gather_8 for ONNX node: /model/decoder/Gather_8
[X] Registering tensor: /model/decoder/Gather_8_output_0 for ONNX tensor: /model/decoder/Gather_8_output_0
[X] /model/decoder/Gather_8 [Gather] outputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /model/decoder/Gather_9 [Gather]
[X] Parsing node: /model/decoder/Gather_9 [Gather]
[X] Searching for input: /model/decoder/decoder/Unsqueeze_3_output_0
[X] Searching for input: /model/encoder/Constant_2_output_0
[X] /model/decoder/Gather_9 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], 
[X] Using Gather axis: 0
[X] Registering layer: ONNXTRT_castHelper_1298 required by ONNX-TRT
[X] Registering layer: /model/decoder/Gather_9 for ONNX node: /model/decoder/Gather_9
[X] Registering tensor: /model/decoder/Gather_9_output_0 for ONNX tensor: /model/decoder/Gather_9_output_0
[X] /model/decoder/Gather_9 [Gather] outputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Split [Split]
[X] Parsing node: /postprocessor/Split [Split]
[X] Searching for input: /model/decoder/Gather_9_output_0
[X] Searching for input: /postprocessor/Constant_output_0
[X] /postprocessor/Split [Split] inputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Constant_output_0 -> (4)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_1299 required by ONNX-TRT
[X] Registering layer: /postprocessor/Split for ONNX node: /postprocessor/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1300 required by ONNX-TRT
[X] Registering layer: /postprocessor/Split_1301 for ONNX node: /postprocessor/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1302 required by ONNX-TRT
[X] Registering layer: /postprocessor/Split_1303 for ONNX node: /postprocessor/Split
[X] Registering layer: ONNXTRT_ShapeSlice_1304 required by ONNX-TRT
[X] Registering layer: /postprocessor/Split_1305 for ONNX node: /postprocessor/Split
[X] Registering tensor: /postprocessor/Split_output_0 for ONNX tensor: /postprocessor/Split_output_0
[X] Registering tensor: /postprocessor/Split_output_1 for ONNX tensor: /postprocessor/Split_output_1
[X] Registering tensor: /postprocessor/Split_output_2 for ONNX tensor: /postprocessor/Split_output_2
[X] Registering tensor: /postprocessor/Split_output_3 for ONNX tensor: /postprocessor/Split_output_3
[X] /postprocessor/Split [Split] outputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Squeeze [Squeeze]
[X] Parsing node: /postprocessor/Squeeze [Squeeze]
[X] Searching for input: /postprocessor/Split_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Squeeze [Squeeze] inputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Squeeze for ONNX node: /postprocessor/Squeeze
[X] Registering tensor: /postprocessor/Squeeze_output_0 for ONNX tensor: /postprocessor/Squeeze_output_0
[X] /postprocessor/Squeeze [Squeeze] outputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Squeeze_1 [Squeeze]
[X] Parsing node: /postprocessor/Squeeze_1 [Squeeze]
[X] Searching for input: /postprocessor/Split_output_1
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Squeeze_1 [Squeeze] inputs: [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Squeeze_1 for ONNX node: /postprocessor/Squeeze_1
[X] Registering tensor: /postprocessor/Squeeze_1_output_0 for ONNX tensor: /postprocessor/Squeeze_1_output_0
[X] /postprocessor/Squeeze_1 [Squeeze] outputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Squeeze_2 [Squeeze]
[X] Parsing node: /postprocessor/Squeeze_2 [Squeeze]
[X] Searching for input: /postprocessor/Split_output_2
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Squeeze_2 [Squeeze] inputs: [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Squeeze_2 for ONNX node: /postprocessor/Squeeze_2
[X] Registering tensor: /postprocessor/Squeeze_2_output_0 for ONNX tensor: /postprocessor/Squeeze_2_output_0
[X] /postprocessor/Squeeze_2 [Squeeze] outputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Squeeze_3 [Squeeze]
[X] Parsing node: /postprocessor/Squeeze_3 [Squeeze]
[X] Searching for input: /postprocessor/Split_output_3
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Squeeze_3 [Squeeze] inputs: [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Squeeze_3 for ONNX node: /postprocessor/Squeeze_3
[X] Registering tensor: /postprocessor/Squeeze_3_output_0 for ONNX tensor: /postprocessor/Squeeze_3_output_0
[X] /postprocessor/Squeeze_3 [Squeeze] outputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Mul [Mul]
[X] Parsing node: /postprocessor/Mul [Mul]
[X] Searching for input: /postprocessor/Squeeze_2_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /postprocessor/Mul [Mul] inputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1306 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1307 required by ONNX-TRT
[X] Registering layer: /postprocessor/Mul for ONNX node: /postprocessor/Mul
[X] Registering tensor: /postprocessor/Mul_output_0 for ONNX tensor: /postprocessor/Mul_output_0
[X] /postprocessor/Mul [Mul] outputs: [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Sub [Sub]
[X] Parsing node: /postprocessor/Sub [Sub]
[X] Searching for input: /postprocessor/Squeeze_output_0
[X] Searching for input: /postprocessor/Mul_output_0
[X] /postprocessor/Sub [Sub] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], 
[X] Registering layer: /postprocessor/Sub for ONNX node: /postprocessor/Sub
[X] Registering tensor: /postprocessor/Sub_output_0 for ONNX tensor: /postprocessor/Sub_output_0
[X] /postprocessor/Sub [Sub] outputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Mul_1 [Mul]
[X] Parsing node: /postprocessor/Mul_1 [Mul]
[X] Searching for input: /postprocessor/Squeeze_3_output_0
[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0
[X] /postprocessor/Mul_1 [Mul] inputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1308 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1309 required by ONNX-TRT
[X] Registering layer: /postprocessor/Mul_1 for ONNX node: /postprocessor/Mul_1
[X] Registering tensor: /postprocessor/Mul_1_output_0 for ONNX tensor: /postprocessor/Mul_1_output_0
[X] /postprocessor/Mul_1 [Mul] outputs: [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Sub_1 [Sub]
[X] Parsing node: /postprocessor/Sub_1 [Sub]
[X] Searching for input: /postprocessor/Squeeze_1_output_0
[X] Searching for input: /postprocessor/Mul_1_output_0
[X] /postprocessor/Sub_1 [Sub] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], 
[X] Registering layer: /postprocessor/Sub_1 for ONNX node: /postprocessor/Sub_1
[X] Registering tensor: /postprocessor/Sub_1_output_0 for ONNX tensor: /postprocessor/Sub_1_output_0
[X] /postprocessor/Sub_1 [Sub] outputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Add [Add]
[X] Parsing node: /postprocessor/Add [Add]
[X] Searching for input: /postprocessor/Squeeze_output_0
[X] Searching for input: /postprocessor/Mul_output_0
[X] /postprocessor/Add [Add] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], 
[X] Registering layer: /postprocessor/Add for ONNX node: /postprocessor/Add
[X] Registering tensor: /postprocessor/Add_output_0 for ONNX tensor: /postprocessor/Add_output_0
[X] /postprocessor/Add [Add] outputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Add_1 [Add]
[X] Parsing node: /postprocessor/Add_1 [Add]
[X] Searching for input: /postprocessor/Squeeze_1_output_0
[X] Searching for input: /postprocessor/Mul_1_output_0
[X] /postprocessor/Add_1 [Add] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], 
[X] Registering layer: /postprocessor/Add_1 for ONNX node: /postprocessor/Add_1
[X] Registering tensor: /postprocessor/Add_1_output_0 for ONNX tensor: /postprocessor/Add_1_output_0
[X] /postprocessor/Add_1 [Add] outputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze [Unsqueeze]
[X] Searching for input: /postprocessor/Sub_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze [Unsqueeze] inputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze for ONNX node: /postprocessor/Unsqueeze
[X] Registering tensor: /postprocessor/Unsqueeze_output_0 for ONNX tensor: /postprocessor/Unsqueeze_output_0
[X] /postprocessor/Unsqueeze [Unsqueeze] outputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze]
[X] Searching for input: /postprocessor/Sub_1_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze_1 [Unsqueeze] inputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze_1 for ONNX node: /postprocessor/Unsqueeze_1
[X] Registering tensor: /postprocessor/Unsqueeze_1_output_0 for ONNX tensor: /postprocessor/Unsqueeze_1_output_0
[X] /postprocessor/Unsqueeze_1 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze]
[X] Searching for input: /postprocessor/Add_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze_2 [Unsqueeze] inputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze_2 for ONNX node: /postprocessor/Unsqueeze_2
[X] Registering tensor: /postprocessor/Unsqueeze_2_output_0 for ONNX tensor: /postprocessor/Unsqueeze_2_output_0
[X] /postprocessor/Unsqueeze_2 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze]
[X] Searching for input: /postprocessor/Add_1_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze_3 [Unsqueeze] inputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze_3 for ONNX node: /postprocessor/Unsqueeze_3
[X] Registering tensor: /postprocessor/Unsqueeze_3_output_0 for ONNX tensor: /postprocessor/Unsqueeze_3_output_0
[X] /postprocessor/Unsqueeze_3 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Concat [Concat]
[X] Parsing node: /postprocessor/Concat [Concat]
[X] Searching for input: /postprocessor/Unsqueeze_output_0
[X] Searching for input: /postprocessor/Unsqueeze_1_output_0
[X] Searching for input: /postprocessor/Unsqueeze_2_output_0
[X] Searching for input: /postprocessor/Unsqueeze_3_output_0
[X] /postprocessor/Concat [Concat] inputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], 
[X] Registering layer: /postprocessor/Concat for ONNX node: /postprocessor/Concat
[X] Registering tensor: /postprocessor/Concat_output_0 for ONNX tensor: /postprocessor/Concat_output_0
[X] /postprocessor/Concat [Concat] outputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Tile [Tile]
[X] Parsing node: /postprocessor/Tile [Tile]
[X] Searching for input: orig_target_sizes
[X] Searching for input: onnx::Tile_3498
[X] /postprocessor/Tile [Tile] inputs: [orig_target_sizes -> (1, 2)[INT64]], [onnx::Tile_3498 -> (2)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_1310 required by ONNX-TRT
[X] Registering layer: /postprocessor/Tile for ONNX node: /postprocessor/Tile
[X] Registering tensor: /postprocessor/Tile_output_0 for ONNX tensor: /postprocessor/Tile_output_0
[X] /postprocessor/Tile [Tile] outputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze]
[X] Searching for input: /postprocessor/Tile_output_0
[X] Searching for input: /model/decoder/Constant_21_output_0
[X] /postprocessor/Unsqueeze_4 [Unsqueeze] inputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], 
[X] Registering layer: /model/decoder/Constant_21_output_0 required by ONNX-TRT
[X] Registering layer: /postprocessor/Unsqueeze_4 for ONNX node: /postprocessor/Unsqueeze_4
[X] Registering tensor: /postprocessor/Unsqueeze_4_output_0 for ONNX tensor: /postprocessor/Unsqueeze_4_output_0
[X] /postprocessor/Unsqueeze_4 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], 
[X] Static check for parsing node: Cast_3039 [Cast]
[X] Parsing node: Cast_3039 [Cast]
[X] Searching for input: /postprocessor/Unsqueeze_4_output_0
[X] Cast_3039 [Cast] inputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], 
[X] Casting to type: float32
[X] Registering layer: Cast_3039 for ONNX node: Cast_3039
[X] Registering tensor: onnx::Mul_3505 for ONNX tensor: onnx::Mul_3505
[X] Cast_3039 [Cast] outputs: [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Mul_2 [Mul]
[X] Parsing node: /postprocessor/Mul_2 [Mul]
[X] Searching for input: /postprocessor/Concat_output_0
[X] Searching for input: onnx::Mul_3505
[X] /postprocessor/Mul_2 [Mul] inputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], 
[X] Registering layer: /postprocessor/Mul_2 for ONNX node: /postprocessor/Mul_2
[X] Registering tensor: /postprocessor/Mul_2_output_0 for ONNX tensor: /postprocessor/Mul_2_output_0
[X] /postprocessor/Mul_2 [Mul] outputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Sigmoid [Sigmoid]
[X] Parsing node: /postprocessor/Sigmoid [Sigmoid]
[X] Searching for input: /model/decoder/Gather_8_output_0
[X] /postprocessor/Sigmoid [Sigmoid] inputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Registering layer: /postprocessor/Sigmoid for ONNX node: /postprocessor/Sigmoid
[X] Registering tensor: /postprocessor/Sigmoid_output_0 for ONNX tensor: /postprocessor/Sigmoid_output_0
[X] /postprocessor/Sigmoid [Sigmoid] outputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/Flatten [Flatten]
[X] Parsing node: /postprocessor/Flatten [Flatten]
[X] Searching for input: /postprocessor/Sigmoid_output_0
[X] /postprocessor/Flatten [Flatten] inputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1311 required by ONNX-TRT
[X] Registering layer: /postprocessor/Flatten for ONNX node: /postprocessor/Flatten
[X] Registering tensor: /postprocessor/Flatten_output_0 for ONNX tensor: /postprocessor/Flatten_output_0
[X] /postprocessor/Flatten [Flatten] outputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], 
[X] Static check for parsing node: /postprocessor/TopK [TopK]
[X] Parsing node: /postprocessor/TopK [TopK]
[X] Searching for input: /postprocessor/Flatten_output_0
[X] Searching for input: /model/decoder/Constant_18_output_0
[X] /postprocessor/TopK [TopK] inputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], 
[X] Registering layer: ONNXTRT_convertToScalar_1312 required by ONNX-TRT
[X] Registering layer: /postprocessor/TopK for ONNX node: /postprocessor/TopK
[X] Registering layer: ONNXTRT_castHelper_1313 required by ONNX-TRT
[X] Registering tensor: scores_1314 for ONNX tensor: scores
[X] Registering tensor: /postprocessor/TopK_output_1 for ONNX tensor: /postprocessor/TopK_output_1
[X] /postprocessor/TopK [TopK] outputs: [scores -> (1, 300)[FLOAT]], [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], 
[X] Static check for parsing node: /postprocessor/Div [Div]
[X] Parsing node: /postprocessor/Div [Div]
[X] Searching for input: /postprocessor/TopK_output_1
[X] Searching for input: /postprocessor/Constant_14_output_0
[X] /postprocessor/Div [Div] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], 
[X] Registering layer: /postprocessor/Constant_14_output_0 required by ONNX-TRT
[X] Registering layer: ONNXTRT_ShapeShuffle_1315 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1316 required by ONNX-TRT
[X] Registering layer: /postprocessor/Div for ONNX node: /postprocessor/Div
[X] Registering tensor: /postprocessor/Div_output_0 for ONNX tensor: /postprocessor/Div_output_0
[X] /postprocessor/Div [Div] outputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], 
[X] Static check for parsing node: /postprocessor/Mul_3 [Mul]
[X] Parsing node: /postprocessor/Mul_3 [Mul]
[X] Searching for input: /postprocessor/Div_output_0
[X] Searching for input: /postprocessor/Constant_14_output_0
[X] /postprocessor/Mul_3 [Mul] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], 
[X] Registering layer: ONNXTRT_ShapeShuffle_1317 required by ONNX-TRT
[X] Registering layer: ONNXTRT_Broadcast_1318 required by ONNX-TRT
[X] Registering layer: /postprocessor/Mul_3 for ONNX node: /postprocessor/Mul_3
[X] Registering tensor: /postprocessor/Mul_3_output_0 for ONNX tensor: /postprocessor/Mul_3_output_0
[X] /postprocessor/Mul_3 [Mul] outputs: [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], 
[X] Static check for parsing node: /postprocessor/Sub_2 [Sub]
[X] Parsing node: /postprocessor/Sub_2 [Sub]
[X] Searching for input: /postprocessor/TopK_output_1
[X] Searching for input: /postprocessor/Mul_3_output_0
[X] /postprocessor/Sub_2 [Sub] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], 
[X] Registering layer: /postprocessor/Sub_2 for ONNX node: /postprocessor/Sub_2
[X] Registering tensor: labels_1319 for ONNX tensor: labels
[X] /postprocessor/Sub_2 [Sub] outputs: [labels -> (1, 300)[INT64]], 
[X] Static check for parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze]
[X] Parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze]
[X] Searching for input: /postprocessor/Div_output_0
[X] Searching for input: /model/encoder/Constant_7_output_0
[X] /postprocessor/Unsqueeze_5 [Unsqueeze] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], 
[X] Registering layer: /postprocessor/Unsqueeze_5 for ONNX node: /postprocessor/Unsqueeze_5
[X] Registering tensor: /postprocessor/Unsqueeze_5_output_0 for ONNX tensor: /postprocessor/Unsqueeze_5_output_0
[X] /postprocessor/Unsqueeze_5 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], 
[X] Static check for parsing node: /postprocessor/Tile_1 [Tile]
[X] Parsing node: /postprocessor/Tile_1 [Tile]
[X] Searching for input: /postprocessor/Unsqueeze_5_output_0
[X] Searching for input: /model/decoder/Concat_5_output_0
[X] /postprocessor/Tile_1 [Tile] inputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], 
[X] Registering layer: ONNXTRT_ShapeSlice_1320 required by ONNX-TRT
[X] Registering layer: /postprocessor/Tile_1 for ONNX node: /postprocessor/Tile_1
[X] Registering tensor: /postprocessor/Tile_1_output_0 for ONNX tensor: /postprocessor/Tile_1_output_0
[X] /postprocessor/Tile_1 [Tile] outputs: [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], 
[X] Static check for parsing node: /postprocessor/GatherElements [GatherElements]
[X] Parsing node: /postprocessor/GatherElements [GatherElements]
[X] Searching for input: /postprocessor/Mul_2_output_0
[X] Searching for input: /postprocessor/Tile_1_output_0
[X] /postprocessor/GatherElements [GatherElements] inputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], 
[X] Using Gather axis: 1
[X] Registering layer: ONNXTRT_castHelper_1321 required by ONNX-TRT
[X] Registering layer: /postprocessor/GatherElements for ONNX node: /postprocessor/GatherElements
[X] Registering tensor: boxes_1322 for ONNX tensor: boxes
[X] /postprocessor/GatherElements [GatherElements] outputs: [boxes -> (1, 300, 4)[FLOAT]], 
[X] Marking /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0_382 as output: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[X] Marking /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0_702 as output: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[X] Marking /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0_1024 as output: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[X] Marking labels_1319 as output: labels
[X] Marking boxes_1322 as output: boxes
[X] Marking scores_1314 as output: scores
[I] Building engine with configuration:
    Flags                  | [TF32]
    Engine Capability      | EngineCapability.STANDARD
    Memory Pools           | [WORKSPACE: 1024.00 MiB, TACTIC_DRAM: 24105.06 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]
    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]
    Profiling Verbosity    | ProfilingVerbosity.DETAILED
    Preview Features       | [PROFILE_SHARING_0806]
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].
[X] Original: 1937 layers
[X] After dead-layer removal: 1937 layers
[X] Graph construction completed in 0.0192089 seconds.
[X] After adding DebugOutput nodes: 1937 layers
[X] Running: ConstShuffleFusion on onnx::MatMul_3619
[X] ConstShuffleFusion: Fusing onnx::MatMul_3619 with ONNXTRT_Broadcast
[X] Running: ConstShuffleFusion on onnx::Add_3614
[X] ConstShuffleFusion: Fusing onnx::Add_3614 with ONNXTRT_Broadcast_105
[X] Running: ConstShuffleFusion on onnx::MatMul_3620
[X] ConstShuffleFusion: Fusing onnx::MatMul_3620 with ONNXTRT_Broadcast_107
[X] Running: ConstShuffleFusion on onnx::Add_3616
[X] ConstShuffleFusion: Fusing onnx::Add_3616 with ONNXTRT_Broadcast_109
[X] Running: ConstShuffleFusion on onnx::MatMul_3621
[X] ConstShuffleFusion: Fusing onnx::MatMul_3621 with ONNXTRT_Broadcast_111
[X] Running: ConstShuffleFusion on onnx::Add_3618
[X] ConstShuffleFusion: Fusing onnx::Add_3618 with ONNXTRT_Broadcast_113
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.self_attn.out_proj.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_122
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.weight
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.weight with ONNXTRT_Broadcast_127
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.bias with ONNXTRT_Broadcast_129
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear1.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear1.bias with ONNXTRT_Broadcast_137
[X] Running: ConstShuffleFusion on /model/encoder/encoder.0/layers.0/activation/Constant_output_0
[X] ConstShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/activation/Constant_output_0 with ONNXTRT_Broadcast_139
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear2.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear2.bias with ONNXTRT_Broadcast_151
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.weight
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.weight with ONNXTRT_Broadcast_155
[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.bias
[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.bias with ONNXTRT_Broadcast_157
[X] Running: ConstShuffleFusion on model.decoder.enc_output.proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_output.proj.bias with ONNXTRT_Broadcast_281
[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.weight
[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.weight with ONNXTRT_Broadcast_285
[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.bias with ONNXTRT_Broadcast_287
[X] Running: ConstShuffleFusion on model.decoder.enc_score_head.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_score_head.bias with ONNXTRT_Broadcast_295
[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.0.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.0.bias with ONNXTRT_Broadcast_301
[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.1.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.1.bias with ONNXTRT_Broadcast_309
[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.2.bias with ONNXTRT_Broadcast_317
[X] Running: ConstShuffleFusion on onnx::MatMul_3736
[X] ConstShuffleFusion: Fusing onnx::MatMul_3736 with ONNXTRT_Broadcast_338
[X] Running: ConstShuffleFusion on onnx::Add_3731
[X] ConstShuffleFusion: Fusing onnx::Add_3731 with ONNXTRT_Broadcast_340
[X] Running: ConstShuffleFusion on onnx::MatMul_3737
[X] ConstShuffleFusion: Fusing onnx::MatMul_3737 with ONNXTRT_Broadcast_342
[X] Running: ConstShuffleFusion on onnx::Add_3733
[X] ConstShuffleFusion: Fusing onnx::Add_3733 with ONNXTRT_Broadcast_344
[X] Running: ConstShuffleFusion on onnx::MatMul_3738
[X] ConstShuffleFusion: Fusing onnx::MatMul_3738 with ONNXTRT_Broadcast_346
[X] Running: ConstShuffleFusion on onnx::Add_3735
[X] ConstShuffleFusion: Fusing onnx::Add_3735 with ONNXTRT_Broadcast_348
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.self_attn.out_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_357
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.weight with ONNXTRT_Broadcast_362
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.bias with ONNXTRT_Broadcast_364
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.value_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.value_proj.bias with ONNXTRT_Broadcast_372
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_381
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.attention_weights.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_389
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.output_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.output_proj.bias with ONNXTRT_Broadcast_582
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.weight with ONNXTRT_Broadcast_586
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.bias with ONNXTRT_Broadcast_588
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear1.bias with ONNXTRT_Broadcast_596
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear2.bias with ONNXTRT_Broadcast_604
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.weight with ONNXTRT_Broadcast_608
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.bias with ONNXTRT_Broadcast_610
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.0.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.0.bias with ONNXTRT_Broadcast_618
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.1.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.1.bias with ONNXTRT_Broadcast_626
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.2.bias with ONNXTRT_Broadcast_634
[X] Running: ConstShuffleFusion on (Unnamed Layer* 1443) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1443) [Constant] with ONNXTRT_Broadcast_639
[X] Running: ConstShuffleFusion on (Unnamed Layer* 1450) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1450) [Constant] with ONNXTRT_Broadcast_646
[X] Running: ConstShuffleFusion on onnx::MatMul_3808
[X] ConstShuffleFusion: Fusing onnx::MatMul_3808 with ONNXTRT_Broadcast_660
[X] Running: ConstShuffleFusion on onnx::Add_3803
[X] ConstShuffleFusion: Fusing onnx::Add_3803 with ONNXTRT_Broadcast_662
[X] Running: ConstShuffleFusion on onnx::MatMul_3809
[X] ConstShuffleFusion: Fusing onnx::MatMul_3809 with ONNXTRT_Broadcast_664
[X] Running: ConstShuffleFusion on onnx::Add_3805
[X] ConstShuffleFusion: Fusing onnx::Add_3805 with ONNXTRT_Broadcast_666
[X] Running: ConstShuffleFusion on onnx::MatMul_3810
[X] ConstShuffleFusion: Fusing onnx::MatMul_3810 with ONNXTRT_Broadcast_668
[X] Running: ConstShuffleFusion on onnx::Add_3807
[X] ConstShuffleFusion: Fusing onnx::Add_3807 with ONNXTRT_Broadcast_670
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.self_attn.out_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.self_attn.out_proj.bias with ONNXTRT_Broadcast_679
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.weight with ONNXTRT_Broadcast_684
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.bias with ONNXTRT_Broadcast_686
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.value_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.value_proj.bias with ONNXTRT_Broadcast_692
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_701
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.attention_weights.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_709
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.output_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.output_proj.bias with ONNXTRT_Broadcast_904
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.weight with ONNXTRT_Broadcast_908
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.bias with ONNXTRT_Broadcast_910
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear1.bias with ONNXTRT_Broadcast_918
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear2.bias with ONNXTRT_Broadcast_926
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.weight with ONNXTRT_Broadcast_930
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.bias with ONNXTRT_Broadcast_932
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.0.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.0.bias with ONNXTRT_Broadcast_940
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.1.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.1.bias with ONNXTRT_Broadcast_948
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.2.bias with ONNXTRT_Broadcast_956
[X] Running: ConstShuffleFusion on (Unnamed Layer* 1851) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1851) [Constant] with ONNXTRT_Broadcast_961
[X] Running: ConstShuffleFusion on (Unnamed Layer* 1858) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1858) [Constant] with ONNXTRT_Broadcast_968
[X] Running: ConstShuffleFusion on onnx::MatMul_3880
[X] ConstShuffleFusion: Fusing onnx::MatMul_3880 with ONNXTRT_Broadcast_982
[X] Running: ConstShuffleFusion on onnx::Add_3875
[X] ConstShuffleFusion: Fusing onnx::Add_3875 with ONNXTRT_Broadcast_984
[X] Running: ConstShuffleFusion on onnx::MatMul_3881
[X] ConstShuffleFusion: Fusing onnx::MatMul_3881 with ONNXTRT_Broadcast_986
[X] Running: ConstShuffleFusion on onnx::Add_3877
[X] ConstShuffleFusion: Fusing onnx::Add_3877 with ONNXTRT_Broadcast_988
[X] Running: ConstShuffleFusion on onnx::MatMul_3882
[X] ConstShuffleFusion: Fusing onnx::MatMul_3882 with ONNXTRT_Broadcast_990
[X] Running: ConstShuffleFusion on onnx::Add_3879
[X] ConstShuffleFusion: Fusing onnx::Add_3879 with ONNXTRT_Broadcast_992
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.self_attn.out_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.self_attn.out_proj.bias with ONNXTRT_Broadcast_1001
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.weight with ONNXTRT_Broadcast_1006
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.bias with ONNXTRT_Broadcast_1008
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.value_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.value_proj.bias with ONNXTRT_Broadcast_1014
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_1023
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.attention_weights.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_1031
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.output_proj.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.output_proj.bias with ONNXTRT_Broadcast_1226
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.weight with ONNXTRT_Broadcast_1230
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.bias with ONNXTRT_Broadcast_1232
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear1.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear1.bias with ONNXTRT_Broadcast_1240
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear2.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear2.bias with ONNXTRT_Broadcast_1248
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.weight
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.weight with ONNXTRT_Broadcast_1252
[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.bias
[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.bias with ONNXTRT_Broadcast_1254
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.0.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.0.bias with ONNXTRT_Broadcast_1262
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.1.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.1.bias with ONNXTRT_Broadcast_1270
[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.2.bias with ONNXTRT_Broadcast_1278
[X] Running: ConstShuffleFusion on (Unnamed Layer* 2259) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2259) [Constant] with ONNXTRT_Broadcast_1283
[X] Running: ConstShuffleFusion on (Unnamed Layer* 2266) [Constant]
[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2266) [Constant] with ONNXTRT_Broadcast_1290
[X] Running: ConstShuffleFusion on model.decoder.dec_score_head.2.bias
[X] ConstShuffleFusion: Fusing model.decoder.dec_score_head.2.bias with ONNXTRT_Broadcast_1296
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear1/Transpose with ONNXTRT_Broadcast_135
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear2/Transpose with ONNXTRT_Broadcast_149
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_output/proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_output/proj/Transpose with ONNXTRT_Broadcast_279
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_score_head/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_score_head/Transpose with ONNXTRT_Broadcast_293
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.0/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.0/Transpose with ONNXTRT_Broadcast_299
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.1/Transpose with ONNXTRT_Broadcast_307
[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.2/Transpose with ONNXTRT_Broadcast_315
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_370
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_379
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_387
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_580
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear1/Transpose with ONNXTRT_Broadcast_594
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear2/Transpose with ONNXTRT_Broadcast_602
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose with ONNXTRT_Broadcast_616
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose with ONNXTRT_Broadcast_624
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose with ONNXTRT_Broadcast_632
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_690
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_699
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_707
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_902
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear1/Transpose with ONNXTRT_Broadcast_916
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear2/Transpose with ONNXTRT_Broadcast_924
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose with ONNXTRT_Broadcast_938
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose with ONNXTRT_Broadcast_946
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose with ONNXTRT_Broadcast_954
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_1012
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_1021
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_1029
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_1224
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear1/Transpose with ONNXTRT_Broadcast_1238
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear2/Transpose with ONNXTRT_Broadcast_1246
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose with ONNXTRT_Broadcast_1260
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose with ONNXTRT_Broadcast_1268
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose with ONNXTRT_Broadcast_1276
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_score_head.2/Transpose
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_score_head.2/Transpose with ONNXTRT_Broadcast_1294
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape with /model/encoder/encoder.0/layers.0/self_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_4
[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_119
[X] Removing ONNXTRT_ShapeShuffle_119
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Transpose_5
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 with /model/encoder/encoder.0/layers.0/self_attn/Reshape_3
[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_4
[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_6
[X] Running: ShuffleShuffleFusion on /model/encoder/Transpose_1
[X] ShuffleShuffleFusion: Fusing /model/encoder/Transpose_1 with /model/encoder/Reshape_1
[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape with /model/decoder/Transpose
[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_1 with /model/decoder/Transpose_1
[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_2 with /model/decoder/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape with /model/decoder/decoder/layers.0/cross_attn/Transpose
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape with /model/decoder/decoder/layers.1/cross_attn/Transpose
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape with /model/decoder/decoder/layers.2/cross_attn/Transpose
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_2 with /model/decoder/decoder/layers.0/self_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape with /model/decoder/decoder/layers.0/self_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_1 with /model/decoder/decoder/layers.0/self_attn/Transpose_4
[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_354
[X] Removing ONNXTRT_ShapeShuffle_354
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Transpose_5
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Transpose_5 with /model/decoder/decoder/layers.0/self_attn/Reshape_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_4
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_4 with /model/decoder/decoder/layers.0/self_attn/Transpose_6
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_391
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_391 with /model/decoder/decoder/layers.0/cross_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_391 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_391 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2 with /model/decoder/decoder/layers.0/cross_attn/Reshape_9
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Transpose_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Transpose_1 with /model/decoder/decoder/layers.0/cross_attn/Reshape_5
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape_10
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape_10 with /model/decoder/decoder/layers.0/cross_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_2 with /model/decoder/decoder/layers.1/self_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape with /model/decoder/decoder/layers.1/self_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_1 with /model/decoder/decoder/layers.1/self_attn/Transpose_4
[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_676
[X] Removing ONNXTRT_ShapeShuffle_676
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Transpose_5
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Transpose_5 with /model/decoder/decoder/layers.1/self_attn/Reshape_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_4
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_4 with /model/decoder/decoder/layers.1/self_attn/Transpose_6
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_711
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_711 with /model/decoder/decoder/layers.1/cross_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_711 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_711 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2 with /model/decoder/decoder/layers.1/cross_attn/Reshape_9
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Transpose_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Transpose_1 with /model/decoder/decoder/layers.1/cross_attn/Reshape_5
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape_10
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape_10 with /model/decoder/decoder/layers.1/cross_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_2
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_2 with /model/decoder/decoder/layers.2/self_attn/Transpose_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape with /model/decoder/decoder/layers.2/self_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_1 with /model/decoder/decoder/layers.2/self_attn/Transpose_4
[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_998
[X] Removing ONNXTRT_ShapeShuffle_998
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Transpose_5
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Transpose_5 with /model/decoder/decoder/layers.2/self_attn/Reshape_3
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_4
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_4 with /model/decoder/decoder/layers.2/self_attn/Transpose_6
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1033
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1033 with /model/decoder/decoder/layers.2/cross_attn/Transpose_2
[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1033 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2
[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1033 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2 with /model/decoder/decoder/layers.2/cross_attn/Reshape_9
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Transpose_1
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Transpose_1 with /model/decoder/decoder/layers.2/cross_attn/Reshape_5
[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape_10
[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape_10 with /model/decoder/decoder/layers.2/cross_attn/Transpose_3
[X] QDQ graph optimizer - constant folding of Q/DQ initializers
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_5
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_9
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_13
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_17
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_19
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_23
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_27
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_31
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_35
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_41
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_45
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_49
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_53
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_57
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_63
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_67
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_71
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_75
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_79
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_85
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_89
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_93
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_95
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_97
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_101
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_132
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_146
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_161
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_165
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_169
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_173
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_177
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_179
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_183
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_187
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_191
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_195
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_199
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_203
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_205
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_209
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_213
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_217
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_221
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_225
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_229
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_231
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_235
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_239
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_243
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_247
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_251
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_255
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_257
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_261
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_263
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_265
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_269
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_276
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_290
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_296
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_304
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_312
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_323
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_331
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_367
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_376
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_384
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_577
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_591
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_599
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_613
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_621
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_629
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_687
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_696
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_704
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_899
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_913
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_921
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_935
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_943
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_951
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1009
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1018
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1026
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1221
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1235
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1243
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1257
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1265
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1273
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear
[X] Removing tmp_weight_1291
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_0
[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_6
[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_14
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_20
[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_28
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_36
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_46
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_54
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_64
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_72
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_80
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_90
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_96
[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_102
[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_147
[X] Removing /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_166
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_174
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_180
[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_188
[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_196
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_204
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_210
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_218
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_226
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_232
[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_240
[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_248
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_256
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_262
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_266
[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_277
[X] Removing /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_297
[X] Removing /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_313
[X] Removing /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_332
[X] Removing /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_377
[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_578
[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_600
[X] Removing /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_622
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_688
[X] Removing /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_705
[X] Removing /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_914
[X] Removing /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_936
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_952
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1019
[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1222
[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1244
[X] Removing /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1266
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1292
[X] Removing /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_3
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_4
[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_7
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_8
[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_11
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_12
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_15
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_16
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_21
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_22
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_25
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_26
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_29
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_37
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_30
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_39
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_40
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_34
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_43
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_44
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_47
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_48
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_51
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_59
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_52
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_61
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_62
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_56
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_65
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_66
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_69
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_70
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_73
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_81
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_74
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_83
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_84
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_78
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_87
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_88
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_91
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_92
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_99
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_100
[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_130
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_131
[X] Removing /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_144
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_145
[X] Removing /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_159
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_160
[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_163
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_164
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_167
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_168
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_171
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_172
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_175
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_176
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_181
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_182
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_185
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_186
[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_189
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_190
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_193
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_194
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_197
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_198
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_201
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_202
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_207
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_208
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_211
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_212
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_215
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_216
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_219
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_220
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_223
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_224
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_227
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_228
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_233
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_234
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_237
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_238
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_241
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_242
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_245
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_246
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_249
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_250
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_253
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_254
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_259
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_260
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_267
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_268
[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_365
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_274
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_366
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_275
[X] Removing /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_288
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_289
[X] Removing /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_302
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_303
[X] Removing /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_310
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_311
[X] Removing /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_321
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_322
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_329
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_330
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_374
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_375
[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_575
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_576
[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_589
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_590
[X] Removing /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_597
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_598
[X] Removing /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_611
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_612
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_619
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_620
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_627
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_628
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear
[X] Removing tmp_weight_647
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear
[X] Removing tmp_weight_648
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear
[X] Removing tmp_weight_653
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear
[X] Removing tmp_weight_654
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_694
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_695
[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_897
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_898
[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_911
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_912
[X] Removing /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_919
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_920
[X] Removing /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_933
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_934
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_941
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_942
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_949
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_950
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear
[X] Removing tmp_weight_969
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear
[X] Removing tmp_weight_970
[X] Removing /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear
[X] Removing tmp_weight_975
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear
[X] Removing tmp_weight_976
[X] Removing /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1016
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1017
[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1219
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1220
[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1233
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1234
[X] Removing /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1241
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1242
[X] Removing /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1255
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1256
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1263
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1264
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_1271
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_1272
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_2
[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_10
[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_18
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_24
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_32
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_42
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_50
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_58
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_68
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_76
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_86
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_94
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_98
[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_133
[X] Removing /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_162
[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_170
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_178
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_184
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_192
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_200
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_206
[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_214
[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_222
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_230
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_236
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_244
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_252
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_258
[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_264
[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_270
[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_291
[X] Removing /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_305
[X] Removing /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_324
[X] Removing /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_368
[X] Removing /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_385
[X] Removing /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_592
[X] Removing /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_614
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_630
[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_697
[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_900
[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_922
[X] Removing /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_944
[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1010
[X] Removing /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1027
[X] Removing /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1236
[X] Removing /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1258
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear
[X] Removing tmp_weight_1274
[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_38
[X] Removing /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_33
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_60
[X] Removing /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_55
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Removing tmp_weight_82
[X] Removing /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/Constant_1_output_0
[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing tmp_weight_77
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0
[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_3 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.1/self_attn/Softmax to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_3 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_4 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.0/self_attn/Softmax to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.2/self_attn/Softmax to be part of self-attention pattern.
[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.
[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_4 to be part of self-attention pattern.
[X] Found /model/encoder/encoder.0/layers.0/self_attn/Softmax to be part of self-attention pattern.
[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.
[X] Found and reassigned Myelin backends for Self-Attention nodes
[X] After Myelin optimization: 470 layers
[X] QDQ graph optimizer - constant folding of Q/DQ initializers
[X] QDQ graph optimizer forward pass - DQ motions and fusions
[X] QDQ graph optimizer backward pass
[X] QDQ graph optimizer quantization pass - Generate quantized ops
[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear
[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear
[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.0/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.0/Add with /model/backbone/res_layers.0/blocks.0/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.1/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.1/Add with /model/backbone/res_layers.0/blocks.1/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.0/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.0/Add with /model/backbone/res_layers.1/blocks.0/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.1/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.1/Add with /model/backbone/res_layers.1/blocks.1/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.0/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.0/Add with /model/backbone/res_layers.2/blocks.0/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.1/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.1/Add with /model/backbone/res_layers.2/blocks.1/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.0/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.0/Add with /model/backbone/res_layers.3/blocks.0/act/Relu
[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.1/Add
[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.1/Add with /model/backbone/res_layers.3/blocks.1/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_1/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_1/norm/BatchNormalization with /model/backbone/conv1/conv1_1/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_2/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_2/norm/BatchNormalization with /model/backbone/conv1/conv1_2/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_3/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_3/norm/BatchNormalization with /model/backbone/conv1/conv1_3/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu
[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization
[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu
[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_1.conv.weight with /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_2.conv.weight with /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_3.conv.weight with /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight with /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.2.conv.weight with /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.0.conv.weight with /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.1.conv.weight with /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.0.conv.weight with /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight with /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight with /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight with /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.1.conv.weight with /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight with /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight with /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight with /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.0.conv.weight with /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight with /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight with /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight with /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.1.conv.weight with /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight with /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight with /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv3.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight with /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.0.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.0.conv.weight with /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.1.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.1.conv.weight with /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.2.conv.weight
[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.2.conv.weight with /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] Running: VanillaSwapWithFollowingQ on /model/backbone/MaxPool
[X] Swapping /model/backbone/MaxPool with /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_2
[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_3
[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_4
[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_5
[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize
[X] Swapping /model/encoder/Resize with /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize_1
[X] Swapping /model/encoder/Resize_1 with /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Running: PointWiseFusion on /model/encoder/lateral_convs.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.0/act/Sigmoid with /model/encoder/lateral_convs.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv1/act/Sigmoid with /model/encoder/fpn_blocks.0/conv1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv2/act/Sigmoid with /model/encoder/fpn_blocks.0/conv2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul) with /model/encoder/fpn_blocks.0/Add
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv3/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv3/act/Sigmoid with /model/encoder/fpn_blocks.0/conv3/act/Mul
[X] Running: PointWiseFusion on /model/encoder/lateral_convs.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.1/act/Sigmoid with /model/encoder/lateral_convs.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv1/act/Sigmoid with /model/encoder/fpn_blocks.1/conv1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv2/act/Sigmoid with /model/encoder/fpn_blocks.1/conv2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul) with /model/encoder/fpn_blocks.1/Add
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)
[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv3/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv3/act/Sigmoid with /model/encoder/fpn_blocks.1/conv3/act/Mul
[X] Running: PointWiseFusion on /model/encoder/downsample_convs.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.0/act/Sigmoid with /model/encoder/downsample_convs.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv1/act/Sigmoid with /model/encoder/pan_blocks.0/conv1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv2/act/Sigmoid with /model/encoder/pan_blocks.0/conv2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul) with /model/encoder/pan_blocks.0/Add
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv3/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv3/act/Sigmoid with /model/encoder/pan_blocks.0/conv3/act/Mul
[X] Running: PointWiseFusion on /model/encoder/downsample_convs.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.1/act/Sigmoid with /model/encoder/downsample_convs.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv1/act/Sigmoid with /model/encoder/pan_blocks.1/conv1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv2/act/Sigmoid with /model/encoder/pan_blocks.1/conv2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul) with /model/encoder/pan_blocks.1/Add
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul
[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul)
[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)
[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv3/act/Sigmoid
[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv3/act/Sigmoid with /model/encoder/pan_blocks.1/conv3/act/Mul
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_1/conv/Conv
[X] Removing /model/backbone/conv1/conv1_1/norm/BatchNormalization + /model/backbone/conv1/conv1_1/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_2/conv/Conv
[X] Removing /model/backbone/conv1/conv1_2/norm/BatchNormalization + /model/backbone/conv1/conv1_2/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_3/conv/Conv
[X] Removing /model/backbone/conv1/conv1_3/norm/BatchNormalization + /model/backbone/conv1/conv1_3/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu
[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.0/conv/Conv
[X] Removing /model/encoder/input_proj.0/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.1/conv/Conv
[X] Removing /model/encoder/input_proj.1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.0/conv/Conv
[X] Removing /model/encoder/lateral_convs.0/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.1/conv/Conv
[X] Removing /model/encoder/lateral_convs.1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.0/conv/Conv
[X] Removing /model/encoder/downsample_convs.0/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.1/conv/Conv
[X] Removing /model/encoder/downsample_convs.1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization
[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.0/conv/Conv with PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.1/conv/Conv with PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv
[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_1/conv/Conv
[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_2/conv/Conv
[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_3/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_3/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_3/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1 and /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.2/conv/Conv
[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.0/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.0/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.0/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.0/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.0/conv/Conv
[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.1/conv/Conv
[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.0/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.0/conv/Conv
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.1/conv/Conv
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.2/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.2/conv/Conv
[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear
[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.1/conv/Conv
[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.1/conv/Conv
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1
[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0
[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear
[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool
[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool
[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool
[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear
[X] Removing /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear
[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))
[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))
[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))
[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))
[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))
[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))
[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear
[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))
[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))
[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear
[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_1/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_2/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_3/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.0/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.1/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.0/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.1/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Running: ConstWeightsFusion on model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.0/conv/Conv
[X] Running: ConstWeightsFusion on model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.1/conv/Conv
[X] Running: ConstWeightsFusion on model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear
[X] ConstWeightsFusion: Fusing model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.2/conv/Conv
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv with /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv
[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] After dupe layer removal: 92 layers
[X] After final dead-layer removal: 92 layers
[X] After tensor merging: 92 layers
[X] QDQ graph optimizer quantization epilogue pass
[X] QDQ optimization pass
[X] QDQ graph optimizer constant fold dangling QDQ pass
[X] Running: QDQToCopy on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] Swap the layer type of /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Swap the layer type of /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] Swap the layer type of /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] Swap the layer type of /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 from QUANTIZE to kQDQ
[X] Running: QDQToCopy on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] Swap the layer type of /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 from QUANTIZE to kQDQ
[X] After dupe layer removal: 92 layers
[X] After final dead-layer removal: 92 layers
[X] After tensor merging: 92 layers
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Modifying configuration of model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Modifying configuration of model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Modifying configuration of model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Modifying configuration of model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] After vertical fusions: 92 layers
[X] After dupe layer removal: 92 layers
[X] After final dead-layer removal: 92 layers
[X] After tensor merging: 92 layers
[X] After slice removal: 92 layers
[X] Eliminating concatenation /model/encoder/Concat_5
[X] Retargeting /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Retargeting /model/encoder/Concat_5_/model/encoder/lateral_convs.0/act/Mul_output_0_clone_1 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Eliminating concatenation /model/encoder/Concat_4
[X] Retargeting /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Retargeting /model/encoder/Concat_4_/model/encoder/lateral_convs.1/act/Mul_output_0_clone_1 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Eliminating concatenation /model/encoder/Concat_3
[X] Generating copy for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.
[X] Retargeting /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] Eliminating concatenation /model/encoder/Concat_2
[X] Generating copy for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.
[X] Retargeting /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0
[X] After concat removal: 90 layers
[X] Trying to split Reshape and strided tensor
[X] Graph optimization time: 0.0628761 seconds.
[X] Building graph using backend strategy 2
[V] Local timing cache in use. Profiling results in this builder pass will not be stored.
[X] Constructing optimization profile number 0 [1/1].
[X] Applying generic optimizations to the graph for inference.
[X] Reserving memory for host IO tensors. Host: 0 bytes
[X] =============== Computing costs for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(3276800,102400,320,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_chw_int8int8_tilesize16x16_k32_fltsteps1_threadspercta256_r3s3_u2v2_scalebias_relu Tactic: 0x11764d94950382f8 Time: 0.00590728
[X] Tactic Name: ampere_first_layer_filter3x3_imma_fwd Tactic: 0x9ae0c0d2fb3a01e5 Time: 0.00648226
[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00656919 seconds. Fastest Tactic: 0x11764d94950382f8 Time: 0.00590728
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x11764d94950382f8
[X] *************** Autotuning format combination: Int8(409600,409600:4,640,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize16x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x3d988d07a78b0918 Time: 0.00502973
[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x5cc792a989a1d1a6 Time: 0.00504326
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0098607
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00975695
[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0115099 seconds. Fastest Tactic: 0x3d988d07a78b0918 Time: 0.00502973
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x3d988d07a78b0918
[X] *************** Autotuning format combination: Int8(409600,1:16,640,1) -> Int8(204800,1:16,640,2) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0391324
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0179509
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0146602
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0149885
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0302116
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0255185
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0373713
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0140996
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0449347
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0267487
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0290693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0160493
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0298693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0157542
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0305532
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0306686
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0339445
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0404705
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0112821
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.015183
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0142031
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0111861
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0435147
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0142453
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0190287
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0120053
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0172651
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0234396
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0368981
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.03072
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0305028
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0308567
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0453867
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0262285
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0144769
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.013968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0186602
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.011258
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0432813
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.01412
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0142373
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0447827
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0434053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0172987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0114044
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.01613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0113362
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0149959
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0314579
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0156155
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0182495
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0196875
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.015066
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0121539
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0457413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0144364
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0157804
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0143996
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0150071
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0237724
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0282738
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0395804
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0129809
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0292169
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0373239
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0143698
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0140738
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.010641
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0321047
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0296249
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.031265
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0118878
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0322259
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0282871
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0404101
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0111936
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0291271
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0133165
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0102248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0165095
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0371923
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0180761
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0123196
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0124302
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.036576
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.042228
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0154565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.027912
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0153425
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0171685
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0170587
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0236615
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0472133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0244693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0110348
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0174853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0275323
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0162723
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.049693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0111701
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0254141
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.039296
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.00998275
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0147562
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0138679
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0245737
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0160376
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0183775
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0405807
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0150201
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0287964
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.0284116
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0290569
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0138287
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0132492
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0109953
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0142822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.0392486
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0181339
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0132611
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.012753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0279671
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0112302
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0220793
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0167333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0150854
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0147293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0138911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0253585
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.016574
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.00995796
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0494461
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0136435
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0283022
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0399016
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0280862
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0164485
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0368512
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.028032
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0485379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0142916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0118672
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0106903
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0166588
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0177179
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0129362
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0139773
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0111556
[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.402971 seconds. Fastest Tactic: 0x2958c78a91e58cda Time: 0.00995796
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2958c78a91e58cda
[X] *************** Autotuning format combination: Int8(409600,409600:32,640,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0261817
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0197998
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0341685
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0355957
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0163002
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0194886
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0302284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0177319
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0262942
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0204122
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0297831
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0293111
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0165887
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0248008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0273715
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0299156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0275577
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0142813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0246293
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0164714
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0198444
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0158216
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0152092
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0212553
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0276078
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.043164
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0296276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0186732
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0196831
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0145322
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0232064
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0316208
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0121436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0450533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0158555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0272066
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0172763
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0484251
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0198626
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0308732
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.015027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0287867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0260406
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.027824
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0299929
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0123765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0201506
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0355765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0251604
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0215287
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0267577
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0159929
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0199197
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0197151
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.038355
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.043656
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.016227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0296258
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0267586
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0196637
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0158913
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0494248
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0172176
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0126447
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0133038
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0187644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0145758
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0137097
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.044248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0262515
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0315219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0200031
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0303864
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0196775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0271754
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0191834
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0447813
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0218453
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0151439
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0271065
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0155704
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0171424
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0270105
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0344331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.019059
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0264812
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0296596
[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.258073 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.0121436
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73
[X] =============== Computing costs for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(3276800,102400,320,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0205453
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0201826
[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00577716 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0201826
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(204800,1:16,640,2) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.03496
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0216807
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0166715
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0167056
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0400213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0323491
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0508602
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0153804
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0398222
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.024048
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0376403
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0205791
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0411354
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0196107
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0274068
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.041696
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0303166
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0361995
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0132176
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0164704
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0139827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0127261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0385067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0182916
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0259019
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0130913
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0226062
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0298187
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.050365
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0273387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0403982
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.042684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0404207
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0238476
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0149588
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0138108
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.020932
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0119642
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0383431
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0137783
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0146008
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0394121
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.038573
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0200314
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.012819
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0190513
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0134609
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.016984
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.043364
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0168427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0259881
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0259873
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0161534
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0137946
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.064128
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0178712
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0173803
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0176298
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0165369
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0342997
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0473467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0601707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0141591
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0413381
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0375964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.016542
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0196147
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0141693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.050787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0291911
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0307646
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0144013
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0509882
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0275799
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0392095
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.014076
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.028768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0167696
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0117565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0257543
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0371117
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0263508
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0153857
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0153382
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0368331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0646116
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0230044
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.045404
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0189553
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0260455
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0225038
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0320999
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0731456
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0247368
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0133255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0264263
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.04502
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0205879
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0759125
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0134558
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0345995
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0382839
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0123078
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0190281
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0179065
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0367403
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.021614
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0266667
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0393019
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0180508
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.046516
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.046208
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0482118
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0184752
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0155816
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0137365
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.020916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.0599253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0265091
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0163464
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0154288
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0412966
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0141644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0319185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0207191
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.020069
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0186015
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0168896
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0259971
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0256558
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.0122221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0747925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0166832
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.043576
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0388634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0277276
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0261596
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0369344
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.043352
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0740693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0161458
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0144782
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0143733
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0222443
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.026775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0164968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0168571
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0134238
[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.422751 seconds. Fastest Tactic: 0x575184c61f040550 Time: 0.0117565
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x575184c61f040550
[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xde3a6a3727f31f34 Time: 0.948224
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0257444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0190643
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbbff0ceb48c87bac Time: 0.925013
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x9c9fd7d74c020c9d Time: 0.488107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0341803
[X] Fast skip Tactic:0xb97409e537081e4c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb97409e537081e4c Time: 3.57376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0348832
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe5f40c565f9c8a09 Time: 0.0475827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0137933
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x4e679e1c8dcfbe3c Time: 0.945493
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0174309
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0283573
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x645c57c8d2bdcafa Time: 0.0530941
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x13cb22041bcdf2f5 Time: 0.858453
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0165562
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0256468
[X] Fast skip Tactic:0xa1513318dd2f5314 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa1513318dd2f5314 Time: 2.36765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0200433
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0290133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0272968
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0155263
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0238301
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2d0a836ca3b48b55 Time: 0.616715
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x307c1c762709b00e Time: 0.83456
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0266576
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0278489
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9826a9122a4e1bac Time: 0.408565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0269227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0128008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0223552
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0159035
[X] Fast skip Tactic:0x9c391ea4731a473c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9c391ea4731a473c Time: 1.86061
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe6fb49f176c8ac20 Time: 0.241008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0195846
[X] Fast skip Tactic:0x136deb7724d5b954 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x136deb7724d5b954 Time: 1.00352
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x718a86dfcb201f10 Time: 0.210373
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0148795
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x415d0d459f475d7a Time: 0.0705771
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0145489
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x7f9cac2d273e24da Time: 0.0399727
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x730183d1b4e07af0 Time: 0.0602702
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0211913
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xab1e201ca705d0dd Time: 0.610059
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x2c80f3b4623a1878 Time: 0.213099
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4dc022b90c990350 Time: 0.0373689
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0272328
[X] Fast skip Tactic:0x8286a69028b3e3f0 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x8286a69028b3e3f0 Time: 4.02944
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.042948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0290916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0177387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0187947
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0139671
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4480741a5fe7a6b0 Time: 0.0310545
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x8c7efb20a3cfa7ec Time: 0.568981
[X] Fast skip Tactic:0xb7622317db162586 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb7622317db162586 Time: 1.06598
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0227541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0289671
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0100734
[X] Fast skip Tactic:0x89a3827f636f5c26 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x89a3827f636f5c26 Time: 1.03731
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xdb35ad3ee7e5f3a9 Time: 0.0634631
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xfe34f7b3aa1f6429 Time: 0.0415573
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0446467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0131676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0269506
[X] Fast skip Tactic:0x3836d6c48cd9dbee which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3836d6c48cd9dbee Time: 1.20832
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x0f57663c97a6a89c Time: 0.477867
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5801f8b0fa1b5d5c Time: 0.807595
[X] Fast skip Tactic:0x48fd1cd53c4157a8 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x48fd1cd53c4157a8 Time: 4.66227
[X] Fast skip Tactic:0x31768067dfa77e0e which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x31768067dfa77e0e Time: 1.11309
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x0a6a5850a77efc64 Time: 0.950059
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0144347
[X] Fast skip Tactic:0xe47e7c8e9e121924 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xe47e7c8e9e121924 Time: 3.68026
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x09cde4f526284108 Time: 0.111003
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x244ad5cff0ca2eb5 Time: 0.432512
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x853ead83f0b1020c Time: 0.755029
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0482347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.019261
[X] Fast skip Tactic:0x3620fc3660c7e024 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3620fc3660c7e024 Time: 1.91795
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xaca2d8f22e95cba6 Time: 0.56832
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.02852
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x23f62d21795a35ce Time: 0.891221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0133871
[X] Fast skip Tactic:0x6b2a895dc9dde74c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x6b2a895dc9dde74c Time: 2.05824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0269227
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5600d95cf91aed70 Time: 0.0371461
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x032a0ef3f4005984 Time: 0.758229
[X] Fast skip Tactic:0xdf8cd3fb81a9e498 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xdf8cd3fb81a9e498 Time: 3.93523
[X] Fast skip Tactic:0x2e05c6cb8ae0ad7c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2e05c6cb8ae0ad7c Time: 4.18714
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0254392
[X] Fast skip Tactic:0xd916513230270d0c which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xd916513230270d0c Time: 1.16531
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0262605
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0296169
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.01129
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.019771
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0351797
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0241311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.020884
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0267824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0134502
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xecb45af50ce22fe9 Time: 0.0355083
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0197816
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0194258
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0376936
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0433707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0159157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0276439
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0262728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0193849
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0150637
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0491871
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0168683
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.010716
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0120411
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0173824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0131594
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0135723
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.043892
[X] Fast skip Tactic:0x5642a4e167e8f364 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x5642a4e167e8f364 Time: 2.10842
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0253813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0308606
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0184561
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0281858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0177269
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0264927
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0173333
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9efe56660532ec2c Time: 0.469333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0447067
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.02154
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9ebc2bdb9bc0f238 Time: 0.123911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0139422
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa25e76bff47b753d Time: 0.769024
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0273789
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x11aaa3b552fd1244 Time: 0.617813
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xb99e8d5a01f89b1d Time: 0.489813
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbe2275b488688066 Time: 0.868011
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x34abf9381f0785c4 Time: 0.514464
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0145679
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0163576
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xc52cdc7983541cc4 Time: 0.421888
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x06f777ac34a0a24e Time: 0.667307
[X] Fast skip Tactic:0xc1336bcfda004054 which exceed time limit during pre-run
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xc1336bcfda004054 Time: 1.7367
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0264115
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x36ca788956376575 Time: 0.450219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0340757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0172261
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x76dcfa8e7440813a Time: 0.0382661
[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x54c7919e8f324660 Time: 0.111136
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0260792
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0277169
[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.749852 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.0100734
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73
[X] =============== Computing costs for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(6553600,102400,320,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(204800,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0324228
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0319486
[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00555757 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0319486
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(409600,1:16,1280,4) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0362656
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0384273
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0293182
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0355595
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0682453
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0365589
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0947467
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0277686
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0450387
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.025888
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0728213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0353909
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0720363
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0347509
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0306938
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0741995
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0322938
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0371887
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0233209
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0333867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0272615
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0203112
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0445947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0316878
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0479756
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0236672
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0387117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.036192
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0929973
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0305891
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0682773
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0734805
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0451507
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0260398
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0309973
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0287742
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.039955
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0219313
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.044408
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0321115
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0278427
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0454813
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0445653
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0266059
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.019781
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0222777
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0245432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0194465
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0745557
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0284098
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0473827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.047088
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0328369
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0250644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.070336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0312194
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0306211
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0335019
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0223125
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0387887
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0861013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.108629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0311641
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0708053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0379437
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0346901
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0332897
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0242941
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0944053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0319777
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0327535
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.026679
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0950613
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0309285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0453053
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.025943
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0316199
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0191976
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0231957
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0464787
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0375241
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0480762
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0274954
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0275233
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.037389
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0773845
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0378062
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.083504
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0379046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0484145
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0412919
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0478735
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0792576
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0257157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0240625
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0483901
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0824256
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0371153
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0813141
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0249173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0478674
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.044288
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0218827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0237397
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0338987
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0684715
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.036528
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0492145
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0452507
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0333515
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0854293
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.084432
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0878667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0354027
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0336075
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0238956
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0366176
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.107517
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0485851
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0287084
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.027832
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0702507
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0258724
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0371058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.034608
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0233685
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0273042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0262958
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0266642
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0468387
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.0209753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0881093
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0228793
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0739264
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0446667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0307462
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0477211
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0374329
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0749888
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0814229
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0326109
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0268357
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0252952
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0387496
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.048253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0294071
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0312427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0258027
[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.464375 seconds. Fastest Tactic: 0xbe784bf72795274c Time: 0.0191976
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbe784bf72795274c
[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(204800,102400:32,320,1) ***************
[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0299209
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.019568
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0342677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0351083
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0162778
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0201794
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0336992
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0169243
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0139764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0257666
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0202384
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0114101
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0290338
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0348629
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.016258
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.024365
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0272246
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0334731
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0270285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0133751
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0259807
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0167483
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0202089
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0156558
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0151527
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0214307
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0314725
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.04312
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0130429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0300427
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.0180654
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.018547
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0188456
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0145558
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0234524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0343445
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0155806
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.044932
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0173835
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0271032
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.017623
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.048221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0201864
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0341525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0149505
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0269817
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0182288
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0260053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0265305
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0296587
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0186767
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.02141
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0354656
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0269432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0230279
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0271352
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0183966
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0203934
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0197955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0376853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0435813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0165399
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0351797
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0263245
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0196599
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0153406
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.049184
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0175269
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0163129
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0193938
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0177375
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0205522
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.019328
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0440133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0261547
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0317585
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.020856
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0339243
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0292204
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0267889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0283502
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.044728
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.021908
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.022442
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0136887
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0266946
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0150122
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0171301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0265789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0342123
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0245859
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0112217
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0266248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0356907
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0134942
[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.266323 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.0112217
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d
[X] =============== Computing costs for /model/backbone/MaxPool
[X] *************** Autotuning format combination: Int8(1638400,102400:4,320,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0xfa45342d0e1d409a Time: 0.0115138
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0xa88280db27a5d09f Time: 0.0101198
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0x3acbbd865df539ed Time: 0.0104136
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0x3d35b618fd3968f1 Time: 0.0101854
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0xbeae815d02985cde Time: 0.0127348
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0xe09c44661dba1b5c Time: 0.00905139
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0xb331e89337ca2bad Time: 0.0101062
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0xc4335d27b08156bf Time: 0.0124733
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0xbadabf84bb0f736c Time: 0.00839413
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x80d8e857bc044afb Time: 0.0124622
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0x199aaf5950022512 Time: 0.0129223
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x67734dfa5b8c00c1 Time: 0.00786654
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0xf307ae442c39b4a3 Time: 0.0139502
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0x2eae5c3accbac70e Time: 0.012969
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x9fe4504b077a26fb Time: 0.0115093
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0x63077323e21b2f73 Time: 0.0118683
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x9dff93820f6f4021 Time: 0.0104517
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x923de46f0f4ddb62 Time: 0.0112352
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0xc9170fb073b2e283 Time: 0.00953935
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0xd3ce7ffb6015b945 Time: 0.0119623
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x072517eca2b23ed1 Time: 0.0120571
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0x1340f65033fdc032 Time: 0.00910068
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0x47a86a624f206290 Time: 0.00995106
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x9a01981cafa3113d Time: 0.00984439
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x69dd2a2a81e4ca53 Time: 0.0089541
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x4a8c38f58c13d6ac Time: 0.0121051
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0x5d711a295c873956 Time: 0.0123204
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0x1dee9180e9950aa0 Time: 0.00856123
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x899a723e9e20bec2 Time: 0.013609
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0xedb816f1de89af60 Time: 0.0162108
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0xa01139e8f028471d Time: 0.0173893
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x898e8c271f222050 Time: 0.0145489
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0xc04763fe0916790d Time: 0.0093354
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0x6e2321b421289b4f Time: 0.0123131
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0x27ecc653ee9e3337 Time: 0.0126712
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x9351f452d5078ab3 Time: 0.00929185
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0xbee85cb73ffdd634 Time: 0.0113749
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x0165782a59f89027 Time: 0.00685344
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0xcee9042ed37eb39f Time: 0.0101679
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0xb474d8546167b9fe Time: 0.00920908
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0x74fa51ff328fc089 Time: 0.015551
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0x543380407ea3cd6f Time: 0.0141511
[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0x3465da56879df37f Time: 0.00903726
[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kMAX Tactic: 0x1f6c40e3e09ec730 Time: 0.00621017
[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.125288 seconds. Fastest Tactic: 0x1f6c40e3e09ec730 Time: 0.00621017
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x1f6c40e3e09ec730
[X] *************** Autotuning format combination: Int8(204800,102400:32,320,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])
[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX Tactic: 0x94215b398b8eb3ba Time: 0.00654913
[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.00291292 seconds. Fastest Tactic: 0x94215b398b8eb3ba Time: 0.00654913
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x94215b398b8eb3ba
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.021104
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0205101
[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00558734 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0205101
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0129157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0146249
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0102361
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00973318
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0115822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0131934
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0131208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0107463
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0149101
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0101925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0127617
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0138364
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0133568
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0115594
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0121825
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0118246
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.009824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0139707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0118966
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0120011
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0116855
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00868868
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0130749
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0136418
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.008848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.013687
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0110176
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.011548
[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.067616 seconds. Fastest Tactic: 0x5f5aa01645d48746 Time: 0.00868868
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5f5aa01645d48746
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.01528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00977493
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0154526
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0183815
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00965028
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0113166
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0160163
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00811657
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.008528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0161255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00986478
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0074195
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.012429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0166912
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0089106
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.012719
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0130675
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0160137
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0118165
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00993255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0139373
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0109058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.012574
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00858448
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00903784
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0124753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0159553
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0201838
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00685366
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0138534
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00975299
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0100835
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.008368
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00895074
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0125227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0166273
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00939585
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.023242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0103955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0129251
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00989961
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0220533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0111538
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0164673
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0099589
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0118106
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0097539
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0161178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0114798
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0126266
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0111641
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0127127
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0166014
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0144271
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0133918
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0153125
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0107782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0125108
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0122853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0193825
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0226994
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00992251
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0171669
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0163647
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0124642
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00927555
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0222812
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0112711
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00973531
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0119014
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00823519
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00941718
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00910818
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0202604
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0128291
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0143898
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0116708
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.01629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0135078
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0164759
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.013111
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0205496
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0126894
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0103493
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00697711
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0116303
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00900987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00997365
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0114812
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0161783
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0125128
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00681012
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0149806
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0172053
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00708978
[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.262821 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.00681012
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0219207
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.020954
[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00550249 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.020954
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.010613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0172059
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0120983
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0149268
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0123067
[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0119732 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.010613
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0174608
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0134737
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0239025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0160234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0168997
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0139213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0169915
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0136734
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0142747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0166992
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0157673
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0168352
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0137937
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0172752
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0135659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.013449
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0127447
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0173483
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0135765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.015537
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0135723
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0172843
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0242431
[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0534388 seconds. Fastest Tactic: 0xb936321f82fd390c Time: 0.0127447
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb936321f82fd390c
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00824716
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0082599
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00846133
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00861019
[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.011627 seconds. Fastest Tactic: 0xa4ae2d82115c3e83 Time: 0.00824716
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa4ae2d82115c3e83
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.0114283
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0104767
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0125096
[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00717607 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.0104767
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00954758
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00793178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00820839
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00897825
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00816338
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00902227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.011923
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0086411
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00999906
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00964541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00977006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00919582
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00808254
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00844453
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00889572
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0106127
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0109502
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00818602
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0085456
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00819356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00884912
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0104149
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00983874
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00908512
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00814907
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00829216
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00789061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00858585
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0109382
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00973196
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00907906
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00877333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0109822
[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0832099 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00789061
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv
[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x0f47434ace2a7d18, 0.0205101 ms
[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4)] got cached result: CaskConvolution, tactic 0x5f5aa01645d48746, 0.00868868 ms
[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x9dafb2758560cc1d, 0.00681012 ms
[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0227008
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0217127
[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00559558 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0217127
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0124887
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0215473
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0141827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0181513
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.014384
[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0120681 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.0124887
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0149064
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0148257
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0223332
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0176006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0180194
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0113582
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0151174
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0151049
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0119451
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0174848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0169728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.017464
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0113308
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0154962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.012197
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0136687
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0138679
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0183085
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0138517
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0131253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0120979
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0147488
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0227413
[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0538409 seconds. Fastest Tactic: 0x0e07dc8353bf7e9f Time: 0.0113308
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0e07dc8353bf7e9f
[X] =============== Computing costs for /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(102400,6400:4,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00323749
[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00320649 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00323749
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(12800,6400:32,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00298248
[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00334246 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00298248
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0161067
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.015281
[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00575674 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.015281
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.00727211
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00665475
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00869333
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00614245
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.00728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00748207
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00725565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00629374
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00670165
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.00920677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00718252
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00664596
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00723449
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0112
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00700244
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00740035
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00631165
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.007592
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00682645
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0115101
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0113372
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00767879
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00707733
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0065805
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00811606
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00747544
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00638994
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0112121
[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0760167 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00614245
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b
[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00751194
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00610017
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0062319
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.00808584
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00670187
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0066537
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00910126
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00760364
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.00944978
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00611336
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0121364
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00975055
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00716641
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00963901
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0077889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00945392
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0116414
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00630622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00777352
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.00802819
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00671488
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00683276
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00700489
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00754323
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00773479
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0112974
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0061504
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00591832
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00843333
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00681643
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.00914191
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00973593
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00769891
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00964907
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00824611
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00762545
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00685823
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0121924
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00691548
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00982149
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00676181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0119836
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.00928118
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0115115
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.012412
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00823051
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00774255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00941955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00776186
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00809295
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00720817
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00841787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00940681
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00648267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00836373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.00934637
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00638631
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0100775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00943407
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.00943022
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00878372
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0123383
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00840667
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00803073
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0084632
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00789705
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00688849
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00662065
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0113927
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.00744865
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00637927
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00697644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00930281
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00872916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00959543
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00821802
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0114596
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00764824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00702555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0114297
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00842267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00667008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0114052
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.00926429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00827005
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00685845
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.010597
[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.242264 seconds. Fastest Tactic: 0x705baf38e41eee0b Time: 0.00591832
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x705baf38e41eee0b
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0280871
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0219867
[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00540077 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0219867
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00907128
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.013687
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00906523
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0117337
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.009288
[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0123033 seconds. Fastest Tactic: 0xa8b56a226b057463 Time: 0.00906523
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa8b56a226b057463
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0211153
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0117304
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0204367
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0103771
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0129087
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0136981
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0134622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0111367
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0148925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0127384
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0103842
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0131106
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0134592
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0138027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0097347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00978347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0103871
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0133141
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00973989
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0116693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00980602
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.020858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0205271
[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.05388 seconds. Fastest Tactic: 0xad886d4d69834922 Time: 0.0097347
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xad886d4d69834922
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] *************** Autotuning format combination: Int8(102400,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00716028
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00707911
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00735327
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00711489
[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0119885 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00707911
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00697889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00674347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00816078
[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00782 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00674347
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00945748
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00540353
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00613372
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00618726
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0063032
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0088699
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00640704
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00681535
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00658656
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00604324
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00602533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0062724
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00554387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00556622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.006986
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00603314
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00611142
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00566039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00608136
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00560693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0086854
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00688805
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00704911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00688653
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0149695
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00569582
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00552551
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00595518
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00665663
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00635431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00599714
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00603962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.007136
[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.10662 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00540353
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0294996
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0267602
[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00562421 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0267602
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0106363
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00872506
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0133385
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00821568
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0105253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0107327
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0102413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00847733
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00874366
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.014472
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0101253
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00862468
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0101191
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0178077
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00922897
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0106747
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00869935
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0105923
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00919149
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0183585
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0181665
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0109891
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0100354
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00868595
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0118315
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0106387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00899481
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0178133
[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0812927 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00821568
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.010451
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00750341
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00794514
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0117491
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.008992
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00813528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0125792
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0106377
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00683646
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0142093
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00776707
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00636921
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0190933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0130396
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00965851
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0138172
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0109492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.012612
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0179705
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00877156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0103906
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.011371
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00917621
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00937215
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00931585
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0107453
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.00870209
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0107548
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0178504
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00985223
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0078973
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00741049
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00729484
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0163987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0121078
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00922666
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0132915
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0135906
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0102248
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0101437
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00798933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0146017
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00933037
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0111225
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00927526
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0193185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00828124
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.00978651
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0134187
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00911683
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.01808
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00764945
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0141124
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0177185
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.019392
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0113116
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0108759
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0144284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.010564
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0111812
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00969051
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00961036
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0143053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00901506
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0122758
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0143
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00882639
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0133268
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0142556
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0142551
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0124804
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.019437
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0116156
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.010455
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0114254
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0108528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00761018
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00734539
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0179284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.010611
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00805511
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0084744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0128
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0106403
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0144271
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0105017
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0182131
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0108112
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00883004
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00961554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0181642
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0120842
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00934815
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0178678
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0140413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0102041
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0094634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00941541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0141564
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00967528
[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.275696 seconds. Fastest Tactic: 0x214f03e23f252333 Time: 0.00636921
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x214f03e23f252333
[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0288187
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0226404
[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00545444 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0226404
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00991121
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0164978
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00969966
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0133478
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0102132
[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0121704 seconds. Fastest Tactic: 0xa8b56a226b057463 Time: 0.00969966
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa8b56a226b057463
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.021766
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0116215
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0193541
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00993914
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0133939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0129428
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0125452
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0113596
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0141769
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0128373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00973866
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0132345
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0126858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0127581
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0088539
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00981929
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0108776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0138551
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00978895
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.010482
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00923704
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.021402
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0196963
[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0532799 seconds. Fastest Tactic: 0xad886d4d69834922 Time: 0.0088539
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xad886d4d69834922
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0276595
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0259339
[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00551945 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0259339
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0105923
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00702933
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0132472
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00685932
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0104267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0106923
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00675392
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00691548
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00704467
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0142707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00671275
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.006946
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00657485
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0177628
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00774061
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.010586
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00710763
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00704044
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00752498
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0183635
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0181782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0109939
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00652164
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00704889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0118227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00699222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00731663
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0178161
[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0779059 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00652164
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00869333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00735142
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00755129
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.011659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00631749
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00616029
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00758788
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0107843
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0143413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00780155
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0192367
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00846293
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00975787
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0138615
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0108903
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00755698
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0181985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00872916
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00690198
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0084856
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00874093
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00929363
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00840214
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00988925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00879944
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0180317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00697578
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00706089
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0122632
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00838454
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.013335
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0077183
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00864657
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0145554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00684952
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0110311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00634868
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0193274
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00606332
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00761745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00905427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0185221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.014168
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0180581
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0194175
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00916295
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00687913
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0144169
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00695266
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00714213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00896253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00685453
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00873381
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00866133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0122789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0142933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00855686
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00852107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0143164
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0085905
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0128328
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0194489
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00874886
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0088713
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00932503
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0111047
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00796851
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00760097
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0180918
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.010543
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00712874
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00616592
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00767636
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00722701
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0143813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00703533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0185742
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0099771
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00923012
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0183164
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.012435
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00892997
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0179941
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0141542
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00690438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0087215
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0103379
[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.23467 seconds. Fastest Tactic: 0x495da4d52da4de96 Time: 0.00606332
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x495da4d52da4de96
[X] =============== Computing costs for /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,1600:4,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00287485
[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00318524 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00287485
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00281188
[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00327089 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00281188
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0501623
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0380836
[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0053155 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0380836
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0115917
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0202861
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00941215
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00936504
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0120088
[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0120848 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00936504
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0345344
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0105717
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0340469
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0115542
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.018992
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0185233
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00990557
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0214913
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0109058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0114581
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0110768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0186157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0187769
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0113892
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00856588
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00878569
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0113575
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00960853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0150803
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0116035
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0341664
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0343637
[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0532515 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00856588
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00827161
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00778468
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00831428
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00792757
[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0116334 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00778468
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00665454
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00551502
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00558542
[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0082455 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00551502
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0104023
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00417933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00611413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00480107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00632232
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00970575
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00524017
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00701955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00949126
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00447488
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00457308
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00488085
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00425303
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00498929
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00719501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00483664
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00491937
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00434244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00453492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00439775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00949896
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00972068
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00647323
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00616824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00485519
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00437818
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00435491
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00483803
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00500585
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00608369
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00549001
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00606526
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00996235
[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0929046 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00417933
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0539596
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0487345
[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0054549 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0487345
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.017184
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0104165
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0223403
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0102581
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.017088
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0172389
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00959146
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0102953
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.010326
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0243642
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0095043
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0103153
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00946489
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0308451
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0110397
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.017144
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.010379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00985412
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0108762
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.031711
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0316247
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0175708
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00941155
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.010419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.019139
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00977737
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0105053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0307588
[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.066222 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00941155
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0135492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0104947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0113234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0188972
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00852347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00743751
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0104517
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0171904
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00929926
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.024003
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.011673
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00936207
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0330676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.01106
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0146871
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.022132
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0173408
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.01039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0310924
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0136303
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00940474
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0134025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0136414
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0144356
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0126929
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0156107
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0101909
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0136832
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0310051
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0159599
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0102953
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00750293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0103263
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0292658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0199209
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0127534
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0216167
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0109192
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0128587
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.016225
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00965821
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0243429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0117639
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0179784
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0086813
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0333717
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00856889
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.0161021
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0107985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.013882
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0311176
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00895326
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.023936
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0307937
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0333781
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0131984
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0102898
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.024323
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00992408
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0105827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.013847
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00928118
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0137382
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0134665
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0200841
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0240693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0135462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0112181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.024115
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0180536
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0203093
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0334283
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0135945
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0130449
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0136107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0174805
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0109784
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0106327
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0310429
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0170645
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0104313
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00842987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.010603
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00881965
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0241813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00936296
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0316005
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0156359
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0138001
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0160325
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.03168
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0195431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0137276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0309382
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0239436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00805206
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0160432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136678
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0148281
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0160239
[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.310156 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.00743751
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4
[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.050557
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0385422
[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00533987 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0385422
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0125649
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0228935
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0102251
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0101524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0131516
[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0119168 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0101524
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0348939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.010038
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0345621
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0116241
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111691
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0190062
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0184432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00935496
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0215427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0108401
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0113952
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0112046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0187117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0187455
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0113436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00828774
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00857709
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0114578
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00934903
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0146783
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0114859
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0346379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0348608
[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0607919 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00828774
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0515886
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.047843
[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00543067 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.047843
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0170923
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0103796
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0222571
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0100408
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0170917
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0171419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00799213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0101075
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0102533
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0241912
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00791988
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0101779
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00792633
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0308548
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0109836
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0172571
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0102704
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00842187
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0107733
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0313775
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0317004
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0175493
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00787448
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.01025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0191058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00824013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.010436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0307462
[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0665299 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00787448
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0134054
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0104327
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0112789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0189387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00875788
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00738064
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00714893
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0172789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.024195
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0119676
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0331093
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0081613
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0151434
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0226745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0174613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0073869
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0316625
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0138035
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00920447
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0121242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0137195
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0146143
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.013097
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0161173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0136589
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.031103
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0103499
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0103056
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0201255
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0129058
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.022044
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00880955
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0123879
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0244587
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00920158
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0182422
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00873026
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0333739
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0084928
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00923157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0139671
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0315161
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0241021
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0311903
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0334773
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0116256
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0100546
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0243109
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00934311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0103192
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.013789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0090929
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0136431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0135031
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0202955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0241524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.013527
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.008336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0242171
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0134741
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0210913
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0334592
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0122491
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0126874
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.011886
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0175483
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0114667
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0109261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.031072
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0171307
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0104377
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00867173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00750625
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0101317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0242484
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00952777
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0317595
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0158376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0143102
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0319971
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0204662
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0138334
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0311602
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0241211
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00753185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136883
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0081894
[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.262765 seconds. Fastest Tactic: 0x322f337abc345152 Time: 0.00714893
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x322f337abc345152
[X] =============== Computing costs for /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,400:4,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00262508
[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00325858 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00262508
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])
[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00282938
[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00320085 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00282938
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0929307
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0703872
[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00548167 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0703872
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0181199
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0334859
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0141276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0127893
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.018797
[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0115954 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0127893
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0609315
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0147743
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0603716
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0174651
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111064
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0320921
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0315181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0133363
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0372231
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0103971
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0174528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0105247
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0318051
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0325081
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0174448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0116311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0113749
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0113959
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0137741
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0243787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0176814
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0605422
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0607467
[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0538683 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.0103971
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] *************** Autotuning format combination: Int8(25600,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0104757
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00941452
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0107447
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00971154
[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0113062 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00941452
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00696066
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00542658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0053396
[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00832089 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.0053396
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.011227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00401067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00676331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00458374
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0068972
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0109863
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00544964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00707711
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0109939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00403226
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00410732
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00452829
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00409366
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00523267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00737113
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00486987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00496612
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00387016
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00413419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.0041768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0108841
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0110665
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00675776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00681143
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00539905
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0052155
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00604743
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00444828
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00595924
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00582455
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00595518
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00668629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0111904
[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0961571 seconds. Fastest Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00387016
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65fbe45b4cb1d8a5
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(204800,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.106141
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0916453
[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00579891 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0916453
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18
[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0302889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0168907
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0405736
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0167157
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.030184
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0303932
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0123269
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0168309
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0169248
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0444027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0124437
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0167291
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0123599
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0571449
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0179251
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0304194
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0169248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0130302
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0174267
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0581298
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0590311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0310691
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0124658
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0168448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0341013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0128702
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0170459
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0570987
[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0739668 seconds. Fastest Tactic: 0x22ebff09f6ab32eb Time: 0.0123269
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x22ebff09f6ab32eb
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0233685
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0170363
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0188557
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0336949
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0134852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0109708
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0108511
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.030401
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0160076
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0438267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.019621
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0158686
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0610951
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.011073
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0254484
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0394809
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0305057
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0104863
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0575644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0237447
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0142711
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.020906
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0236459
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0245935
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0217613
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0277153
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0166644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0234908
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0574453
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0291156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0167557
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00985255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0169344
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0556569
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0357408
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0215707
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0386347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0136538
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0205515
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0293893
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.0160899
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0441053
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0144542
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0322104
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0134886
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0616942
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0135855
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.029424
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0152121
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0238629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0576213
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0113621
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0439293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0573067
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0614969
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0189772
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0166608
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0441467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.014356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0169003
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.023685
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0143653
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0236139
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.023461
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0360299
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0439293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0234112
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0113056
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0438547
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0235122
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0361568
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0616391
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0207718
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0210887
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.019213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0306764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0177572
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0170789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0572676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0302267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0169131
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0135735
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0103551
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0133385
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.04402
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0143213
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0587111
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0270736
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0237369
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0291324
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0587573
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0348309
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.023771
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0575929
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0441173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.011413
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0292764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0235662
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0121497
[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0292276
[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.317132 seconds. Fastest Tactic: 0x1d53511430a5d47e Time: 0.00985255
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1d53511430a5d47e
[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0936107
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0710848
[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00539778 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0710848
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0191058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0360661
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0150252
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0135659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0197898
[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0115804 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0135659
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0613458
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0151753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0609173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0176982
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0112071
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0320776
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.031679
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0136469
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0372551
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0103926
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0175585
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0107568
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0319409
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0318788
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.017742
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0114468
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0116447
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0114987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0138436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0244983
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0179284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0608107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0611556
[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0547091 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.0103926
[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb
[X] =============== Computing costs for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146221
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0119539
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0150854
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126392
[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0105129 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0119539
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00647836
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00468607
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00445255
[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00837165 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00445255
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0138735
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00372172
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00836747
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00453348
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00855768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0134711
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00571842
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00863125
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0131598
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00431754
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00438164
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00453665
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0037476
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.0057795
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00901304
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00543828
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00552936
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00383497
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00408637
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00399175
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0134067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0133444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00823155
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00821099
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00574048
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00389557
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00387519
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0039616
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00466193
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00669802
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00594676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00807492
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.013414
[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0906548 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00372172
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_107.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]}
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_107.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023])
[V] Compiler backend is used during engine build.
[X]  (foreignNode) Set user's cuda kernel library
[X] Subgraph compilation completed in 2.735 seconds.
[X] Tactic: 0x0000000000000000 Time: 0.0474453
[X] {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_107.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023]) profiling completed in 2.75881 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0474453
[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000
[X] =============== Computing costs for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00850667
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.0083752
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00897656
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00859815
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00890077
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0153071 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.0083752
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.0085176
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00832624
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00898246
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00858557
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00899565
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0157037 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00832624
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(204800,1:16,2560,32) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00596248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00784595
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00557102
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00665684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00598476
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00679061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00668651
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00671488
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00742661
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00584644
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00677568
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00577655
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00682949
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00651631
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00612946
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00792657
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00579163
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00558293
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00559396
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0076526
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00662881
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00657276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00578409
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00602153
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00668331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00667563
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00590934
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00663279
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0777088 seconds. Fastest Tactic: 0x381eb065135d177f Time: 0.00557102
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x381eb065135d177f
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00560409
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00598705
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00791219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00558489
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00638873
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00663613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.0060061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00678336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00674347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.0066816
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00742424
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00586536
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00678357
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00580101
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00577674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00688979
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00649456
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00603333
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00551205
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0077956
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00580212
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00559876
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00547165
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00755461
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00580488
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.0064359
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00645251
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00658991
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.00573451
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.0065159
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00672
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00737554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00574319
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.00562631
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00604743
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00648616
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00665684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00670869
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00588501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00664973
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00770012
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.00646482
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.118715 seconds. Fastest Tactic: 0x712e1cc2c7813ee9 Time: 0.00547165
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x712e1cc2c7813ee9
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00529744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00627062
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00615467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00572294
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00654243
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.0062159
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00668138
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00515873
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.0050326
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00641149
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0073382
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00686999
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00672512
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00542108
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00620859
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00628227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00556711
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00535026
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00687347
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00551677
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00617343
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00559236
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00585507
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00679488
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00562382
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00529778
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.0050111
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00730272
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.006464
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00560462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00572366
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00545394
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00701244
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00561973
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00556836
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00627022
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00545634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00595638
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00568515
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00679424
[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00533299
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00563218
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00655728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00529338
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00576681
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00581462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00617561
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00732661
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00622045
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00616301
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00587528
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00609649
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00514314
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00725125
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00564356
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0057611
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00506117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00610366
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00643836
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00611724
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00629233
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00569889
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00543536
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00681796
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00609978
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00582419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00579789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00671403
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00613334
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00591046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00667285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00611627
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00607147
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0063523
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00511838
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00567575
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00779957
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00566671
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00558809
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00567268
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00757973
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00559538
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00538219
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00575283
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00543191
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00691744
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00610754
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00579237
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00548984
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00592075
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00627575
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00540095
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.0052845
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00703822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0061632
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00584717
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00629215
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00604857
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00634526
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00734237
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00664136
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00790053
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00548617
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.0071419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00703355
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00561618
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00753209
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00635834
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00538925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00555929
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.0053098
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00692288
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00611064
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00540645
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.0053208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00625442
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00595331
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00564409
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00548686
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00605467
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00523033
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00585095
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00635351
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00684647
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.0052927
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00821672
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.361464 seconds. Fastest Tactic: 0x5e4918ccf433630e Time: 0.0050111
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4918ccf433630e
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00625008
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00611452
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.0057554
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00656774
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00626133
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00665914
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00517104
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00502288
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.006424
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00739896
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00684756
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00672
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00542417
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00607011
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00610948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00555489
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00534485
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00687303
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00551956
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00617387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00559769
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00599543
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00684082
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00565478
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.005312
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00495608
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00728487
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00648349
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00560231
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00571499
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00543363
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00698067
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00563218
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00569745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.0062242
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00558514
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.0059389
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00566039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00680707
[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.0053994
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00567087
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00660915
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00534451
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00579807
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00583375
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00611956
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00727745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00614129
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00611394
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00592955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00602191
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00510901
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00727559
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00567015
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00579292
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00509915
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0060802
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00648739
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00613004
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00634989
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00569275
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00549036
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00678229
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00607321
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00581205
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00578336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00674325
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00615855
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00587939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00672021
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00610793
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00604153
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00633238
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00510853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00569636
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00780304
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00559911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00555858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00568714
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00755958
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00559787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00542314
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00576773
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00547218
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00691526
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00610793
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00578759
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00548931
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0059447
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00626489
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00537261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00536229
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00707222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00618943
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00582161
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00627398
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00603067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00637726
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00739826
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00664241
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00787299
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00537431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00718683
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.006976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0057554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00750744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00632091
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00535399
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00560729
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.0053015
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00688827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00609862
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00542056
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00537807
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00626647
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00591925
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00562702
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00545549
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00599695
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00524333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00583467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00636076
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00689219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.0053291
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00818992
[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.366522 seconds. Fastest Tactic: 0x5e4918ccf433630e Time: 0.00495608
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4918ccf433630e
[X] =============== Computing costs for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00736278
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.0071857
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00916411
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00851627
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00878793
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146578 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.0071857
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00729948
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00716573
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00914566
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00850587
[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00881235
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146909 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00716573
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00584019
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00502209
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00527183
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00442863
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00570486
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00408806
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00428198
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00744083
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.0044014
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00427761
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00408442
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00538787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00438261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00727397
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.0061634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00490761
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00429634
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00531183
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00431289
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00427126
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00738852
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.0073273
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00541488
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00435491
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.0041025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00410133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00569998
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00431877
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0802382 seconds. Fastest Tactic: 0x08af511817b7463e Time: 0.00408442
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x08af511817b7463e
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00533113
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00581959
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00497942
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0052565
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00723404
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00443775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00567575
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00409769
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00428636
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00743443
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00443088
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00424722
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00412023
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00542211
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00426856
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.004424
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00723903
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00613741
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00419506
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00495984
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00437305
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00524233
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0043215
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00432054
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00597372
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.00725055
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00395507
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00746098
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.00546623
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00731803
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00430318
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00442681
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00549159
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.00530709
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00432971
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00396461
[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.0041388
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00411239
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00568208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00430113
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00491451
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.00428226
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.133635 seconds. Fastest Tactic: 0x788dd0382d5ebd44 Time: 0.00395507
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x788dd0382d5ebd44
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.005696
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00449863
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00441754
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00469694
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00798959
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00418147
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00817691
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00547567
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00545411
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00798121
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00436128
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00455596
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00807695
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00466074
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00720113
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00451791
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00640423
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00429128
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00463096
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00510335
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00681644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00463511
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00484406
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.0043369
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00474306
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00567792
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00536754
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00557085
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00773745
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00467659
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00484004
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00621571
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.0052725
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00451431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00396436
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00416606
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00452901
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.0072851
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00434188
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00486895
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00457366
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00475222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00598819
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00480777
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00649046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00745102
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00465704
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00762521
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00435006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00667029
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00437347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00549596
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00440772
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00446464
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00413168
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00485441
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00399873
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00444587
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00506941
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00794895
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00635572
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00510675
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00833627
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00459646
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00460595
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00647795
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00458637
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00434687
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00410927
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00444743
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.0041384
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00501126
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00485503
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00385067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00481966
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00583448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00632594
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00623684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00638128
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.004632
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00645621
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00625284
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00632413
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00596533
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00832026
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00459295
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00451762
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00442414
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00469664
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00487266
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00583651
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00565406
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00484869
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00745932
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00430072
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00441221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.0043055
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00430988
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00455942
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00446208
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00474486
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00499407
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00483494
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00489244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00648759
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00438247
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00772073
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00509155
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00471707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00456562
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00503944
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00750412
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00466104
[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00676267
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00582952
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00740764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00496392
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00612248
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.0043993
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00740939
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00482042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00406803
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00475688
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00463985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00393173
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0054326
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.365476 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00385067
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00566852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00453939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00443846
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00468667
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.0079774
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00422359
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.0081613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00543088
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00540834
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00795708
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00436821
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.0045391
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00805841
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00465304
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00721793
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00454083
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00637062
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00430523
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00462071
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00507669
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00670421
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.0046517
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00484375
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00439888
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00472698
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00569907
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00536838
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00556036
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00771685
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00469799
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00483352
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00629635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00530658
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00453175
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00392935
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00420253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00453175
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00727374
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00433427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00486616
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00457775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00474216
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00598895
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00479482
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00651446
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00747875
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00466148
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00759345
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.004392
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0066606
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00437596
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00548529
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00438441
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00444899
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00412892
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00486168
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00400787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00446563
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00506537
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00800508
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00636599
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00504931
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00832755
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00458258
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00460946
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00647857
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00459251
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00431603
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00406881
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00446279
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00413537
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00498754
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00485735
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00385594
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00484267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00583117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00631084
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00621373
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00639557
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00463152
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00643241
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00624395
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00630722
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00599181
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00830049
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00459178
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00452526
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00444112
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00471917
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00486987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0058389
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00564409
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00486817
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00747757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.0043477
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00442288
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00429798
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00428485
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00456942
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00446777
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00472774
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00495969
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00481966
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00467096
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00650503
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0043671
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0076897
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00506634
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00472924
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00456782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00504899
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00744889
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.004648
[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.006688
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00583798
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00747852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00492063
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.0060928
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.0043599
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00739014
[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00480929
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00405218
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00472939
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00465511
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00392211
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00541093
[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.36253 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00385594
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27
[X] =============== Computing costs for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv
[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00977189
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00869552
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0100571
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00902775
[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0114014 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00869552
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00599448
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00458462
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00438095
[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00872816 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00438095
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0109206
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00334027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00692658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0038983
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00710127
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0105847
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00495325
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00720885
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0102503
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00396085
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00596057
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0040275
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00383914
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00493114
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00732475
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00473525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00478415
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00347533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00369782
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00376306
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0102697
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.01039
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00674218
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00679211
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00483803
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00393475
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00357674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00356743
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00411996
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00567196
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00501731
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.0066309
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0104457
[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0989233 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00334027
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul)
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.0024194
[X] Tactic: 0x0000000000000001 Time: 0.0024168
[X] Tactic: 0x0000000000000002 Time: 0.00244609
[X] Tactic: 0x0000000000000003 Time: 0.00253988
[X] Tactic: 0x0000000000000004 Time: 0.00244383
[X] Tactic: 0x0000000000000005 Time: 0.00228924
[X] Tactic: 0x0000000000000006 Time: 0.00307591
[X] Tactic: 0x0000000000000007 Time: 0.00267238
[X] Tactic: 0x0000000000000008 Time: 0.00240199
[X] Tactic: 0x0000000000000009 Time: 0.00240031
[X] Tactic: 0x000000000000001c Time: 0.00238476
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0347366 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00228924
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00241673
[X] Tactic: 0x0000000000000001 Time: 0.00242752
[X] Tactic: 0x0000000000000002 Time: 0.00241801
[X] Tactic: 0x0000000000000003 Time: 0.0025195
[X] Tactic: 0x0000000000000004 Time: 0.00241832
[X] Tactic: 0x0000000000000005 Time: 0.00230209
[X] Tactic: 0x0000000000000006 Time: 0.003112
[X] Tactic: 0x0000000000000007 Time: 0.00270701
[X] Tactic: 0x0000000000000008 Time: 0.00241809
[X] Tactic: 0x0000000000000009 Time: 0.00241512
[X] Tactic: 0x000000000000001c Time: 0.00241266
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0303549 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00230209
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00242334
[X] Tactic: 0x0000000000000001 Time: 0.00243745
[X] Tactic: 0x0000000000000002 Time: 0.0023872
[X] Tactic: 0x0000000000000003 Time: 0.0025552
[X] Tactic: 0x0000000000000004 Time: 0.00245749
[X] Tactic: 0x0000000000000005 Time: 0.00229392
[X] Tactic: 0x0000000000000006 Time: 0.00313042
[X] Tactic: 0x0000000000000007 Time: 0.00269557
[X] Tactic: 0x0000000000000008 Time: 0.00240967
[X] Tactic: 0x0000000000000009 Time: 0.00242373
[X] Tactic: 0x000000000000001c Time: 0.0024066
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.030086 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00229392
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00232207
[X] Tactic: 0x0000000000000001 Time: 0.00247767
[X] Tactic: 0x0000000000000002 Time: 0.0024408
[X] Tactic: 0x0000000000000003 Time: 0.00282434
[X] Tactic: 0x0000000000000004 Time: 0.00267537
[X] Tactic: 0x0000000000000005 Time: 0.00255251
[X] Tactic: 0x0000000000000006 Time: 0.00353707
[X] Tactic: 0x0000000000000007 Time: 0.0033823
[X] Tactic: 0x0000000000000008 Time: 0.00339954
[X] Tactic: 0x0000000000000009 Time: 0.00310637
[X] Tactic: 0x000000000000000a Time: 0.00256204
[X] Tactic: 0x000000000000000b Time: 0.00240399
[X] Tactic: 0x000000000000000c Time: 0.00228749
[X] Tactic: 0x000000000000000d Time: 0.00269187
[X] Tactic: 0x000000000000000e Time: 0.00254795
[X] Tactic: 0x000000000000000f Time: 0.00243955
[X] Tactic: 0x0000000000000010 Time: 0.00339954
[X] Tactic: 0x0000000000000011 Time: 0.00298505
[X] Tactic: 0x0000000000000012 Time: 0.00270331
[X] Tactic: 0x0000000000000013 Time: 0.00279378
[X] Tactic: 0x0000000000000014 Time: 0.00243038
[X] Tactic: 0x0000000000000015 Time: 0.00240138
[X] Tactic: 0x0000000000000016 Time: 0.00261858
[X] Tactic: 0x0000000000000017 Time: 0.00308012
[X] Tactic: 0x000000000000001c Time: 0.00230429
[X] Tactic: 0x000000000000001d Time: 0.00256574
[X] Tactic: 0x000000000000001e Time: 0.00241289
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0808483 seconds. Fastest Tactic: 0x000000000000000c Time: 0.00228749
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000000c
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000018 Time: 0.00228931
[X] Tactic: 0x0000000000000019 Time: 0.00244344
[X] Tactic: 0x000000000000001a Time: 0.00284755
[X] Tactic: 0x000000000000001b Time: 0.00369829
[X] Tactic: 0x000000000000001f Time: 0.00228836
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.015144 seconds. Fastest Tactic: 0x000000000000001f Time: 0.00228836
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001f
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000001c Time: 0.00357447
[X] Tactic: 0x000000000000001d Time: 0.00354656
[X] Tactic: 0x000000000000001e Time: 0.00386685
[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.00868407 seconds. Fastest Tactic: 0x000000000000001d Time: 0.00354656
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001d
[X] =============== Computing costs for /model/encoder/Resize
[X] *************** Autotuning format combination: Int8(102400,400,20,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])
[X] Tactic: 0x0000000000000000 Time: 0.00933185
[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.00234274 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00933185
[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])
[X] Tactic: 0x0000000000000003 Time: 0.00258486
[X] Tactic: 0x0000000000000005 Time: 0.00243154
[X] Tactic: 0x0000000000000006 Time: 0.00286067
[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.00864464 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00243154
[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005
[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146393
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0119928
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0150242
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0125748
[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107811 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0119928
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00676821
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00509495
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00508784
[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00882067 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00508784
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0138718
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00478156
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00833893
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0046643
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0085392
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0135629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00584589
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00867063
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0112565
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00457775
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0046517
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.004684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00492016
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00581628
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00914047
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00566509
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00571083
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00445611
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00471181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00437915
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0133157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0114635
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00828748
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00828982
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00565171
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00449379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.0042988
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00500267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0049877
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00674154
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00606158
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00816911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0115413
[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0903671 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.0042988
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e
[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0104317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00460478
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0102768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00743893
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00446706
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00529236
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00718116
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00485936
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00433205
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0104357
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00441038
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00602629
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00434119
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00522
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00539045
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00477074
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0052655
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00447644
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00420866
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0104042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0105827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00695133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.0055728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00724834
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00701822
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00699022
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00532859
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00451676
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00433995
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00516103
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.011142
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00431467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00552726
[X] model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0938678 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00420866
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3
[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0110221
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00568787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00698511
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0056144
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00526583
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.01787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0077827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00710672
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.010717
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00548686
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.010555
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0178139
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00704378
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0108284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0069096
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00596933
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00540525
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00533469
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0181749
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00528767
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0177819
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00720045
[X] model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.057585 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.00526583
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84
[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0xc722efd60bc6ea84, 0.00526583 ms
[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0274535
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0215873
[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00497364 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0215873
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00818055
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0135765
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00660789
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0063835
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00850267
[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0128694 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0063835
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.02097
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00637967
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0188178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00754133
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00569239
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0122476
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0124298
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00622104
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0134647
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00545669
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00743443
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00559111
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0120472
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0121166
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00749985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00570938
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0053889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00580616
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00631427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00947585
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00779733
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.020758
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0190963
[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0662968 seconds. Fastest Tactic: 0xb936321f82fd390c Time: 0.0053889
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb936321f82fd390c
[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))
[X] *************** Autotuning format combination: Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00304272
[X] Tactic: 0x0000000000000001 Time: 0.00284212
[X] Tactic: 0x0000000000000002 Time: 0.00284991
[X] Tactic: 0x0000000000000003 Time: 0.00289929
[X] Tactic: 0x0000000000000004 Time: 0.0028655
[X] Tactic: 0x0000000000000005 Time: 0.00281466
[X] Tactic: 0x0000000000000006 Time: 0.00340876
[X] Tactic: 0x0000000000000007 Time: 0.0031376
[X] Tactic: 0x0000000000000008 Time: 0.00300742
[X] Tactic: 0x0000000000000009 Time: 0.00283887
[X] Tactic: 0x000000000000001c Time: 0.00298238
[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0361376 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00281466
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005
[X] *************** Autotuning format combination: Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000000a Time: 0.00376138
[X] Tactic: 0x000000000000000b Time: 0.00410992
[X] Tactic: 0x000000000000000c Time: 0.0055061
[X] Tactic: 0x000000000000000d Time: 0.0055061
[X] Tactic: 0x000000000000000e Time: 0.00664805
[X] Tactic: 0x000000000000000f Time: 0.00963657
[X] Tactic: 0x0000000000000010 Time: 0.00868951
[X] Tactic: 0x0000000000000011 Time: 0.0106173
[X] Tactic: 0x0000000000000012 Time: 0.0160823
[X] Tactic: 0x0000000000000013 Time: 0.0195147
[X] Tactic: 0x0000000000000014 Time: 0.00346469
[X] Tactic: 0x0000000000000015 Time: 0.0033906
[X] Tactic: 0x0000000000000016 Time: 0.00425856
[X] Tactic: 0x0000000000000017 Time: 0.00594601
[X] Tactic: 0x0000000000000018 Time: 0.0030001
[X] Tactic: 0x0000000000000019 Time: 0.00324256
[X] Tactic: 0x000000000000001a Time: 0.00339168
[X] Tactic: 0x000000000000001b Time: 0.0041654
[X] Tactic: 0x000000000000001d Time: 0.00558773
[X] Tactic: 0x000000000000001e Time: 0.00349967
[X] Tactic: 0x000000000000001f Time: 0.00304698
[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0578638 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.0030001
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018
[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00603791
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00386256
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00606449
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00512968
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00401851
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00395658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00497349
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00407597
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00401876
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00622677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00397867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00484468
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00359903
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00385937
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00411983
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00401054
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00402377
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00401524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00342063
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00624435
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00620168
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00474471
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00430796
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00486663
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.0047744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00463674
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00407336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00384098
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00388056
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00387151
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00650728
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00382752
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00421307
[X] model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.100983 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00342063
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3
[X] =============== Computing costs for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00981364
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00868841
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0101553
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00906205
[X] model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0111573 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00868841
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00595257
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00520599
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.005266
[X] model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00879667 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00520599
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0109588
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00493867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00697089
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00534705
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00711376
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0107751
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00560107
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00724085
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0105813
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.0050964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00520833
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0055856
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00505455
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00550715
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00746169
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00541746
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00541471
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00505955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00536009
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00531183
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0104393
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0106413
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00702222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00703844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00539148
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00506958
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00519418
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00500346
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00536178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00625817
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00566219
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00686955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.010622
[X] model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0999015 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00493867
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00354324
[X] Tactic: 0x0000000000000001 Time: 0.00297819
[X] Tactic: 0x0000000000000002 Time: 0.00287991
[X] Tactic: 0x0000000000000003 Time: 0.0031511
[X] Tactic: 0x0000000000000004 Time: 0.002992
[X] Tactic: 0x0000000000000005 Time: 0.00298276
[X] Tactic: 0x0000000000000006 Time: 0.00329956
[X] Tactic: 0x0000000000000007 Time: 0.00297487
[X] Tactic: 0x0000000000000008 Time: 0.00297857
[X] Tactic: 0x0000000000000009 Time: 0.00270684
[X] Tactic: 0x000000000000001c Time: 0.00354644
[X] PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0320455 seconds. Fastest Tactic: 0x0000000000000009 Time: 0.00270684
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000009
[X] *************** Autotuning format combination: Float(409600,1,10240,256) -> Float(409600,1,10240,256) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00352831
[X] Tactic: 0x0000000000000001 Time: 0.00296769
[X] Tactic: 0x0000000000000002 Time: 0.00286541
[X] Tactic: 0x0000000000000003 Time: 0.00313441
[X] Tactic: 0x0000000000000004 Time: 0.00297024
[X] Tactic: 0x0000000000000005 Time: 0.00301694
[X] Tactic: 0x0000000000000006 Time: 0.00326255
[X] Tactic: 0x0000000000000007 Time: 0.00295316
[X] Tactic: 0x0000000000000008 Time: 0.00299352
[X] Tactic: 0x0000000000000009 Time: 0.00276037
[X] Tactic: 0x000000000000001c Time: 0.0035447
[X] PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0338496 seconds. Fastest Tactic: 0x0000000000000009 Time: 0.00276037
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000009
[X] *************** Autotuning format combination: Float(1,1600,40,1) -> Float(1,1600,40,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00355631
[X] Tactic: 0x0000000000000001 Time: 0.00296637
[X] Tactic: 0x0000000000000002 Time: 0.00287006
[X] Tactic: 0x0000000000000003 Time: 0.00310607
[X] Tactic: 0x0000000000000004 Time: 0.00298495
[X] Tactic: 0x0000000000000005 Time: 0.00296628
[X] Tactic: 0x0000000000000006 Time: 0.00322564
[X] Tactic: 0x0000000000000007 Time: 0.00306202
[X] Tactic: 0x0000000000000008 Time: 0.00299219
[X] Tactic: 0x0000000000000009 Time: 0.0028169
[X] Tactic: 0x000000000000001c Time: 0.00356925
[X] PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0340793 seconds. Fastest Tactic: 0x0000000000000009 Time: 0.0028169
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000009
[X] *************** Autotuning format combination: Float(102400,1:4,2560,64) -> Float(102400,1:4,2560,64) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00355189
[X] Tactic: 0x0000000000000001 Time: 0.00353684
[X] Tactic: 0x0000000000000002 Time: 0.00341802
[X] Tactic: 0x0000000000000003 Time: 0.0037395
[X] Tactic: 0x0000000000000004 Time: 0.00365276
[X] Tactic: 0x0000000000000005 Time: 0.00350167
[X] Tactic: 0x0000000000000006 Time: 0.00397841
[X] Tactic: 0x0000000000000007 Time: 0.00383497
[X] Tactic: 0x0000000000000008 Time: 0.00383461
[X] Tactic: 0x0000000000000009 Time: 0.00365658
[X] Tactic: 0x000000000000000a Time: 0.00351686
[X] Tactic: 0x000000000000000b Time: 0.00352202
[X] Tactic: 0x000000000000000c Time: 0.0035646
[X] Tactic: 0x000000000000000d Time: 0.00379043
[X] Tactic: 0x000000000000000e Time: 0.00350945
[X] Tactic: 0x000000000000000f Time: 0.00341094
[X] Tactic: 0x0000000000000010 Time: 0.00397994
[X] Tactic: 0x0000000000000011 Time: 0.00384564
[X] Tactic: 0x0000000000000012 Time: 0.00368
[X] Tactic: 0x0000000000000013 Time: 0.00351854
[X] Tactic: 0x0000000000000014 Time: 0.00351495
[X] Tactic: 0x0000000000000015 Time: 0.00301492
[X] Tactic: 0x0000000000000016 Time: 0.00311131
[X] Tactic: 0x0000000000000017 Time: 0.00341083
[X] Tactic: 0x000000000000001c Time: 0.0035364
[X] Tactic: 0x000000000000001d Time: 0.00353965
[X] Tactic: 0x000000000000001e Time: 0.00351529
[X] PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0766883 seconds. Fastest Tactic: 0x0000000000000015 Time: 0.00301492
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000015
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000018 Time: 0.00353954
[X] Tactic: 0x0000000000000019 Time: 0.00356085
[X] Tactic: 0x000000000000001a Time: 0.00385214
[X] Tactic: 0x000000000000001b Time: 0.00399759
[X] Tactic: 0x000000000000001f Time: 0.00354769
[X] PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0136962 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00353954
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018
[X] *************** Autotuning format combination: Float(1:4,1600,40,1) -> Float(1:4,1600,40,1) ***************
[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000001c Time: 0.00774279
[X] Tactic: 0x000000000000001d Time: 0.00792062
[X] Tactic: 0x000000000000001e Time: 0.00822634
[X] PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.00641527 seconds. Fastest Tactic: 0x000000000000001c Time: 0.00774279
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c
[X] =============== Computing costs for /model/encoder/Resize_1
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])
[X] Tactic: 0x0000000000000000 Time: 0.0293333
[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00184841 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0293333
[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])
[X] Tactic: 0x0000000000000003 Time: 0.00373251
[X] Tactic: 0x0000000000000005 Time: 0.00325468
[X] Tactic: 0x0000000000000006 Time: 0.00414815
[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00819155 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00325468
[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005
[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0151272
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0122038
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0159599
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0127818
[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.01061 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0122038
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00852533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00813105
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0104787
[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00780297 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00813105
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0140253
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0101647
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00980419
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00933748
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0100693
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0136964
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00861101
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0102054
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0128402
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00824195
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0083592
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00924339
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0103515
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00820449
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0106787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00823259
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00822764
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.0094874
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00980863
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00893951
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0134908
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0130396
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00997396
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0098858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00810971
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00948741
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00896898
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0102494
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0084928
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00973379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00835093
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00978743
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0132332
[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0789482 seconds. Fastest Tactic: 0x7720f198395e7d3d Time: 0.00810971
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7720f198395e7d3d
[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(3276800,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1:16,2560,32) -> Int8(51200,1:16,640,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0106387
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.0103046
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0104843
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00754489
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0084464
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00585862
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00731455
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0101857
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00918544
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0106867
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00921513
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00749464
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.0065554
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00609881
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00661501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.0105473
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00641969
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00948711
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00640181
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0106627
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.010772
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00705666
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00681077
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00745695
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00717458
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00716278
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00596838
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00684473
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00901852
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00602343
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0111556
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00893446
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0061924
[X] model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0836735 seconds. Fastest Tactic: 0x458f02d2b10db57c Time: 0.00585862
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x458f02d2b10db57c
[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0110428
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0104097
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00737252
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0133243
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00817561
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0180295
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00809245
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00805511
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0106947
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0129157
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0107337
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0179868
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00732313
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0110744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00784248
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.0085336
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0131737
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0126416
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0182692
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0101052
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0179952
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00756219
[X] model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0543205 seconds. Fastest Tactic: 0xbd976ef514eaa406 Time: 0.00732313
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbd976ef514eaa406
[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0xbd976ef514eaa406, 0.00732313 ms
[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0219867 ms
[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0xa8b56a226b057463, 0.00906523 ms
[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0xad886d4d69834922, 0.0097347 ms
[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))
[X] *************** Autotuning format combination: Float(819200,6400,80,1), Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.00533384
[X] Tactic: 0x0000000000000001 Time: 0.00414933
[X] Tactic: 0x0000000000000002 Time: 0.00413787
[X] Tactic: 0x0000000000000003 Time: 0.00400152
[X] Tactic: 0x0000000000000004 Time: 0.00380242
[X] Tactic: 0x0000000000000005 Time: 0.00378895
[X] Tactic: 0x0000000000000006 Time: 0.003969
[X] Tactic: 0x0000000000000007 Time: 0.00378211
[X] Tactic: 0x0000000000000008 Time: 0.00666102
[X] Tactic: 0x0000000000000009 Time: 0.00388663
[X] Tactic: 0x000000000000001c Time: 0.0052235
[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0339643 seconds. Fastest Tactic: 0x0000000000000007 Time: 0.00378211
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000007
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000000a Time: 0.00811962
[X] Tactic: 0x000000000000000b Time: 0.00679787
[X] Tactic: 0x000000000000000c Time: 0.00807543
[X] Tactic: 0x000000000000000d Time: 0.00707844
[X] Tactic: 0x000000000000000e Time: 0.0080414
[X] Tactic: 0x000000000000000f Time: 0.0111264
[X] Tactic: 0x0000000000000010 Time: 0.00919236
[X] Tactic: 0x0000000000000011 Time: 0.0111129
[X] Tactic: 0x0000000000000012 Time: 0.0167019
[X] Tactic: 0x0000000000000013 Time: 0.0202196
[X] Tactic: 0x0000000000000014 Time: 0.00702844
[X] Tactic: 0x0000000000000015 Time: 0.00655582
[X] Tactic: 0x0000000000000016 Time: 0.00663634
[X] Tactic: 0x0000000000000017 Time: 0.00700511
[X] Tactic: 0x0000000000000018 Time: 0.00603124
[X] Tactic: 0x0000000000000019 Time: 0.00594433
[X] Tactic: 0x000000000000001a Time: 0.00602305
[X] Tactic: 0x000000000000001b Time: 0.00658719
[X] Tactic: 0x000000000000001d Time: 0.00878344
[X] Tactic: 0x000000000000001e Time: 0.00689589
[X] Tactic: 0x000000000000001f Time: 0.00604553
[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0501106 seconds. Fastest Tactic: 0x0000000000000019 Time: 0.00594433
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000019
[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00615622
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00707244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00625936
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00570396
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00781371
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00599638
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00552848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00773236
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00698844
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00641006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.0074925
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00786654
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00600534
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00622914
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.006624
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00723586
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00641231
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00760727
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00583117
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00647672
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00639417
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00637726
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00731061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00539561
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00532368
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00513936
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00625245
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00653415
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00706222
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00612034
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00666144
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00676395
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00643528
[X] model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0995185 seconds. Fastest Tactic: 0x65a38dbc9e991257 Time: 0.00513936
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65a38dbc9e991257
[X] =============== Computing costs for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv
[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Float(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0123387
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0120571
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0129813
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0127087
[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0116033 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0120571
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.0113522
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0100201
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0115402
[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00706773 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.0100201
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.014112
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0101379
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0123013
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0102804
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0124602
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0138911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0116289
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0126914
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0136725
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.0103425
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0104071
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0102261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0102956
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.0117536
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.012719
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0112597
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.011434
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.0102878
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0109505
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00976183
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0134575
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0138586
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.011886
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0120381
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0113884
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.010284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00962042
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.010726
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0110255
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0139422
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0121669
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.0119185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0140196
[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0906873 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00962042
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e
[X] =============== Computing costs for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(25600,1:16,640,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0104032
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.010524
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0130031
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00838827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0314793
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0119276
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0104407
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175243
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0115903
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172389
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0315423
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0113028
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0178307
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.010431
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00854892
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0129957
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0114076
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0319544
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0102061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0315898
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0107537
[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0580196 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.00838827
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.010444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104847
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0129834
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.008368
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0315268
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0119227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0104753
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175944
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0115428
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172592
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0314812
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0112786
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176247
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0104317
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00852613
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0129653
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0113817
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0319341
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0102261
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0315055
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.01075
[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0579019 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.008368
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84
[X] =============== Computing costs for model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv
[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0xff6944b17d5b2e32, 0.0119928 ms
[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x5e4f6d7c83746fd6, 0.00508784 ms
[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x9ec201b34455146e, 0.0042988 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)
[X] model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) [Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x2eba0b6a8ec55fa3, 0.00420866 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)
[X] model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0xc722efd60bc6ea84, 0.00526583 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)
[X] model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0xc722efd60bc6ea84, 0.00526583 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv
[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0215873 ms
[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x85c1a5f7f239cf84, 0.0063835 ms
[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0xb936321f82fd390c, 0.0053889 ms
[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))
[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000005, 0.00281466 ms
[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000018, 0.0030001 ms
[X] =============== Computing costs for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)
[X] model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x2eba0b6a8ec55fa3, 0.00342063 ms
[X] =============== Computing costs for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00983937
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00872451
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0101663
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00903438
[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0108569 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00872451
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.005632
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00493474
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00497239
[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00866111 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00493474
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0109227
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00489986
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00695466
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00536821
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00713214
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.010755
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00546099
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00724267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.010446
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00504645
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00517432
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00539114
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00500426
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00549351
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00741594
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00533587
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00534976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00508784
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00536161
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00526467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0104787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.010505
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00697911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00692027
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0053396
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00508638
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00513986
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00488533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00534891
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0062564
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00566147
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00680707
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.010605
[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0876148 seconds. Fastest Tactic: 0x445983715412fbda Time: 0.00488533
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x445983715412fbda
[X] =============== Computing costs for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(102400,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(3200,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(6400,1:16,320,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178279
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0076063
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104058
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00625817
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00740359
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0312291
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.011662
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0102746
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175641
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00684103
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172608
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0313348
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0110269
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0175333
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0102381
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00861101
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00576865
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0066976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0318865
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00711489
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0313028
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0105943
[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0534032 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00576865
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178206
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00763539
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104343
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0062803
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00738458
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0312485
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0117311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0103215
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175067
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00682427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0312533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0110324
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0175601
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0102562
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00863234
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00581757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0067232
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0319185
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00714848
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0312921
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0105807
[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0536322 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00581757
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4
[X] =============== Computing costs for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146606
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0119223
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0150929
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126084
[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0106564 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0119223
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00678293
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0049885
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00472218
[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00878907 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00472218
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0138624
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00349178
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00828982
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0045182
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00846427
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.013478
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0058034
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00862386
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0111225
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00432082
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00442681
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00455495
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00365797
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00575136
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00898779
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00554824
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00565858
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00530861
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00594526
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00486647
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0132025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0112946
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00817847
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00824169
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00566563
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00381164
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00385692
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00385778
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00468548
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00673024
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00602952
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00817223
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0114439
[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0992693 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00349178
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(51200,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(1600,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(3200,1:16,160,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0104433
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00354347
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0101831
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00749558
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00432014
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00529101
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00722519
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00388428
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00369653
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0104507
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00405616
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00595331
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.0041692
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00505826
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.005239
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.0035798
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0052175
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00400787
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00413985
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0102878
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0105997
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00701444
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00535467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.007076
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00697466
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00680533
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00537007
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00452094
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00399911
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00494886
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0111712
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00466148
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00555893
[X] model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0974527 seconds. Fastest Tactic: 0xc6cdb1e47323bb01 Time: 0.00354347
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc6cdb1e47323bb01
[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0109987
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00556658
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00701422
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00470235
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00536754
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0178751
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00780825
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00709244
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0107327
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00502066
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0105483
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0177948
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00712011
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0108404
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00695311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00598191
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00444744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0048589
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0181389
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0052285
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0178655
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00721384
[X] model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0606867 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00444744
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4
[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)
[X] model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1)] got cached result: CaskConvolution, tactic 0xc985777c89c6b3a4, 0.00444744 ms
[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.027086
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.021588
[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00512422 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.021588
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2
[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00820267
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0135889
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00662149
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00619971
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00848027
[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0184171 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00619971
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84
[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0210353
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00622677
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0187959
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00751312
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00438442
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0121531
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0116142
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00613915
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0134955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0048192
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0074368
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00500744
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.011976
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0120442
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00750341
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00552761
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00531437
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00467496
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00621037
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00943467
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00776757
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.020714
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0190661
[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0593401 seconds. Fastest Tactic: 0xd14bd6d95fefd45e Time: 0.00438442
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd14bd6d95fefd45e
[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))
[X] *************** Autotuning format combination: Float(51200,400,20,1), Float(51200,400,20,1) -> Int8(51200,400,20,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x0000000000000000 Time: 0.0024334
[X] Tactic: 0x0000000000000001 Time: 0.00240338
[X] Tactic: 0x0000000000000002 Time: 0.00242133
[X] Tactic: 0x0000000000000003 Time: 0.00269273
[X] Tactic: 0x0000000000000004 Time: 0.00255112
[X] Tactic: 0x0000000000000005 Time: 0.00243246
[X] Tactic: 0x0000000000000006 Time: 0.00336277
[X] Tactic: 0x0000000000000007 Time: 0.00299857
[X] Tactic: 0x0000000000000008 Time: 0.00271332
[X] Tactic: 0x0000000000000009 Time: 0.00255332
[X] Tactic: 0x000000000000001c Time: 0.00243504
[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0322123 seconds. Fastest Tactic: 0x0000000000000001 Time: 0.00240338
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000001
[X] *************** Autotuning format combination: Float(1600,400:32,20,1), Float(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])
[X] Tactic: 0x000000000000000a Time: 0.00335947
[X] Tactic: 0x000000000000000b Time: 0.00395809
[X] Tactic: 0x000000000000000c Time: 0.00547847
[X] Tactic: 0x000000000000000d Time: 0.00552446
[X] Tactic: 0x000000000000000e Time: 0.00625324
[X] Tactic: 0x000000000000000f Time: 0.00947881
[X] Tactic: 0x0000000000000010 Time: 0.00863945
[X] Tactic: 0x0000000000000011 Time: 0.0103027
[X] Tactic: 0x0000000000000012 Time: 0.0145971
[X] Tactic: 0x0000000000000013 Time: 0.0193576
[X] Tactic: 0x0000000000000014 Time: 0.00279671
[X] Tactic: 0x0000000000000015 Time: 0.00312344
[X] Tactic: 0x0000000000000016 Time: 0.00409743
[X] Tactic: 0x0000000000000017 Time: 0.00592992
[X] Tactic: 0x0000000000000018 Time: 0.00239429
[X] Tactic: 0x0000000000000019 Time: 0.00267733
[X] Tactic: 0x000000000000001a Time: 0.00308198
[X] Tactic: 0x000000000000001b Time: 0.00411473
[X] Tactic: 0x000000000000001d Time: 0.00515085
[X] Tactic: 0x000000000000001e Time: 0.00283697
[X] Tactic: 0x000000000000001f Time: 0.00244921
[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0544958 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00239429
[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018
[X] =============== Computing costs for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(102400,400,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(3200,400:32,20,1) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(6400,1:16,320,16) ***************
[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0060961
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.0031124
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00593797
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0050977
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00358606
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00383853
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00485735
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00338284
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.0032778
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00620484
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00339814
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00454501
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00330092
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.0037645
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00401079
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00314918
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00394742
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00340289
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.003311
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00611355
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00624692
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00469979
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00411291
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00482682
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00473855
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00456303
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00399175
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00351798
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00337368
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00369876
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00644021
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00321498
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00414828
[X] model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.101657 seconds. Fastest Tactic: 0xc6cdb1e47323bb01 Time: 0.0031124
[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc6cdb1e47323bb01
[X] =============== Computing costs for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv
[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00978438
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00873764
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00995953
[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0090485
[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0108405 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00873764
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00569094
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00430496
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00406608
[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00868997 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00406608
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6
[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0108975
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00334443
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.006962
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00388192
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00717163
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0105447
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00487992
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00712442
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0104055
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00384098
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00390264
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00406194
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0034252
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00489461
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00734006
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0047025
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.004736
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00348522
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0036684
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00357061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0104061
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0103955
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00669547
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00676075
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0048192
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00356823
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00354973
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00358859
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0041438
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00562827
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00508768
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00663864
[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0105457
[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0916634 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00334443
[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv
[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190
[X] =============== Computing costs for {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]}
[X] *************** Autotuning format combination: Int64(2,1), Float(1638400,6400,80,1), Float(409600,1600,40,1), Float(102400,400,20,1) -> Float(57600,192,1), Float(57600,192,1), Float(57600,192,1), Int64(300,1), Float(1200,4,1), Float(300,1) ***************
[X] --------------- Timing Runner: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023])
[X]  (foreignNode) Set user's cuda kernel library
[X] Subgraph compilation completed in 7.306 seconds.
[X] Tactic: 0x0000000000000000 Time: 0.413013
[X] {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023]) profiling completed in 7.44017 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.413013
[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(1228800,409600,640,1) ***************
[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00497719
[X] Tactic: 0x00000000000003ea Time: 0.00761647
[X] Tactic: 0x0000000000000000 Time: 0.00494635
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00910646 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00494635
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************
[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00932918
[X] Tactic: 0x00000000000003ea Time: 0.0109547
[X] Tactic: 0x0000000000000000 Time: 0.00404241
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00739785 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00404241
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************
[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0287209
[X] Tactic: 0x00000000000003ea Time: 0.0239429
[X] Tactic: 0x0000000000000000 Time: 0.0287422
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0058318 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0239429
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************
[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.216347
[X] Tactic: 0x00000000000003ea Time: 0.0202447
[X] Tactic: 0x0000000000000000 Time: 0.215872
[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00719783 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0202447
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0101679
[X] Tactic: 0x00000000000003ea Time: 0.0105137
[X] Tactic: 0x0000000000000000 Time: 0.0101585
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00801022 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0101585
[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0295271
[X] Tactic: 0x00000000000003ea Time: 0.0259889
[X] Tactic: 0x0000000000000000 Time: 0.0297004
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0081876 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0259889
[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.216176
[X] Tactic: 0x00000000000003ea Time: 0.0281804
[X] Tactic: 0x0000000000000000 Time: 0.215781
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00798403 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0281804
[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(1228800,409600,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00893109
[X] Tactic: 0x00000000000003ea Time: 0.0229191
[X] Tactic: 0x0000000000000000 Time: 0.00859132
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00847185 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00859132
[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,1:16,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.030064
[X] Tactic: 0x00000000000003ea Time: 0.0182007
[X] Tactic: 0x0000000000000000 Time: 0.0300329
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00693223 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0182007
[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,409600:32,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.216405
[X] Tactic: 0x00000000000003ea Time: 0.030849
[X] Tactic: 0x0000000000000000 Time: 0.0160437
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00680689 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0160437
[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(1228800,409600,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00895495
[X] Tactic: 0x00000000000003ea Time: 0.05024
[X] Tactic: 0x0000000000000000 Time: 0.00892828
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00777528 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00892828
[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:4,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0108105
[X] Tactic: 0x00000000000003ea Time: 0.0146574
[X] Tactic: 0x0000000000000000 Time: 0.0108783
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00745209 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0108105
[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:32,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.217083
[X] Tactic: 0x00000000000003ea Time: 0.0241219
[X] Tactic: 0x0000000000000000 Time: 0.217088
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00682887 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0241219
[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(1228800,409600,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0117734
[X] Tactic: 0x00000000000003ea Time: 0.0502781
[X] Tactic: 0x0000000000000000 Time: 0.0117734
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00743501 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0117734
[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,409600:4,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0130917
[X] Tactic: 0x00000000000003ea Time: 0.0186756
[X] Tactic: 0x0000000000000000 Time: 0.00673579
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00999731 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00673579
[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,1:16,640,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0300942
[X] Tactic: 0x00000000000003ea Time: 0.0146579
[X] Tactic: 0x0000000000000000 Time: 0.0302009
[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00699656 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0146579
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0262712
[X] Tactic: 0x00000000000003ea Time: 0.0147636
[X] Tactic: 0x0000000000000000 Time: 0.026208
[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00653843 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0147636
[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0640196
[X] Tactic: 0x00000000000003ea Time: 0.0120964
[X] Tactic: 0x0000000000000000 Time: 0.064016
[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00650843 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0120964
[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00996988
[X] Tactic: 0x00000000000003ea Time: 0.0213147
[X] Tactic: 0x0000000000000000 Time: 0.0051415
[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00896689 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0051415
[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0199416
[X] Tactic: 0x00000000000003ea Time: 0.0146722
[X] Tactic: 0x0000000000000000 Time: 0.0198839
[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00645165 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0146722
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0147636 ms
[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0120964 ms
[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0051415 ms
[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0146722 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(1638400,102400:4,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0501775
[X] Tactic: 0x00000000000003ea Time: 0.0135014
[X] Tactic: 0x0000000000000000 Time: 0.0501227
[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00635553 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0135014
[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(204800,102400:32,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.138923
[X] Tactic: 0x00000000000003ea Time: 0.0139467
[X] Tactic: 0x0000000000000000 Time: 0.138908
[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00682352 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0139467
[X] *************** Autotuning Reformat: Int8(204800,102400:32,320,1) -> Int8(1638400,102400:4,320,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0155404
[X] Tactic: 0x00000000000003ea Time: 0.015824
[X] Tactic: 0x0000000000000000 Time: 0.00662735
[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00790201 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00662735
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0106037
[X] Tactic: 0x00000000000003ea Time: 0.0113173
[X] Tactic: 0x0000000000000000 Time: 0.00952148
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00690299 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00952148
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0102258
[X] Tactic: 0x00000000000003ea Time: 0.00983561
[X] Tactic: 0x0000000000000000 Time: 0.0101867
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00664201 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00983561
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0265633
[X] Tactic: 0x00000000000003ea Time: 0.00888477
[X] Tactic: 0x0000000000000000 Time: 0.00370227
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00734177 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00370227
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0134498
[X] Tactic: 0x00000000000003ea Time: 0.00761988
[X] Tactic: 0x0000000000000000 Time: 0.0134131
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.0063523 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00761988
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00630702
[X] Tactic: 0x00000000000003ea Time: 0.00868868
[X] Tactic: 0x0000000000000000 Time: 0.00335317
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00837654 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00335317
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0101831
[X] Tactic: 0x00000000000003ea Time: 0.0101873
[X] Tactic: 0x0000000000000000 Time: 0.0102555
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00666253 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0101831
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00950163
[X] Tactic: 0x00000000000003ea Time: 0.011259
[X] Tactic: 0x0000000000000000 Time: 0.00445867
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00774195 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00445867
[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.010784
[X] Tactic: 0x00000000000003ea Time: 0.0113312
[X] Tactic: 0x0000000000000000 Time: 0.0107795
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00690251 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0107795
[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0279911
[X] Tactic: 0x00000000000003ea Time: 0.0115994
[X] Tactic: 0x0000000000000000 Time: 0.0280436
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00576069 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0115994
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0102242
[X] Tactic: 0x00000000000003ea Time: 0.0106027
[X] Tactic: 0x0000000000000000 Time: 0.0102381
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.007132 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0102242
[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0266347
[X] Tactic: 0x00000000000003ea Time: 0.0145405
[X] Tactic: 0x0000000000000000 Time: 0.00492627
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00711006 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00492627
[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0136418
[X] Tactic: 0x00000000000003ea Time: 0.0162159
[X] Tactic: 0x0000000000000000 Time: 0.013719
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00712426 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0136418
[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0344203
[X] Tactic: 0x00000000000003ea Time: 0.0181659
[X] Tactic: 0x0000000000000000 Time: 0.0343115
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00654753 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0181659
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00638772
[X] Tactic: 0x00000000000003ea Time: 0.0136832
[X] Tactic: 0x0000000000000000 Time: 0.0035579
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00808138 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0035579
[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0101918
[X] Tactic: 0x00000000000003ea Time: 0.00767855
[X] Tactic: 0x0000000000000000 Time: 0.0101737
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00686138 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00767855
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0136418 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0181659 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0035579 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00445867 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0115994 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00492627 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0136418 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0181659 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0035579 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0415692
[X] Tactic: 0x00000000000003ea Time: 0.00818003
[X] Tactic: 0x0000000000000000 Time: 0.0416225
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00623665 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00818003
[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0222167
[X] Tactic: 0x00000000000003ea Time: 0.00946074
[X] Tactic: 0x0000000000000000 Time: 0.022214
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00629433 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00946074
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0416012
[X] Tactic: 0x00000000000003ea Time: 0.009728
[X] Tactic: 0x0000000000000000 Time: 0.0415585
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00625411 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.009728
[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0221987
[X] Tactic: 0x00000000000003ea Time: 0.0102765
[X] Tactic: 0x0000000000000000 Time: 0.0221933
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00635429 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0102765
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00528017
[X] Tactic: 0x00000000000003ea Time: 0.00970057
[X] Tactic: 0x0000000000000000 Time: 0.00529185
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00761129 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00528017
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00849333
[X] Tactic: 0x00000000000003ea Time: 0.00824429
[X] Tactic: 0x0000000000000000 Time: 0.0037331
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00766944 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0037331
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00922984
[X] Tactic: 0x00000000000003ea Time: 0.00753493
[X] Tactic: 0x0000000000000000 Time: 0.00922349
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0066234 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00753493
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0277891
[X] Tactic: 0x00000000000003ea Time: 0.00743893
[X] Tactic: 0x0000000000000000 Time: 0.0277136
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00605846 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00743893
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0224704
[X] Tactic: 0x00000000000003ea Time: 0.00797536
[X] Tactic: 0x0000000000000000 Time: 0.0224768
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00589369 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00797536
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0225543
[X] Tactic: 0x00000000000003ea Time: 0.0084848
[X] Tactic: 0x0000000000000000 Time: 0.0225252
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00601863 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0084848
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00931466
[X] Tactic: 0x00000000000003ea Time: 0.00816026
[X] Tactic: 0x0000000000000000 Time: 0.00928622
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00663969 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00816026
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0409043
[X] Tactic: 0x00000000000003ea Time: 0.00824846
[X] Tactic: 0x0000000000000000 Time: 0.0408581
[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00603226 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00824846
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00445867 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x0000000000000000, 0.0107795 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0115994 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0102242 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00492627 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0136418 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0181659 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0035579 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00767855 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0136418 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0181659 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0035579 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00818003 ms
[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00946074 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.1/act/Relu_output_0) [Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.009728 ms
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.1/act/Relu_output_0) [Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0102765 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0037331 ms
[X] /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00743893 ms
[X] /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0084848 ms
[X] /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00824846 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00492627 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0035579 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00528017 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0037331 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00753493 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00743893 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00797536 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0084848 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00816026 ms
[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00824846 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00445867 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x0000000000000000, 0.0107795 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0115994 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0102242 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00492627 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0136418 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0181659 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0035579 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00767855 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0129116
[X] Tactic: 0x00000000000003ea Time: 0.00754536
[X] Tactic: 0x0000000000000000 Time: 0.0127419
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00635003 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00754536
[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0224263
[X] Tactic: 0x00000000000003ea Time: 0.00787994
[X] Tactic: 0x0000000000000000 Time: 0.0224313
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00584868 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00787994
[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00470715
[X] Tactic: 0x00000000000003ea Time: 0.00712987
[X] Tactic: 0x0000000000000000 Time: 0.00286423
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00812361 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00286423
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(102400,6400:4,80,1) -> Int8(12800,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00950459
[X] Tactic: 0x00000000000003ea Time: 0.00568678
[X] Tactic: 0x0000000000000000 Time: 0.00257017
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00750191 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00257017
[X] *************** Autotuning Reformat: Int8(12800,6400:32,80,1) -> Int8(102400,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00361462
[X] Tactic: 0x00000000000003ea Time: 0.00563734
[X] Tactic: 0x0000000000000000 Time: 0.00267051
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00801932 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267051
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0221053
[X] Tactic: 0x00000000000003ea Time: 0.00561867
[X] Tactic: 0x0000000000000000 Time: 0.0220813
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0061844 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561867
[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0123097
[X] Tactic: 0x00000000000003ea Time: 0.00533655
[X] Tactic: 0x0000000000000000 Time: 0.0123078
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00646304 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00533655
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.022122
[X] Tactic: 0x00000000000003ea Time: 0.005312
[X] Tactic: 0x0000000000000000 Time: 0.0221107
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00619901 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.005312
[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0123299
[X] Tactic: 0x00000000000003ea Time: 0.00528617
[X] Tactic: 0x0000000000000000 Time: 0.0123379
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00648599 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00528617
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00368493
[X] Tactic: 0x00000000000003ea Time: 0.00544688
[X] Tactic: 0x0000000000000000 Time: 0.00367027
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00805131 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00367027
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00542555
[X] Tactic: 0x00000000000003ea Time: 0.00561262
[X] Tactic: 0x0000000000000000 Time: 0.00306026
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00795383 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00306026
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.009872
[X] Tactic: 0x00000000000003ea Time: 0.00557262
[X] Tactic: 0x0000000000000000 Time: 0.009888
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00640314 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557262
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0154099
[X] Tactic: 0x00000000000003ea Time: 0.00561085
[X] Tactic: 0x0000000000000000 Time: 0.015424
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00611479 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561085
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0126131
[X] Tactic: 0x00000000000003ea Time: 0.00558453
[X] Tactic: 0x0000000000000000 Time: 0.0126266
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00626126 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558453
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.012719
[X] Tactic: 0x00000000000003ea Time: 0.00559449
[X] Tactic: 0x0000000000000000 Time: 0.0127423
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00633988 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559449
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00586218
[X] Tactic: 0x00000000000003ea Time: 0.00558098
[X] Tactic: 0x0000000000000000 Time: 0.00585269
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00711454 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558098
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0222635
[X] Tactic: 0x00000000000003ea Time: 0.00552673
[X] Tactic: 0x0000000000000000 Time: 0.0222315
[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00594285 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552673
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00622795
[X] Tactic: 0x00000000000003ea Time: 0.00574011
[X] Tactic: 0x0000000000000000 Time: 0.00326798
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00712593 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00326798
[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00954545
[X] Tactic: 0x00000000000003ea Time: 0.00566599
[X] Tactic: 0x0000000000000000 Time: 0.00956556
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00654445 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00566599
[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0156606
[X] Tactic: 0x00000000000003ea Time: 0.00572113
[X] Tactic: 0x0000000000000000 Time: 0.015583
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00618841 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00572113
[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00640423
[X] Tactic: 0x00000000000003ea Time: 0.00562578
[X] Tactic: 0x0000000000000000 Time: 0.0063682
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00717995 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562578
[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0155253
[X] Tactic: 0x00000000000003ea Time: 0.00550645
[X] Tactic: 0x0000000000000000 Time: 0.00312623
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00747526 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00312623
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00754536 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00787994 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286423 ms
[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00630319
[X] Tactic: 0x00000000000003ea Time: 0.00552866
[X] Tactic: 0x0000000000000000 Time: 0.00629454
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00701586 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552866
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00754536 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00787994 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286423 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561867 ms
[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00533655 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.1/act/Relu_output_0) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.005312 ms
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.1/act/Relu_output_0) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00528617 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00367027 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00306026 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557262 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561085 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558453 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559449 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558098 ms
[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552673 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00326798 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566599 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572113 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562578 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00312623 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00754536 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00787994 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286423 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552866 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00306026 ms
[X] /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561085 ms
[X] /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559449 ms
[X] /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552673 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00312623 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286423 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00806933
[X] Tactic: 0x00000000000003ea Time: 0.00562702
[X] Tactic: 0x0000000000000000 Time: 0.00814699
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00673621 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562702
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0130412
[X] Tactic: 0x00000000000003ea Time: 0.0055075
[X] Tactic: 0x0000000000000000 Time: 0.0130519
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00629676 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055075
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00360774
[X] Tactic: 0x00000000000003ea Time: 0.00565785
[X] Tactic: 0x0000000000000000 Time: 0.00268425
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00889316 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268425
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00643733
[X] Tactic: 0x00000000000003ea Time: 0.00559005
[X] Tactic: 0x0000000000000000 Time: 0.00256334
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00762892 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00256334
[X] *************** Autotuning Reformat: Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00311092
[X] Tactic: 0x00000000000003ea Time: 0.00555156
[X] Tactic: 0x0000000000000000 Time: 0.00253867
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00834272 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00253867
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0123706
[X] Tactic: 0x00000000000003ea Time: 0.00550085
[X] Tactic: 0x0000000000000000 Time: 0.0123923
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00626189 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00550085
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00747875
[X] Tactic: 0x00000000000003ea Time: 0.00539217
[X] Tactic: 0x0000000000000000 Time: 0.00751028
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00677631 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00539217
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0123804
[X] Tactic: 0x00000000000003ea Time: 0.00557493
[X] Tactic: 0x0000000000000000 Time: 0.0124286
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00643891 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557493
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00747354
[X] Tactic: 0x00000000000003ea Time: 0.00558365
[X] Tactic: 0x0000000000000000 Time: 0.00749132
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00681184 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558365
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00297972
[X] Tactic: 0x00000000000003ea Time: 0.0054345
[X] Tactic: 0x0000000000000000 Time: 0.00299314
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00874889 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00297972
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00398629
[X] Tactic: 0x00000000000003ea Time: 0.00556516
[X] Tactic: 0x0000000000000000 Time: 0.00258322
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00809511 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00258322
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0072
[X] Tactic: 0x00000000000003ea Time: 0.00555929
[X] Tactic: 0x0000000000000000 Time: 0.00720454
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00676653 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00555929
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00934192
[X] Tactic: 0x00000000000003ea Time: 0.00564516
[X] Tactic: 0x0000000000000000 Time: 0.00927585
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00642872 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00564516
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00765455
[X] Tactic: 0x00000000000003ea Time: 0.00554859
[X] Tactic: 0x0000000000000000 Time: 0.00765139
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00669602 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554859
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00779163
[X] Tactic: 0x00000000000003ea Time: 0.00545893
[X] Tactic: 0x0000000000000000 Time: 0.00774958
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00673631 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00545893
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00415091
[X] Tactic: 0x00000000000003ea Time: 0.00551607
[X] Tactic: 0x0000000000000000 Time: 0.004218
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00789085 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00415091
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0127222
[X] Tactic: 0x00000000000003ea Time: 0.00554247
[X] Tactic: 0x0000000000000000 Time: 0.0127439
[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00661842 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554247
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.004447
[X] Tactic: 0x00000000000003ea Time: 0.00579053
[X] Tactic: 0x0000000000000000 Time: 0.00288993
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00801561 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00288993
[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00719728
[X] Tactic: 0x00000000000003ea Time: 0.00547515
[X] Tactic: 0x0000000000000000 Time: 0.00716187
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00699672 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00547515
[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00943318
[X] Tactic: 0x00000000000003ea Time: 0.00558436
[X] Tactic: 0x0000000000000000 Time: 0.00944533
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00652893 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558436
[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00459222
[X] Tactic: 0x00000000000003ea Time: 0.00552464
[X] Tactic: 0x0000000000000000 Time: 0.00462232
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00755247 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00459222
[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00939081
[X] Tactic: 0x00000000000003ea Time: 0.00548057
[X] Tactic: 0x0000000000000000 Time: 0.00300406
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00762257 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00300406
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562702 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055075 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268425 ms
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00444729
[X] Tactic: 0x00000000000003ea Time: 0.0055367
[X] Tactic: 0x0000000000000000 Time: 0.00446919
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00759798 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00444729
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562702 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055075 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268425 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(409600,1600,40,1) -> Float(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00550085 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00539217 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.1/act/Relu_output_0) [Float(409600,1600,40,1) -> Float(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557493 ms
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.1/act/Relu_output_0) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558365 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00297972 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00258322 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00555929 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564516 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554859 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00545893 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00415091 ms
[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554247 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288993 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00547515 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558436 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00459222 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00300406 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562702 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055075 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268425 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00444729 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear
[X] /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00258322 ms
[X] /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564516 ms
[X] /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00545893 ms
[X] /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554247 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00300406 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268425 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0055712
[X] Tactic: 0x00000000000003ea Time: 0.00545807
[X] Tactic: 0x0000000000000000 Time: 0.00563627
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00726851 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00545807
[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00827785
[X] Tactic: 0x00000000000003ea Time: 0.00547602
[X] Tactic: 0x0000000000000000 Time: 0.00831376
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00666327 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00547602
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0031357
[X] Tactic: 0x00000000000003ea Time: 0.0054369
[X] Tactic: 0x0000000000000000 Time: 0.00247032
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00810412 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00247032
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00450119
[X] Tactic: 0x00000000000003ea Time: 0.00546219
[X] Tactic: 0x0000000000000000 Time: 0.00260492
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00825091 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00260492
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00285026
[X] Tactic: 0x00000000000003ea Time: 0.00547095
[X] Tactic: 0x0000000000000000 Time: 0.00230128
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00845679 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00230128
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00727791
[X] Tactic: 0x00000000000003ea Time: 0.00531098
[X] Tactic: 0x0000000000000000 Time: 0.00729762
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00685525 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00531098
[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00504677
[X] Tactic: 0x00000000000003ea Time: 0.00543002
[X] Tactic: 0x0000000000000000 Time: 0.00506069
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00766351 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00504677
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00731409
[X] Tactic: 0x00000000000003ea Time: 0.00541437
[X] Tactic: 0x0000000000000000 Time: 0.00725519
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00716527 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00541437
[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00507588
[X] Tactic: 0x00000000000003ea Time: 0.00542125
[X] Tactic: 0x0000000000000000 Time: 0.00503451
[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00764855 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00503451
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(204800,400,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00264559
[X] Tactic: 0x00000000000003ea Time: 0.00529287
[X] Tactic: 0x0000000000000000 Time: 0.0026857
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00945226 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00264559
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00315995
[X] Tactic: 0x00000000000003ea Time: 0.00539613
[X] Tactic: 0x0000000000000000 Time: 0.00232437
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00908022 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00232437
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00516677
[X] Tactic: 0x00000000000003ea Time: 0.00537295
[X] Tactic: 0x0000000000000000 Time: 0.00516496
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00746591 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00516496
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00586798
[X] Tactic: 0x00000000000003ea Time: 0.00548721
[X] Tactic: 0x0000000000000000 Time: 0.00587022
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00717754 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00548721
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(204800,400,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00507345
[X] Tactic: 0x00000000000003ea Time: 0.00548756
[X] Tactic: 0x0000000000000000 Time: 0.00503737
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00745541 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00503737
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00514658
[X] Tactic: 0x00000000000003ea Time: 0.00539458
[X] Tactic: 0x0000000000000000 Time: 0.00515955
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00749704 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00514658
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00328126
[X] Tactic: 0x00000000000003ea Time: 0.00530963
[X] Tactic: 0x0000000000000000 Time: 0.00329088
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00867398 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00328126
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00778022
[X] Tactic: 0x00000000000003ea Time: 0.00542693
[X] Tactic: 0x0000000000000000 Time: 0.00778071
[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00688355 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00542693
[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00344204
[X] Tactic: 0x00000000000003ea Time: 0.00552219
[X] Tactic: 0x0000000000000000 Time: 0.00273618
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00906324 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00273618
[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00564231
[X] Tactic: 0x00000000000003ea Time: 0.00552341
[X] Tactic: 0x0000000000000000 Time: 0.00565496
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00753306 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552341
[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00616204
[X] Tactic: 0x00000000000003ea Time: 0.00556764
[X] Tactic: 0x0000000000000000 Time: 0.00614575
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00728369 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00556764
[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0037754
[X] Tactic: 0x00000000000003ea Time: 0.00555662
[X] Tactic: 0x0000000000000000 Time: 0.00374139
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0081645 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00374139
[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00598114
[X] Tactic: 0x00000000000003ea Time: 0.00562738
[X] Tactic: 0x0000000000000000 Time: 0.00299705
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00765907 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00299705
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00545807 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00547602 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00247032 ms
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0035486
[X] Tactic: 0x00000000000003ea Time: 0.00553757
[X] Tactic: 0x0000000000000000 Time: 0.0035545
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00818304 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0035486
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00545807 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00547602 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00247032 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(204800,400,20,1) -> Float(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00531098 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(6400,400:32,20,1) -> Float(204800,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00504677 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00232437 ms
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00548721 ms
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00514658 ms
[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00542693 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299705 ms
[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00247032 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00364997
[X] Tactic: 0x00000000000003ea Time: 0.0054486
[X] Tactic: 0x0000000000000000 Time: 0.00367098
[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00861074 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00364997
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00326798 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566599 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572113 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562578 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00312623 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00754536 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00787994 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286423 ms
[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552866 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(3276800,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0238781
[X] Tactic: 0x00000000000003ea Time: 0.00638531
[X] Tactic: 0x0000000000000000 Time: 0.0239497
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00595111 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00638531
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0244838
[X] Tactic: 0x00000000000003ea Time: 0.00560249
[X] Tactic: 0x0000000000000000 Time: 0.0243375
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0060634 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00560249
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0427173
[X] Tactic: 0x00000000000003ea Time: 0.00615137
[X] Tactic: 0x0000000000000000 Time: 0.042712
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00584348 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00615137
[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(3276800,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0131077
[X] Tactic: 0x00000000000003ea Time: 0.00737577
[X] Tactic: 0x0000000000000000 Time: 0.0131143
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00634209 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00737577
[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00634566
[X] Tactic: 0x00000000000003ea Time: 0.00786927
[X] Tactic: 0x0000000000000000 Time: 0.00338511
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00803494 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00338511
[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(204800,1:16,2560,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0103664
[X] Tactic: 0x00000000000003ea Time: 0.00689306
[X] Tactic: 0x0000000000000000 Time: 0.0103916
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0065887 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00689306
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(3276800,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0222013
[X] Tactic: 0x00000000000003ea Time: 0.00797003
[X] Tactic: 0x0000000000000000 Time: 0.0221873
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00624204 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00797003
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0228679
[X] Tactic: 0x00000000000003ea Time: 0.00990086
[X] Tactic: 0x0000000000000000 Time: 0.0227705
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00614393 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00990086
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(204800,1:16,2560,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0103286
[X] Tactic: 0x00000000000003ea Time: 0.0101026
[X] Tactic: 0x0000000000000000 Time: 0.0103308
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00662151 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0101026
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0417867
[X] Tactic: 0x00000000000003ea Time: 0.00959604
[X] Tactic: 0x0000000000000000 Time: 0.0419533
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00574775 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00959604
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0131803
[X] Tactic: 0x00000000000003ea Time: 0.00977371
[X] Tactic: 0x0000000000000000 Time: 0.0131274
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00628325 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00977371
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00636579
[X] Tactic: 0x00000000000003ea Time: 0.00912894
[X] Tactic: 0x0000000000000000 Time: 0.00339491
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00814931 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00339491
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0103667
[X] Tactic: 0x00000000000003ea Time: 0.00828566
[X] Tactic: 0x0000000000000000 Time: 0.0103586
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00661062 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00828566
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0273017
[X] Tactic: 0x00000000000003ea Time: 0.00841467
[X] Tactic: 0x0000000000000000 Time: 0.00385508
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00707687 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00385508
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288993 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00547515 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558436 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00459222 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00300406 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562702 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055075 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268425 ms
[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00444729 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(819200,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00845093
[X] Tactic: 0x00000000000003ea Time: 0.00731988
[X] Tactic: 0x0000000000000000 Time: 0.00846747
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00675964 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00731988
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00866626
[X] Tactic: 0x00000000000003ea Time: 0.00725519
[X] Tactic: 0x0000000000000000 Time: 0.00862058
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00659889 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00725519
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0136738
[X] Tactic: 0x00000000000003ea Time: 0.00741878
[X] Tactic: 0x0000000000000000 Time: 0.0136939
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0062223 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00741878
[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00578832
[X] Tactic: 0x00000000000003ea Time: 0.00737762
[X] Tactic: 0x0000000000000000 Time: 0.00577931
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00722078 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00577931
[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00363444
[X] Tactic: 0x00000000000003ea Time: 0.00649456
[X] Tactic: 0x0000000000000000 Time: 0.00268954
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00847053 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268954
[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00445184
[X] Tactic: 0x00000000000003ea Time: 0.00548966
[X] Tactic: 0x0000000000000000 Time: 0.00442863
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00777502 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00442863
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00781395
[X] Tactic: 0x00000000000003ea Time: 0.00538529
[X] Tactic: 0x0000000000000000 Time: 0.00784695
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00688465 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00538529
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00815454
[X] Tactic: 0x00000000000003ea Time: 0.00566942
[X] Tactic: 0x0000000000000000 Time: 0.0081148
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00670115 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00566942
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00442877
[X] Tactic: 0x00000000000003ea Time: 0.00557653
[X] Tactic: 0x0000000000000000 Time: 0.00438638
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00763635 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00438638
[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0130445
[X] Tactic: 0x00000000000003ea Time: 0.00551956
[X] Tactic: 0x0000000000000000 Time: 0.0130736
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00629829 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551956
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00575025
[X] Tactic: 0x00000000000003ea Time: 0.00559698
[X] Tactic: 0x0000000000000000 Time: 0.00575154
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00755538 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559698
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00364325
[X] Tactic: 0x00000000000003ea Time: 0.00553076
[X] Tactic: 0x0000000000000000 Time: 0.0026621
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00787851 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0026621
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00443705
[X] Tactic: 0x00000000000003ea Time: 0.00560676
[X] Tactic: 0x0000000000000000 Time: 0.00445582
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00771112 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00443705
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00977371
[X] Tactic: 0x00000000000003ea Time: 0.00561138
[X] Tactic: 0x0000000000000000 Time: 0.00269307
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00703521 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00269307
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(25600,400:4,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00271185
[X] Tactic: 0x00000000000003ea Time: 0.00542142
[X] Tactic: 0x0000000000000000 Time: 0.00225629
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00840801 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00225629
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00440954
[X] Tactic: 0x00000000000003ea Time: 0.00565062
[X] Tactic: 0x0000000000000000 Time: 0.00444365
[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00777726 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00440954
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00260492 ms
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00230128 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00351843
[X] Tactic: 0x00000000000003ea Time: 0.00545497
[X] Tactic: 0x0000000000000000 Time: 0.00244391
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0091885 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00244391
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00271818
[X] Tactic: 0x00000000000003ea Time: 0.00559307
[X] Tactic: 0x0000000000000000 Time: 0.00529456
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00786585 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00271818
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.003541
[X] Tactic: 0x00000000000003ea Time: 0.00552813
[X] Tactic: 0x0000000000000000 Time: 0.00242412
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00807261 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00242412
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00492973
[X] Tactic: 0x00000000000003ea Time: 0.00544516
[X] Tactic: 0x0000000000000000 Time: 0.00492016
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00744643 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00492016
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00444629
[X] Tactic: 0x00000000000003ea Time: 0.00558276
[X] Tactic: 0x0000000000000000 Time: 0.00524667
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00831574 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00444629
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00364997 ms
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00271019
[X] Tactic: 0x00000000000003ea Time: 0.00532673
[X] Tactic: 0x0000000000000000 Time: 0.00272338
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00806785 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00271019
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00365299
[X] Tactic: 0x00000000000003ea Time: 0.00549788
[X] Tactic: 0x0000000000000000 Time: 0.00367672
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00827306 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00365299
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00267179
[X] Tactic: 0x00000000000003ea Time: 0.00559431
[X] Tactic: 0x0000000000000000 Time: 0.00272364
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00777835 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00267179
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0112028
[X] Tactic: 0x00000000000003ea Time: 0.00591064
[X] Tactic: 0x0000000000000000 Time: 0.0112018
[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00645104 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00591064
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00355245
[X] Tactic: 0x00000000000003ea Time: 0.0056336
[X] Tactic: 0x0000000000000000 Time: 0.00239139
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00818877 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00239139
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00269557
[X] Tactic: 0x00000000000003ea Time: 0.00565026
[X] Tactic: 0x0000000000000000 Time: 0.0052205
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00745611 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00269557
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00353044
[X] Tactic: 0x00000000000003ea Time: 0.00561707
[X] Tactic: 0x0000000000000000 Time: 0.00241174
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00854015 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00241174
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00488162
[X] Tactic: 0x00000000000003ea Time: 0.0056725
[X] Tactic: 0x0000000000000000 Time: 0.00489012
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00797199 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00488162
[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00446037
[X] Tactic: 0x00000000000003ea Time: 0.005713
[X] Tactic: 0x0000000000000000 Time: 0.00523233
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00761488 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00446037
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00374676
[X] Tactic: 0x00000000000003ea Time: 0.0056064
[X] Tactic: 0x0000000000000000 Time: 0.00230591
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00823287 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00230591
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00372516
[X] Tactic: 0x00000000000003ea Time: 0.00552726
[X] Tactic: 0x0000000000000000 Time: 0.00369899
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00807459 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00369899
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00276046
[X] Tactic: 0x00000000000003ea Time: 0.00548267
[X] Tactic: 0x0000000000000000 Time: 0.00276743
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00880811 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00276046
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00586442
[X] Tactic: 0x00000000000003ea Time: 0.00558151
[X] Tactic: 0x0000000000000000 Time: 0.00587528
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00703584 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558151
[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0109777
[X] Tactic: 0x00000000000003ea Time: 0.00595543
[X] Tactic: 0x0000000000000000 Time: 0.0109939
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00643133 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00595543
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00228902
[X] Tactic: 0x00000000000003ea Time: 0.00549246
[X] Tactic: 0x0000000000000000 Time: 0.00549753
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00750024 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00228902
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00353954
[X] Tactic: 0x00000000000003ea Time: 0.00552516
[X] Tactic: 0x0000000000000000 Time: 0.00351472
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00807481 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00351472
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00352741
[X] Tactic: 0x00000000000003ea Time: 0.00545927
[X] Tactic: 0x0000000000000000 Time: 0.00349022
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00813442 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00349022
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0048793
[X] Tactic: 0x00000000000003ea Time: 0.00537007
[X] Tactic: 0x0000000000000000 Time: 0.00491639
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00746021 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0048793
[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00439817
[X] Tactic: 0x00000000000003ea Time: 0.0166156
[X] Tactic: 0x0000000000000000 Time: 0.00439382
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00699337 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00439382
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00374376
[X] Tactic: 0x00000000000003ea Time: 0.00549193
[X] Tactic: 0x0000000000000000 Time: 0.0022658
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0080277 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0022658
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00273539
[X] Tactic: 0x00000000000003ea Time: 0.00548354
[X] Tactic: 0x0000000000000000 Time: 0.00274815
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00888845 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00273539
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00371236
[X] Tactic: 0x00000000000003ea Time: 0.00538168
[X] Tactic: 0x0000000000000000 Time: 0.0036875
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00798316 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0036875
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00587022
[X] Tactic: 0x00000000000003ea Time: 0.00555419
[X] Tactic: 0x0000000000000000 Time: 0.00585245
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00758301 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00555419
[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0113188
[X] Tactic: 0x00000000000003ea Time: 0.00593217
[X] Tactic: 0x0000000000000000 Time: 0.0113195
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00684988 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00593217
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00369512
[X] Tactic: 0x00000000000003ea Time: 0.00537261
[X] Tactic: 0x0000000000000000 Time: 0.00366547
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00838301 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00366547
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00274291
[X] Tactic: 0x00000000000003ea Time: 0.00545015
[X] Tactic: 0x0000000000000000 Time: 0.00272945
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00941448 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00272945
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00369418
[X] Tactic: 0x00000000000003ea Time: 0.00539751
[X] Tactic: 0x0000000000000000 Time: 0.00371271
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00823987 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00369418
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00270125
[X] Tactic: 0x00000000000003ea Time: 0.00552149
[X] Tactic: 0x0000000000000000 Time: 0.00270658
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00943099 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00270125
[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.01121
[X] Tactic: 0x00000000000003ea Time: 0.00592038
[X] Tactic: 0x0000000000000000 Time: 0.0112476
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00648489 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00592038
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00278427
[X] Tactic: 0x00000000000003ea Time: 0.0159238
[X] Tactic: 0x0000000000000000 Time: 0.0055325
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00751044 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00278427
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,1,5120,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00357663
[X] Tactic: 0x00000000000003ea Time: 0.0170048
[X] Tactic: 0x0000000000000000 Time: 0.00359753
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00764753 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00357663
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(1,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00275526
[X] Tactic: 0x00000000000003ea Time: 0.0160574
[X] Tactic: 0x0000000000000000 Time: 0.00277501
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00876063 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00275526
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(25600,1:4,1280,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00364047
[X] Tactic: 0x00000000000003ea Time: 0.0170555
[X] Tactic: 0x0000000000000000 Time: 0.00366324
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00734334 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00364047
[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0048892
[X] Tactic: 0x00000000000003ea Time: 0.0170389
[X] Tactic: 0x0000000000000000 Time: 0.00485395
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00677026 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00485395
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00243659
[X] Tactic: 0x00000000000003ea Time: 0.00536872
[X] Tactic: 0x0000000000000000 Time: 0.00275786
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00848697 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00243659
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(102400,400,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00440954 ms
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0038257
[X] Tactic: 0x00000000000003ea Time: 0.0104353
[X] Tactic: 0x0000000000000000 Time: 0.0038074
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00827859 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0038074
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00548389
[X] Tactic: 0x00000000000003ea Time: 0.00671061
[X] Tactic: 0x0000000000000000 Time: 0.00553163
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00760482 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00548389
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00286468
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00253617 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00286468
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0043913
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00252118 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0043913
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00382303
[X] Tactic: 0x00000000000003ea Time: 0.00545325
[X] Tactic: 0x0000000000000000 Time: 0.00382061
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00787236 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00382061
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00545462
[X] Tactic: 0x00000000000003ea Time: 0.00549089
[X] Tactic: 0x0000000000000000 Time: 0.00547392
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00719282 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00545462
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00379782
[X] Tactic: 0x00000000000003ea Time: 0.00545617
[X] Tactic: 0x0000000000000000 Time: 0.00378715
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0078263 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00378715
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00539613
[X] Tactic: 0x00000000000003ea Time: 0.00551012
[X] Tactic: 0x0000000000000000 Time: 0.00542125
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00719058 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00539613
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00286879
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0031634 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00286879
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00435283
[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00251427 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00435283
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00458477
[X] Tactic: 0x00000000000003ea Time: 0.00564391
[X] Tactic: 0x0000000000000000 Time: 0.00456709
[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00750706 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00456709
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00338392
[X] Tactic: 0x00000000000003ea Time: 0.00548302
[X] Tactic: 0x0000000000000000 Time: 0.00337121
[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00821776 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00337121
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00272607
[X] Tactic: 0x00000000000003ea Time: 0.00543966
[X] Tactic: 0x0000000000000000 Time: 0.00272546
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00918724 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00272546
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00439761
[X] Tactic: 0x00000000000003ea Time: 0.00543914
[X] Tactic: 0x0000000000000000 Time: 0.00285776
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00833011 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00285776
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00720159
[X] Tactic: 0x00000000000003ea Time: 0.0056432
[X] Tactic: 0x0000000000000000 Time: 0.0072118
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00730225 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0056432
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00946963
[X] Tactic: 0x00000000000003ea Time: 0.00587247
[X] Tactic: 0x0000000000000000 Time: 0.00945896
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00647375 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00587247
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00581223
[X] Tactic: 0x00000000000003ea Time: 0.00568407
[X] Tactic: 0x0000000000000000 Time: 0.00579973
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00700942 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00568407
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0036152
[X] Tactic: 0x00000000000003ea Time: 0.00555805
[X] Tactic: 0x0000000000000000 Time: 0.00270039
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00880991 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00270039
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00444643
[X] Tactic: 0x00000000000003ea Time: 0.00560338
[X] Tactic: 0x0000000000000000 Time: 0.00443424
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00789031 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00443424
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00974476
[X] Tactic: 0x00000000000003ea Time: 0.00563449
[X] Tactic: 0x0000000000000000 Time: 0.00271939
[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00809339 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00271939
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00627398
[X] Tactic: 0x00000000000003ea Time: 0.00559218
[X] Tactic: 0x0000000000000000 Time: 0.00341344
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00803158 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00341344
[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0160335
[X] Tactic: 0x00000000000003ea Time: 0.00559342
[X] Tactic: 0x0000000000000000 Time: 0.0160071
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00623755 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559342
[X] *************** Autotuning Reformat: Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0155602
[X] Tactic: 0x00000000000003ea Time: 0.00559253
[X] Tactic: 0x0000000000000000 Time: 0.0039409
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00730109 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0039409
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0136695
[X] Tactic: 0x00000000000003ea Time: 0.0056304
[X] Tactic: 0x0000000000000000 Time: 0.0137344
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0064456 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0056304
[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0232228
[X] Tactic: 0x00000000000003ea Time: 0.00577508
[X] Tactic: 0x0000000000000000 Time: 0.0231979
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00613879 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00577508
[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00465674
[X] Tactic: 0x00000000000003ea Time: 0.00642708
[X] Tactic: 0x0000000000000000 Time: 0.0028212
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00851726 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0028212
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559342 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0039409 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00577508 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00253867 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(204800,1600,40,1) -> Float(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00730064
[X] Tactic: 0x00000000000003ea Time: 0.00544637
[X] Tactic: 0x0000000000000000 Time: 0.00731687
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00704065 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544637
[X] *************** Autotuning Reformat: Float(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00501668
[X] Tactic: 0x00000000000003ea Time: 0.00542968
[X] Tactic: 0x0000000000000000 Time: 0.00502097
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0079384 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00501668
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00544637 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00501668 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00628701
[X] Tactic: 0x00000000000003ea Time: 0.00562507
[X] Tactic: 0x0000000000000000 Time: 0.00629857
[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00700754 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562507
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268425 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(409600,1,10240,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00695689
[X] Tactic: 0x00000000000003ea Time: 0.00567286
[X] Tactic: 0x0000000000000000 Time: 0.0030464
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00720117 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0030464
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(1,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00378199
[X] Tactic: 0x00000000000003ea Time: 0.00566997
[X] Tactic: 0x0000000000000000 Time: 0.0141973
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00694449 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00378199
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(102400,1:4,2560,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00698155
[X] Tactic: 0x00000000000003ea Time: 0.00548774
[X] Tactic: 0x0000000000000000 Time: 0.00304582
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0081147 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00304582
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) [Float(409600,1600,40,1) -> Float(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00550085 ms
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(1:4,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00992157
[X] Tactic: 0x00000000000003ea Time: 0.00595088
[X] Tactic: 0x0000000000000000 Time: 0.0142391
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00658196 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00595088
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00539217 ms
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1,10240,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00387323
[X] Tactic: 0x00000000000003ea Time: 0.00548687
[X] Tactic: 0x0000000000000000 Time: 0.00383927
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00867924 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00383927
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(1,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0074073
[X] Tactic: 0x00000000000003ea Time: 0.00549788
[X] Tactic: 0x0000000000000000 Time: 0.00742329
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00701221 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00549788
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(102400,1:4,2560,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00386881
[X] Tactic: 0x00000000000003ea Time: 0.00542933
[X] Tactic: 0x0000000000000000 Time: 0.00388403
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00829146 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00386881
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(1:4,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0308131
[X] Tactic: 0x00000000000003ea Time: 0.0133623
[X] Tactic: 0x0000000000000000 Time: 0.0307947
[X] Optimizer Reformat(/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00538219 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0133623
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(409600,1,10240,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00697933
[X] Tactic: 0x00000000000003ea Time: 0.00577692
[X] Tactic: 0x0000000000000000 Time: 0.00305542
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00825354 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00305542
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(1,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00380533
[X] Tactic: 0x00000000000003ea Time: 0.00550085
[X] Tactic: 0x0000000000000000 Time: 0.0141671
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00696865 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00380533
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(102400,1:4,2560,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00699444
[X] Tactic: 0x00000000000003ea Time: 0.00557547
[X] Tactic: 0x0000000000000000 Time: 0.00304824
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00767258 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00304824
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) [Float(409600,1600,40,1) -> Float(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557493 ms
[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(1:4,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00992596
[X] Tactic: 0x00000000000003ea Time: 0.00604191
[X] Tactic: 0x0000000000000000 Time: 0.0141898
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00632659 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00604191
[X] *************** Autotuning Reformat: Float(409600,1,10240,256) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00744083
[X] Tactic: 0x00000000000003ea Time: 0.00548442
[X] Tactic: 0x0000000000000000 Time: 0.00307582
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00761255 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00307582
[X] *************** Autotuning Reformat: Float(409600,1,10240,256) -> Float(1,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00743656
[X] Tactic: 0x00000000000003ea Time: 0.00539148
[X] Tactic: 0x0000000000000000 Time: 0.00742732
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00676053 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00539148
[X] *************** Autotuning Reformat: Float(409600,1,10240,256) -> Float(102400,1:4,2560,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0038552
[X] Tactic: 0x00000000000003ea Time: 0.0055318
[X] Tactic: 0x0000000000000000 Time: 0.00383363
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00789801 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00383363
[X] *************** Autotuning Reformat: Float(409600,1,10240,256) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0165206
[X] Tactic: 0x00000000000003ea Time: 0.0055005
[X] Tactic: 0x0000000000000000 Time: 0.0165008
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0062211 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055005
[X] *************** Autotuning Reformat: Float(409600,1,10240,256) -> Float(1:4,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0311835
[X] Tactic: 0x00000000000003ea Time: 0.013271
[X] Tactic: 0x0000000000000000 Time: 0.0311011
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00543961 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.013271
[X] *************** Autotuning Reformat: Float(1,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00285292
[X] Tactic: 0x00000000000003ea Time: 0.00575209
[X] Tactic: 0x0000000000000000 Time: 0.0149046
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00732889 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00285292
[X] *************** Autotuning Reformat: Float(1,1600,40,1) -> Float(409600,1,10240,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0068776
[X] Tactic: 0x00000000000003ea Time: 0.00541437
[X] Tactic: 0x0000000000000000 Time: 0.00689415
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00696349 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00541437
[X] *************** Autotuning Reformat: Float(1,1600,40,1) -> Float(102400,1:4,2560,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00693333
[X] Tactic: 0x00000000000003ea Time: 0.00543123
[X] Tactic: 0x0000000000000000 Time: 0.00692245
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00695026 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00543123
[X] *************** Autotuning Reformat: Float(1,1600,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0123563
[X] Tactic: 0x00000000000003ea Time: 0.00554964
[X] Tactic: 0x0000000000000000 Time: 0.0123658
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00625507 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554964
[X] *************** Autotuning Reformat: Float(1,1600,40,1) -> Float(1:4,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00988423
[X] Tactic: 0x00000000000003ea Time: 0.0557316
[X] Tactic: 0x0000000000000000 Time: 0.00986635
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0057596 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00986635
[X] *************** Autotuning Reformat: Float(102400,1:4,2560,64) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00751431
[X] Tactic: 0x00000000000003ea Time: 0.00727165
[X] Tactic: 0x0000000000000000 Time: 0.00310203
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00733254 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00310203
[X] *************** Autotuning Reformat: Float(102400,1:4,2560,64) -> Float(409600,1,10240,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00387647
[X] Tactic: 0x00000000000003ea Time: 0.00546955
[X] Tactic: 0x0000000000000000 Time: 0.0038655
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00793529 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0038655
[X] *************** Autotuning Reformat: Float(102400,1:4,2560,64) -> Float(1,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00749464
[X] Tactic: 0x00000000000003ea Time: 0.00546955
[X] Tactic: 0x0000000000000000 Time: 0.00749345
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00677397 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00546955
[X] *************** Autotuning Reformat: Float(102400,1:4,2560,64) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.016448
[X] Tactic: 0x00000000000003ea Time: 0.00537007
[X] Tactic: 0x0000000000000000 Time: 0.0164566
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00627073 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00537007
[X] *************** Autotuning Reformat: Float(102400,1:4,2560,64) -> Float(1:4,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0311069
[X] Tactic: 0x00000000000003ea Time: 0.0133091
[X] Tactic: 0x0000000000000000 Time: 0.031041
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00543436 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0133091
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558365 ms
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1,10240,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00385484
[X] Tactic: 0x00000000000003ea Time: 0.00695422
[X] Tactic: 0x0000000000000000 Time: 0.00385459
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00799507 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00385459
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(1,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00744557
[X] Tactic: 0x00000000000003ea Time: 0.00594077
[X] Tactic: 0x0000000000000000 Time: 0.00742163
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00678681 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00594077
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(102400,1:4,2560,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00384503
[X] Tactic: 0x00000000000003ea Time: 0.00558116
[X] Tactic: 0x0000000000000000 Time: 0.00385962
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00794643 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00384503
[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(1:4,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0308926
[X] Tactic: 0x00000000000003ea Time: 0.0133202
[X] Tactic: 0x0000000000000000 Time: 0.0308451
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00537982 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0133202
[X] *************** Autotuning Reformat: Float(1:4,1600,40,1) -> Float(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00392025
[X] Tactic: 0x00000000000003ea Time: 0.0530377
[X] Tactic: 0x0000000000000000 Time: 0.0151082
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00660202 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00392025
[X] *************** Autotuning Reformat: Float(1:4,1600,40,1) -> Float(409600,1,10240,256) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00716573
[X] Tactic: 0x00000000000003ea Time: 0.0572142
[X] Tactic: 0x0000000000000000 Time: 0.00715007
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00640866 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00715007
[X] *************** Autotuning Reformat: Float(1:4,1600,40,1) -> Float(1,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00390586
[X] Tactic: 0x00000000000003ea Time: 0.0533928
[X] Tactic: 0x0000000000000000 Time: 0.00388453
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00802161 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00388453
[X] *************** Autotuning Reformat: Float(1:4,1600,40,1) -> Float(102400,1:4,2560,64) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0071764
[X] Tactic: 0x00000000000003ea Time: 0.0572267
[X] Tactic: 0x0000000000000000 Time: 0.0071183
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00650048 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0071183
[X] *************** Autotuning Reformat: Float(1:4,1600,40,1) -> Float(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.012595
[X] Tactic: 0x00000000000003ea Time: 0.0572498
[X] Tactic: 0x0000000000000000 Time: 0.0125989
[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.1/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0060896 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.012595
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(409600,1600,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00297972 ms
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564516 ms
[X] *************** Autotuning format combination: Float(409600,1,10240,256) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00803733
[X] Tactic: 0x00000000000003ea Time: 0.0115307
[X] Tactic: 0x0000000000000000 Time: 0.00762133
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0080182 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00762133
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(409600,1,10240,256) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0132258
[X] Tactic: 0x00000000000003ea Time: 0.0127708
[X] Tactic: 0x0000000000000000 Time: 0.0132156
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00659877 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0127708
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(1,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00387721
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0026382 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00387721
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,1600,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00934281
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0023613 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00934281
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,1:4,2560,64) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00774885
[X] Tactic: 0x00000000000003ea Time: 0.0116292
[X] Tactic: 0x0000000000000000 Time: 0.00777178
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.007229 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00774885
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,1:4,2560,64) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0132849
[X] Tactic: 0x00000000000003ea Time: 0.0121253
[X] Tactic: 0x0000000000000000 Time: 0.0132398
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00641815 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0121253
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554859 ms
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554247 ms
[X] *************** Autotuning format combination: Float(1:4,1600,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00405744
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0030787 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00405744
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,1600,40,1) -> Int8(12800,1600:32,40,1) ***************
[X] --------------- Timing Runner: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00948355
[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00224288 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00948355
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558436 ms
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00580175
[X] Tactic: 0x00000000000003ea Time: 0.00940918
[X] Tactic: 0x0000000000000000 Time: 0.00580892
[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00741317 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00580175
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00359318
[X] Tactic: 0x00000000000003ea Time: 0.00802718
[X] Tactic: 0x0000000000000000 Time: 0.00360763
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00858933 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00359318
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00973135
[X] Tactic: 0x00000000000003ea Time: 0.00714599
[X] Tactic: 0x0000000000000000 Time: 0.00438793
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00719529 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00438793
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0211153
[X] Tactic: 0x00000000000003ea Time: 0.00706955
[X] Tactic: 0x0000000000000000 Time: 0.0210833
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00610671 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00706955
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0283644
[X] Tactic: 0x00000000000003ea Time: 0.00790077
[X] Tactic: 0x0000000000000000 Time: 0.0284427
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00645054 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00790077
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0131179
[X] Tactic: 0x00000000000003ea Time: 0.00563609
[X] Tactic: 0x0000000000000000 Time: 0.0131434
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00672308 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563609
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00636317
[X] Tactic: 0x00000000000003ea Time: 0.0217393
[X] Tactic: 0x0000000000000000 Time: 0.00351506
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00852562 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00351506
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0103554
[X] Tactic: 0x00000000000003ea Time: 0.00552533
[X] Tactic: 0x0000000000000000 Time: 0.0103754
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00685648 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552533
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0275143
[X] Tactic: 0x00000000000003ea Time: 0.00783702
[X] Tactic: 0x0000000000000000 Time: 0.00384343
[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00721744 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00384343
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0168112
[X] Tactic: 0x00000000000003ea Time: 0.00819226
[X] Tactic: 0x0000000000000000 Time: 0.00624889
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00691755 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00624889
[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0533135
[X] Tactic: 0x00000000000003ea Time: 0.00984838
[X] Tactic: 0x0000000000000000 Time: 0.0535832
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00623607 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00984838
[X] *************** Autotuning Reformat: Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0483048
[X] Tactic: 0x00000000000003ea Time: 0.0110754
[X] Tactic: 0x0000000000000000 Time: 0.010923
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00650393 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.010923
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.045304
[X] Tactic: 0x00000000000003ea Time: 0.0106243
[X] Tactic: 0x0000000000000000 Time: 0.0453093
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0062525 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0106243
[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0816661
[X] Tactic: 0x00000000000003ea Time: 0.0118319
[X] Tactic: 0x0000000000000000 Time: 0.0815125
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00646084 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0118319
[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00959878
[X] Tactic: 0x00000000000003ea Time: 0.0113895
[X] Tactic: 0x0000000000000000 Time: 0.00434909
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00811408 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00434909
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00984838 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.010923 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0118319 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00286423 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561867 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00533655 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561867 ms
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00533655 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00572113 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0131491
[X] Tactic: 0x00000000000003ea Time: 0.00915171
[X] Tactic: 0x0000000000000000 Time: 0.0131442
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.0065335 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00915171
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00642769
[X] Tactic: 0x00000000000003ea Time: 0.00839653
[X] Tactic: 0x0000000000000000 Time: 0.00339836
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00824513 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00339836
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,1:16,1280,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0103683
[X] Tactic: 0x00000000000003ea Time: 0.00762473
[X] Tactic: 0x0000000000000000 Time: 0.0104463
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00650894 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00762473
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(409600,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00973287
[X] Tactic: 0x00000000000003ea Time: 0.00761285
[X] Tactic: 0x0000000000000000 Time: 0.00440519
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00724202 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00440519
[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0283467
[X] Tactic: 0x00000000000003ea Time: 0.0074093
[X] Tactic: 0x0000000000000000 Time: 0.0283502
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00586551 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0074093
[X] *************** Autotuning Reformat: Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0267159
[X] Tactic: 0x00000000000003ea Time: 0.00787944
[X] Tactic: 0x0000000000000000 Time: 0.00444042
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00697707 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00444042
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(409600,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0226866
[X] Tactic: 0x00000000000003ea Time: 0.00769794
[X] Tactic: 0x0000000000000000 Time: 0.0227392
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00635181 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00769794
[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0418173
[X] Tactic: 0x00000000000003ea Time: 0.00804063
[X] Tactic: 0x0000000000000000 Time: 0.0417733
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00575627 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00804063
[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00633922
[X] Tactic: 0x00000000000003ea Time: 0.00767127
[X] Tactic: 0x0000000000000000 Time: 0.00341573
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00794783 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00341573
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0074093 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00444042 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00804063 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00577931 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268954 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x0000000000000000, 0.00442863 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559698 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0026621 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00443705 ms
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00269307 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00299019
[X] Tactic: 0x00000000000003ea Time: 0.00604438
[X] Tactic: 0x0000000000000000 Time: 0.00298867
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00890761 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00298867
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00400813
[X] Tactic: 0x00000000000003ea Time: 0.00549491
[X] Tactic: 0x0000000000000000 Time: 0.00260043
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00783243 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00260043
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00718842
[X] Tactic: 0x00000000000003ea Time: 0.00548459
[X] Tactic: 0x0000000000000000 Time: 0.00718071
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00679508 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00548459
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00943822
[X] Tactic: 0x00000000000003ea Time: 0.00533384
[X] Tactic: 0x0000000000000000 Time: 0.00944089
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00666949 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00533384
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(409600,1,10240,256) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00764121
[X] Tactic: 0x00000000000003ea Time: 0.00524533
[X] Tactic: 0x0000000000000000 Time: 0.00761988
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00723108 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00524533
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(409600,1,10240,256) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00777997
[X] Tactic: 0x00000000000003ea Time: 0.00535721
[X] Tactic: 0x0000000000000000 Time: 0.00780701
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00671472 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00535721
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(409600,1,10240,256) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00414011
[X] Tactic: 0x00000000000003ea Time: 0.00539544
[X] Tactic: 0x0000000000000000 Time: 0.00413063
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00776015 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00413063
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(409600,1,10240,256) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.013501
[X] Tactic: 0x00000000000003ea Time: 0.00534654
[X] Tactic: 0x0000000000000000 Time: 0.013545
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00624363 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00534654
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(1,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00333718
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00285351 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00333718
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00399682
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00279738 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00399682
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00719501
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00239166 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00719501
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00944652
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00219146 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00944652
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,1:4,2560,64) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00768873
[X] Tactic: 0x00000000000003ea Time: 0.0103541
[X] Tactic: 0x0000000000000000 Time: 0.00768752
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00671647 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00768752
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1:4,2560,64) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00783132
[X] Tactic: 0x00000000000003ea Time: 0.010177
[X] Tactic: 0x0000000000000000 Time: 0.00786877
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00672213 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00783132
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,1:4,2560,64) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0041692
[X] Tactic: 0x00000000000003ea Time: 0.00540542
[X] Tactic: 0x0000000000000000 Time: 0.0041409
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00781253 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0041409
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1:4,2560,64) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0135398
[X] Tactic: 0x00000000000003ea Time: 0.00544155
[X] Tactic: 0x0000000000000000 Time: 0.0135586
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00645226 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544155
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00765091
[X] Tactic: 0x00000000000003ea Time: 0.00543088
[X] Tactic: 0x0000000000000000 Time: 0.00769867
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00668332 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00543088
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00776632
[X] Tactic: 0x00000000000003ea Time: 0.00535958
[X] Tactic: 0x0000000000000000 Time: 0.00775127
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00675786 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00535958
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00415381
[X] Tactic: 0x00000000000003ea Time: 0.00544224
[X] Tactic: 0x0000000000000000 Time: 0.0041571
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00777044 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00415381
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0130412
[X] Tactic: 0x00000000000003ea Time: 0.00538787
[X] Tactic: 0x0000000000000000 Time: 0.0130174
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00625497 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00538787
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(1:4,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00399251
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00261331 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00399251
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00413603
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00255695 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00413603
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0074259
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00205068 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0074259
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00963413
[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00192289 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00963413
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00341344 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559342 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0039409 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0056304 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00577508 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0028212 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559342 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0039409 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00577508 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00253867 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00544637 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00501668 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00544637 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00501668 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562507 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0057532
[X] Tactic: 0x00000000000003ea Time: 0.00547375
[X] Tactic: 0x0000000000000000 Time: 0.00576754
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.0071859 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00547375
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00361761
[X] Tactic: 0x00000000000003ea Time: 0.00554877
[X] Tactic: 0x0000000000000000 Time: 0.0026892
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00855159 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0026892
[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0044581
[X] Tactic: 0x00000000000003ea Time: 0.00557476
[X] Tactic: 0x0000000000000000 Time: 0.00446222
[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00754339 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0044581
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288993 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558436 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00300406 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562702 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055075 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268425 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558436 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00300406 ms
[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055075 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(204800,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00341954
[X] Tactic: 0x00000000000003ea Time: 0.00558951
[X] Tactic: 0x0000000000000000 Time: 0.00395846
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00781521 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00341954
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0037585
[X] Tactic: 0x00000000000003ea Time: 0.00543553
[X] Tactic: 0x0000000000000000 Time: 0.00228478
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00777802 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00228478
[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00309798
[X] Tactic: 0x00000000000003ea Time: 0.00548004
[X] Tactic: 0x0000000000000000 Time: 0.00383301
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.0079701 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00309798
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(204800,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00382218
[X] Tactic: 0x00000000000003ea Time: 0.0106417
[X] Tactic: 0x0000000000000000 Time: 0.00380473
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00686646 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00380473
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(51200,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00290031
[X] Tactic: 0x00000000000003ea Time: 0.00546588
[X] Tactic: 0x0000000000000000 Time: 0.00229618
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00846548 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00229618
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(12800,1:16,640,32) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00311121
[X] Tactic: 0x00000000000003ea Time: 0.00549491
[X] Tactic: 0x0000000000000000 Time: 0.00313231
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00849139 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00311121
[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(6400,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00467807
[X] Tactic: 0x00000000000003ea Time: 0.00552376
[X] Tactic: 0x0000000000000000 Time: 0.00254738
[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00775537 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00254738
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00243986
[X] Tactic: 0x00000000000003ea Time: 0.00523167
[X] Tactic: 0x0000000000000000 Time: 0.00243441
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00797491 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00243441
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00270856
[X] Tactic: 0x00000000000003ea Time: 0.00535467
[X] Tactic: 0x0000000000000000 Time: 0.00226494
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00860006 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00226494
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00380909
[X] Tactic: 0x00000000000003ea Time: 0.00539372
[X] Tactic: 0x0000000000000000 Time: 0.00383743
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00777349 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00380909
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00438095
[X] Tactic: 0x00000000000003ea Time: 0.00546676
[X] Tactic: 0x0000000000000000 Time: 0.00441502
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00752073 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00438095
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00374904
[X] Tactic: 0x00000000000003ea Time: 0.00541849
[X] Tactic: 0x0000000000000000 Time: 0.00371591
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00786359 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00371591
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00388626
[X] Tactic: 0x00000000000003ea Time: 0.00538512
[X] Tactic: 0x0000000000000000 Time: 0.00384613
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00779589 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00384613
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.002994
[X] Tactic: 0x00000000000003ea Time: 0.00537738
[X] Tactic: 0x0000000000000000 Time: 0.00298257
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00859138 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00298257
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00537634
[X] Tactic: 0x00000000000003ea Time: 0.00540009
[X] Tactic: 0x0000000000000000 Time: 0.00543622
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00715065 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00537634
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00242999
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00258931 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00242999
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00274168
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00334644 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00274168
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00389532
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00304287 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00389532
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00440491
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00266745 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00440491
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00382909
[X] Tactic: 0x00000000000003ea Time: 0.0052993
[X] Tactic: 0x0000000000000000 Time: 0.0038217
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00788185 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0038217
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00382534
[X] Tactic: 0x00000000000003ea Time: 0.00534823
[X] Tactic: 0x0000000000000000 Time: 0.00385778
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0079708 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00382534
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00300627
[X] Tactic: 0x00000000000003ea Time: 0.00540783
[X] Tactic: 0x0000000000000000 Time: 0.00299552
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00826737 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00299552
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00543828
[X] Tactic: 0x00000000000003ea Time: 0.00531573
[X] Tactic: 0x0000000000000000 Time: 0.00544585
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00726846 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00531573
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00380691
[X] Tactic: 0x00000000000003ea Time: 0.00545703
[X] Tactic: 0x0000000000000000 Time: 0.00377169
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00780477 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00377169
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00377157
[X] Tactic: 0x00000000000003ea Time: 0.00539303
[X] Tactic: 0x0000000000000000 Time: 0.00378667
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00795531 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00377157
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00299305
[X] Tactic: 0x00000000000003ea Time: 0.00538254
[X] Tactic: 0x0000000000000000 Time: 0.00300858
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00890336 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00299305
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00529118
[X] Tactic: 0x00000000000003ea Time: 0.0055026
[X] Tactic: 0x0000000000000000 Time: 0.00534112
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00723065 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00529118
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(204800,400,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00287963
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00313553 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00287963
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00289692
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0031158 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00289692
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00380485
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00265242 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00380485
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************
[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00434507
[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00249834 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00434507
[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1
[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00273618 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00556764 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299705 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00545807 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00547602 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00247032 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00556764 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299705 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00547602 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(1600,400:32,20,1) -> Int8(12800,400:4,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.0026997
[X] Tactic: 0x00000000000003ea Time: 0.00549333
[X] Tactic: 0x0000000000000000 Time: 0.00214072
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00890588 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00214072
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(51200,400,20,1) -> Float(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00354403
[X] Tactic: 0x00000000000003ea Time: 0.00534383
[X] Tactic: 0x0000000000000000 Time: 0.00355393
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00814314 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00354403
[X] *************** Autotuning Reformat: Float(1600,400:32,20,1) -> Float(51200,400,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00300493
[X] Tactic: 0x00000000000003ea Time: 0.00542176
[X] Tactic: 0x0000000000000000 Time: 0.003002
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00853479 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.003002
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(51200,400,20,1) -> Float(1600,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00354403 ms
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(1600,400:32,20,1) -> Float(51200,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.003002 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Int8(51200,400,20,1) -> Int8(1600,400:32,20,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.00383657
[X] Tactic: 0x00000000000003ea Time: 0.00556658
[X] Tactic: 0x0000000000000000 Time: 0.00385165
[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00785127 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00383657
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00230128 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] *************** Autotuning Reformat: Float(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************
[X] --------------- Timing Runner: Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])
[X] Tactic: 0x00000000000003e8 Time: 0.022104
[X] Tactic: 0x00000000000003ea Time: 0.0068062
[X] Tactic: 0x0000000000000000 Time: 0.0220887
[X] Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0059486 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0068062
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/decoder/input_proj.1/conv/Conv_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00539217 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs: 
[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/Conv_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00364997 ms
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] =============== Computing reformatting costs for available format set
[X] Formats and tactics selection completed in 24.9037 seconds.
[X] After reformat layers: 90 layers
[X] Total number of blocks in pre-optimized block assignment: 86
[V] Detected 2 inputs and 6 output network tensors.
[X] Layer: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Host Persistent: 4880 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: /model/backbone/MaxPool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Host Persistent: 4080 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Host Persistent: 5200 bytes Device Persistent: 38912 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Host Persistent: 4944 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Host Persistent: 4944 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_107.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 13107200 bytes
[X] Layer: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Host Persistent: 308 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Host Persistent: 308 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes
[X] Layer: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 54067200 bytes
[X] Skipped printing memory information for 21 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.
[V] Total Host Persistent Memory: 307440 bytes
[V] Total Device Persistent Memory: 38912 bytes
[V] Max Scratch Memory: 54067200 bytes
[V] [BlockAssignment] Started assigning block shifts. This will take 87 steps to complete.
[X] STILL ALIVE: Started step 51 of 87
[V] [BlockAssignment] Algorithm ShiftNTopDown took 1.40179ms to assign 6 blocks to 87 nodes requiring 63692800 bytes.
[X] Total number of blocks in optimized block assignment: 6
[V] Total Activation Memory: 63692800 bytes
[V] Total Weights Memory: 23939972 bytes
[X] Finalize: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Set kernel index: 0
[X] Finalize: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Set kernel index: 1
[X] Finalize: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Set kernel index: 2
[X] Finalize: /model/backbone/MaxPool Set kernel index: 3
[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Set kernel index: 2
[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Set kernel index: 4
[X] Finalize: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Set kernel index: 5
[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Set kernel index: 2
[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Set kernel index: 4
[X] Finalize: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Set kernel index: 6
[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Set kernel index: 7
[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Set kernel index: 8
[X] Finalize: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Set kernel index: 9
[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Set kernel index: 10
[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Set kernel index: 8
[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Set kernel index: 11
[X] Finalize: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Set kernel index: 12
[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Set kernel index: 13
[X] Finalize: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Set kernel index: 14
[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Set kernel index: 15
[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Set kernel index: 13
[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Set kernel index: 16
[X] Finalize: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Set kernel index: 12
[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Set kernel index: 17
[X] Finalize: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Set kernel index: 18
[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Set kernel index: 19
[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Set kernel index: 17
[X] Finalize: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Set kernel index: 20
[X] Finalize: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Set kernel index: 21
[X] Finalize: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Set kernel index: 22
[X] Finalize: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Set kernel index: 20
[X] Finalize: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Set kernel index: 23
[X] Finalize: /model/encoder/Resize Set kernel index: 24
[X] Finalize: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Set kernel index: 25
[X] Finalize: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Set kernel index: 26
[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 27
[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 27
[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 28
[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Set kernel index: 29
[X] Finalize: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Set kernel index: 26
[X] Finalize: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv Set kernel index: 5
[X] Finalize: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Set kernel index: 30
[X] Finalize: /model/encoder/Resize_1 Set kernel index: 24
[X] Finalize: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Set kernel index: 31
[X] Finalize: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Set kernel index: 32
[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 33
[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 33
[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 34
[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Set kernel index: 35
[X] Finalize: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Set kernel index: 36
[X] Finalize: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Set kernel index: 5
[X] Finalize: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Set kernel index: 27
[X] Finalize: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Set kernel index: 25
[X] Finalize: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Set kernel index: 26
[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 27
[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 27
[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 28
[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Set kernel index: 29
[X] Finalize: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Set kernel index: 26
[X] Finalize: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Set kernel index: 5
[X] Finalize: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Set kernel index: 37
[X] Finalize: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Set kernel index: 14
[X] Finalize: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Set kernel index: 38
[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 37
[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 37
[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 39
[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Set kernel index: 29
[X] Finalize: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Set kernel index: 38
[X] Finalize: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Set kernel index: 20
[X] Total number of generated kernels selected for the engine: 40
[X] Kernel: 0 CASK_STATIC
[X] Kernel: 1 CASK_STATIC
[X] Kernel: 2 CASK_STATIC
[X] Kernel: 3 CASK_STATIC
[X] Kernel: 4 CASK_STATIC
[X] Kernel: 5 CASK_STATIC
[X] Kernel: 6 CASK_STATIC
[X] Kernel: 7 CASK_STATIC
[X] Kernel: 8 CASK_STATIC
[X] Kernel: 9 CASK_STATIC
[X] Kernel: 10 CASK_STATIC
[X] Kernel: 11 CASK_STATIC
[X] Kernel: 12 CASK_STATIC
[X] Kernel: 13 CASK_STATIC
[X] Kernel: 14 CASK_STATIC
[X] Kernel: 15 CASK_STATIC
[X] Kernel: 16 CASK_STATIC
[X] Kernel: 17 CASK_STATIC
[X] Kernel: 18 CASK_STATIC
[X] Kernel: 19 CASK_STATIC
[X] Kernel: 20 CASK_STATIC
[X] Kernel: 21 CASK_STATIC
[X] Kernel: 22 CASK_STATIC
[X] Kernel: 23 TRT_SERIALIZABLE:generatedNativePointwise
[X] Kernel: 24 TRT_SERIALIZABLE:ResizeVectorizedC4x4NearestKernel
[X] Kernel: 25 CASK_STATIC
[X] Kernel: 26 CASK_STATIC
[X] Kernel: 27 CASK_STATIC
[X] Kernel: 28 CASK_STATIC
[X] Kernel: 29 TRT_SERIALIZABLE:generatedNativePointwise
[X] Kernel: 30 TRT_SERIALIZABLE:generatedNativePointwise
[X] Kernel: 31 CASK_STATIC
[X] Kernel: 32 CASK_STATIC
[X] Kernel: 33 CASK_STATIC
[X] Kernel: 34 CASK_STATIC
[X] Kernel: 35 TRT_SERIALIZABLE:generatedNativePointwise
[X] Kernel: 36 CASK_STATIC
[X] Kernel: 37 CASK_STATIC
[X] Kernel: 38 CASK_STATIC
[X] Kernel: 39 CASK_STATIC
[V] Compiler backend is used during engine execution.
[X] Disabling unused tactic source: JIT_CONVOLUTIONS
[V] Engine generation completed in 25.4827 seconds.
[X] Layers:
    Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: images, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 864}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize16x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8, TacticValue: 0x3d988d07a78b0918, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_1/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 9216}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3, TacticValue: 0x13463e9bf9ae0d73, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_2/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 18432}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_3/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear]
    Name: /model/backbone/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/MaxPool]
    Name: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0x23b890da05937b9e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 4096}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0x23b890da05937b9e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE, TacticValue: 0xb4d3d3158ab4fbc4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/pool/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 73728}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x705baf38e41eee0b, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0xa8b56a226b057463, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 8192}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1, TacticValue: 0xff6944b17d5b2e32, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x214f03e23f252333, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4, TacticValue: 0xa8b56a226b057463, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 294912}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32, TacticValue: 0x495da4d52da4de96, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/pool/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 1179648}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32, TacticValue: 0x322f337abc345152, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/pool/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x65fbe45b4cb1d8a5, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x1d53511430a5d47e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/act/Relu]
    Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye9020_0_myl40_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddResTra_myl40_1, LayerType: kgen, Inputs: [ { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_MulAddResTra_0x862813689358e08ec79eab32f31fafdf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/encoder/Reshape][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1]
    Name: __mye8937_myl40_2, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_TraAdd_myl40_3, LayerType: kgen, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Constant_output_0_constantFloat, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_TraAdd_0x5a9388c92c5b2a167638420a28fa3cf0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Transpose][ONNX Layer: /model/encoder/encoder.0/layers.0/Add]
    Name: __mye8939_myl40_4, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_2_myl40_5, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8387_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8277/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8278/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8626_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2]
    Name: __mye8941_myl40_6, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_1+/model/encoder/encoder_0/layers_0/self_attn/MatMul_myl40_7, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8769_dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye8319/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8320/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8774_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye8684, Dimensions: [2,400,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add]
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_3_myl40_8, LayerType: gemm, Inputs: [ { Name: __mye8684, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8684, Dimensions: [8,32,400], Format/Datatype: Float }, { Name: __mye8642, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye8333/model/encoder/encoder_0/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x128x16_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl40_9, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x486901888507314d28178a529899ff30, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax]
    Name: __mye8943_myl40_10, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_myl40_11, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8343/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8344/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl40_12, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0x053154cc4b930530fcf23b0caf04c63a, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3]
    Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_myl40_13, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8849dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8357/model/encoder/encoder_0/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8358/model/encoder/encoder_0/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_122_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl40_14, LayerType: kgen, Inputs: [ { Name: __mye8585_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8575_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye9016_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x81c6f38dc18b20647aef42cb9b16a94b, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/encoder/encoder_0/layers_0/linear1/MatMul_myl40_15, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Int8 }, { Name: __mye8854dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye8646_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye8653zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear1_bias _ ONNXTRT_Broadcast_137_constantFloat, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_gelu_erf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Div][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Erf][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl40_16, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }, { Name: __mye8859dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye8657_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8664zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear2_bias _ ONNXTRT_Broadcast_151_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/Add_2]
    Name: __myl_ResMeaSubMulMea_myl40_17, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMea_0xade3a566ff3432c2f2753f66a7f593a6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization]
    Name: __myl_AddSqrDivMulMulAddResTra_myl40_18, LayerType: kgen, Inputs: [ { Name: __mye8539_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8549_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Reshape_1_output_0, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_AddSqrDivMulMulAddResTra_0x0caebe133d43683f5670c896620d9227, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization][ONNX Layer: /model/encoder/Transpose_1]
    [ONNX Layer: /model/encoder/Reshape_1]
    Name: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x5e4918ccf433630e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x483ad1560c6e5e27, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Reshape_1_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.0/act/Mul]
    Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    Name: /model/encoder/Resize, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize]
    Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_2]
    Name: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xb936321f82fd390c, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.1/act/Mul_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000009, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Mul]
    Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.1/act/Mul_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    Name: /model/encoder/Resize_1, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize_1]
    Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_3]
    Name: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x7720f198395e7d3d, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xbd976ef514eaa406, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xbd976ef514eaa406, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xad886d4d69834922, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000019, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x65a38dbc9e991257, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.1/act/Mul_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xb936321f82fd390c, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/Add]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xd14bd6d95fefd45e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/Add]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye157975_0_myl89_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^signal^1_myl89_1, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^wait^1_myl89_2, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl89_3, LayerType: kgen, Inputs: [ { Name: __mye155438_dconst, Dimensions: [1,1,6400], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xb7911a963641d99b9b7644b75b6b02a0, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulMinMaxRouCasResTra_myl89_4, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: __mye157875_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x53ec280dcdcbc7be42089db5a99e26ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl89_5, LayerType: kgen, Inputs: [ { Name: __mye155461_dconst, Dimensions: [1,1,1600], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xc7826108fa2ff5e34bf8bfa07dbc52f7, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/input_proj.1/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __myl_MulMinMaxRouCasResTra_myl89_6, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: __mye157875_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x8592f20b4eb6c9ee9a9e56f44ec5871e, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __mye157299_myl89_7, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __mye157301_myl89_8, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_myl89_9, LayerType: kgen, Inputs: [ { Name: __mye157875_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __mye155484_dconst, Dimensions: [1,1,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __mye153891_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_0x14d97ab92d57b85a1bd3815e99f6e152, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Concat_3][ONNX Layer: /model/decoder/Reshape_2]
    [ONNX Layer: /model/decoder/Transpose_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear]
    Name: __mye157303_myl89_10, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157305_myl89_11, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_1/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_0/cross_attn/value_proj/MatMul_myl89_12, LayerType: gemm, Inputs: [ { Name: __mye153891_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156133dconst, Dimensions: [3,256,256], Format/Datatype: Int8 }, { Name: __mye153915_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye153936_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye154934_dconst, Dimensions: [3,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153891, Dimensions: [3,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add]
    Name: /model/decoder/enc_output/proj/MatMul_myl89_13, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156138dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153194_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153201zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_proj_bias _ ONNXTRT_Broadcast_281_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/proj/MatMul][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/Add]
    Name: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl89_14, LayerType: kgen, Inputs: [ { Name: model_decoder_enc_output_norm_weight _ ONNXTRT_Broadcast_285_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_norm_bias _ ONNXTRT_Broadcast_287_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: __mye157885_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0xf1c80ff651c1b506b1815818d6281ad3, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/norm/LayerNormalization][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear]
    Name: __mye157307_myl89_15, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157309_myl89_16, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_score_head/MatMul_myl89_17, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156148dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153232_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153239zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_enc_score_head_bias _ ONNXTRT_Broadcast_295_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/enc_score_head/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/Add]
    Name: __myl_Max_myl89_18, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400,1], Format/Datatype: Float }], TacticName: __myl_Max_0x4330a02939b906fc5f8c1bd769456467, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/ReduceMax]
    Name: __myl_Top_myl89_19, LayerType: kgen, Inputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/TopK_output_0'.1, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x7e62297dffa2e596ee60049838a70f81, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/TopK]
    Name: __mye157311_myl89_20, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_bbox_head/layers_0/MatMul_myl89_21, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157373_xformed___mye156143dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157381_xformed___mye153216_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153212zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157377_xformed___mye153225_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.0/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/Add]
    Name: /model/decoder/enc_bbox_head/layers_1/MatMul_myl89_22, LayerType: gemm, Inputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157385_xformed___mye156153dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157393_xformed___mye153254_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153250zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157389_xformed___mye153263_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.1/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act_1/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/Add]
    Name: __myl_FcAdd_myl89_23, LayerType: fusion, Inputs: [ { Name: model_decoder_anchors_constantFloat, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156163dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153270_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153277zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_enc_bbox_head_layers_2_bias _ ONNXTRT_Broadcast_317_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.2/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/Add][ONNX Layer: /model/decoder/Add]
    Name: __mye157313_myl89_24, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_myl89_25, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __mye157889_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], Outputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,4], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], TacticName: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_0xea994e8a02766a6b87cc77a0ab1bb663, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/Unsqueeze][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Sigmoid][ONNX Layer: /model/decoder/GatherElements]
    Name: __myl_MovCon_myl89_26, LayerType: kgen, Inputs: [ { Name: __mye156470, Dimensions: [1,300,12], Format/Datatype: Int8 }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,4], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MovCon_0x9482c2d60923b5d68d1030431d0b6d2e, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_0/MatMul_myl89_27, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156484_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153292_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153288zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153301_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1/MatMul_myl89_28, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156173dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153308_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153315zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye149975_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/Add]
    Name: __myl_RepGatResAdd_myl89_29, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], Outputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_RepGatResAdd_0x3585782c9d9cf8f0d2b18744e46affde, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/GatherElements_1][ONNX Layer: /model/decoder/decoder/layers.0/Add]
    Name: __mye157315_myl89_30, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157317_myl89_31, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_2_myl89_32, LayerType: gemm, Inputs: [ { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156158dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149287/model/decoder/decoder/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149288/model/decoder/decoder/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye153089_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_2]
    Name: __mye157319_myl89_33, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_1+/model/decoder/decoder/layers_0/self_attn/MatMul_myl89_34, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156178dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149341/model/decoder/decoder/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149342/model/decoder/decoder/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155184_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153876, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add]
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_3_myl89_35, LayerType: gemm, Inputs: [ { Name: __mye153876, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153876, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153149, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149376/model/decoder/decoder/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x128x32_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl89_36, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Softmax]
    Name: __mye157321_myl89_37, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_myl89_38, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149386/model/decoder/decoder/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149387/model/decoder/decoder/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl89_39, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_0/self_attn/Gemm_myl89_40, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye149991_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149400/model/decoder/decoder/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149401/model/decoder/decoder/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_357_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl89_41, LayerType: kgen, Inputs: [ { Name: __mye152985_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152975_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157893_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x91b2c7046943674462a660380f1917c4, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/Add_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157323_myl89_42, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157325_myl89_43, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/cross_attn/attention_weights/MatMul_myl89_44, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156183dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153319_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153326zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_389_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add]
    Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/MatMul_myl89_45, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156188dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153330_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153337zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_381_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add]
    Name: __myl_MaxSubExpSum_myl89_46, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_44, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_43, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax]
    Name: __mye157327_myl89_47, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl89_48, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18222, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18237, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18252, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18267, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18446, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18461, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18476, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18491, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18670, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18685, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18700, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18715, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150465_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150475_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150485_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150579, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150575, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye149996_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_47, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_46, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_45, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0xf96d9f98493dda1d394fe8f1b3a4a64a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8]
    Name: __mye157329_myl89_49, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl89_50, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_44, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157907_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_46, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_45, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_47, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_43, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150954_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl89_51, LayerType: kgen, Inputs: [ { Name: __mye150954_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_49, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl89_52, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_49, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155354_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153341_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153348zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_582_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_50, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl89_53, LayerType: kgen, Inputs: [ { Name: __mye152940_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152930_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157911_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_50, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_52, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_0/linear1/MatMul_myl89_54, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157397_xformed___mye156193dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153363_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153359zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153372_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_53, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl89_55, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_52, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_53, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156198dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153379_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153386zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_linear2_bias _ ONNXTRT_Broadcast_604_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_54, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl89_56, LayerType: kgen, Inputs: [ { Name: __mye152904_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152890_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157915_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_54, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157331_myl89_57, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157333_myl89_58, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_2_myl89_59, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156208dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149485/model/decoder/decoder/layers_1/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149486/model/decoder/decoder/layers_1/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152875_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_2]
    Name: __mye157335_myl89_60, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/MatMul_myl89_61, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157401_xformed___mye156203dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153401_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153397zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153410_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_58, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_1/MatMul_myl89_62, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_58, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157405_xformed___mye156213dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153428_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153424zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153437_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_59, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/MatMul_myl89_63, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_59, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156218dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153444_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153451zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_0_layers_2_bias _ ONNXTRT_Broadcast_634_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_60, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add]
    Name: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_myl89_64, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_60, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156491, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye157889_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_0xefac8e563c6580f9cd110df4750663ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log][ONNX Layer: /model/decoder/decoder/Sigmoid_1][ONNX Layer: /model/decoder/decoder/Add][ONNX Layer: /model/decoder/decoder/Div][ONNX Layer: /model/decoder/decoder/Sub][ONNX Layer: /model/decoder/decoder/Clip]
    Name: /model/decoder/decoder/query_pos_head/layers_0_1/MatMul_myl89_65, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156507_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153466_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153462zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153475_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_63, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_1/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_1/MatMul_myl89_66, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_63, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156228dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153482_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153489zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150074_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add]
    Name: __myl_Add_myl89_67, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_1+/model/decoder/decoder/layers_1/self_attn/MatMul_myl89_68, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156233dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149550/model/decoder/decoder/layers_1/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149551/model/decoder/decoder/layers_1/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155194_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153853, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_3_myl89_69, LayerType: gemm, Inputs: [ { Name: __mye153853, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153853, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153153, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149585/model/decoder/decoder/layers_1/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_67, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x128x32_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl89_70, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_67, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_67, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_68, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Softmax]
    Name: __mye157337_myl89_71, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_myl89_72, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_68, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149595/model/decoder/decoder/layers_1/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149596/model/decoder/decoder/layers_1/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_69, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4]
    Name: __myl_Tra_myl89_73, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_69, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_1/self_attn/Gemm_myl89_74, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150090_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149609/model/decoder/decoder/layers_1/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149610/model/decoder/decoder/layers_1/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_self_attn_out_proj_bias _ ONNXTRT_Broadcast_679_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl89_75, LayerType: kgen, Inputs: [ { Name: __mye152825_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152815_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157922_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_73, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157339_myl89_76, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157341_myl89_77, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_1/cross_attn/attention_weights/MatMul_myl89_78, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156238dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153493_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153500zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_709_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add]
    Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/MatMul_myl89_79, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156243dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153504_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153511zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_701_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add]
    Name: __myl_MaxSubExpSum_myl89_80, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_77, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_76, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax]
    Name: __mye157343_myl89_81, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl89_82, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18921, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18936, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18951, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18966, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19145, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19160, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19175, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19190, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19369, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19384, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19399, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19414, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150495_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150505_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150515_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150607, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150603, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye150095_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_80, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_79, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_78, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0x11b440b78a8fa74f33eb0bb614ceee80, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8]
    Name: __mye157345_myl89_83, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl89_84, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_77, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157935_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_79, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_78, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_80, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_76, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150960_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl89_85, LayerType: kgen, Inputs: [ { Name: __mye150960_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_82, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl89_86, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_73, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_82, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155300_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153515_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153522zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_904_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_83, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl89_87, LayerType: kgen, Inputs: [ { Name: __mye152780_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152770_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157939_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_83, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_85, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_1/linear1/MatMul_myl89_88, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157409_xformed___mye156248dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153537_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153533zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153546_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_86, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.1/linear1/Add]
    Name: __myl_FcAdd_myl89_89, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_85, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_86, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156253dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153553_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153560zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_linear2_bias _ ONNXTRT_Broadcast_926_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_87, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl89_90, LayerType: kgen, Inputs: [ { Name: __mye152744_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152730_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157943_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_87, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157347_myl89_91, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157349_myl89_92, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_2_myl89_93, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156263dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149694/model/decoder/decoder/layers_2/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149695/model/decoder/decoder/layers_2/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152715_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_2]
    Name: __mye157351_myl89_94, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/MatMul_myl89_95, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157413_xformed___mye156258dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153575_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153571zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153584_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_91, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_1/MatMul_myl89_96, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_91, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157417_xformed___mye156268dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153602_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153598zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153611_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_92, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/MatMul_myl89_97, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_92, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156273dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153618_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153625zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_1_layers_2_bias _ ONNXTRT_Broadcast_956_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_93, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add]
    Name: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_myl89_98, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_93, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156514, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye157889_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_95, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_94, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_0xa06819df43d11e9f71ec4d6314dfc9b2, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log_1][ONNX Layer: /model/decoder/decoder/Add_1][ONNX Layer: /model/decoder/decoder/Sigmoid_2][ONNX Layer: /model/decoder/decoder/Div_1][ONNX Layer: /model/decoder/decoder/Sub_1][ONNX Layer: /model/decoder/decoder/Clip_3]
    Name: /model/decoder/decoder/query_pos_head/layers_0_2/MatMul_myl89_99, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_94, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156530_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153640_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153636zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153649_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_96, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_2/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_2/MatMul_myl89_100, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_96, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156283dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153656_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153663zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150169_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add]
    Name: __myl_Add_myl89_101, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_1+/model/decoder/decoder/layers_2/self_attn/MatMul_myl89_102, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156288dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149759/model/decoder/decoder/layers_2/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149760/model/decoder/decoder/layers_2/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155204_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153830, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_3_myl89_103, LayerType: gemm, Inputs: [ { Name: __mye153830, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153830, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153157, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149794/model/decoder/decoder/layers_2/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_100, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x128x32_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl89_104, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_100, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_100, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_101, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Softmax]
    Name: __mye157353_myl89_105, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_myl89_106, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_101, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149804/model/decoder/decoder/layers_2/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149805/model/decoder/decoder/layers_2/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_102, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4]
    Name: __myl_Tra_myl89_107, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_102, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_2/self_attn/Gemm_myl89_108, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150185_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149818/model/decoder/decoder/layers_2/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149819/model/decoder/decoder/layers_2/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_self_attn_out_proj_bias _ ONNXTRT_Broadcast_1001_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl89_109, LayerType: kgen, Inputs: [ { Name: __mye152665_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152655_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157950_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_106, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157355_myl89_110, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157357_myl89_111, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/cross_attn/attention_weights/MatMul_myl89_112, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156293dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153667_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153674zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_1031_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add]
    Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/MatMul_myl89_113, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156298dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153678_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153685zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_1023_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add]
    Name: __myl_MaxSubExpSum_myl89_114, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/cross_attn/attention_weights/Add_output_0'.1, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_110, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_109, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax]
    Name: __mye157359_myl89_115, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl89_116, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_95, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19620, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19635, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19650, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19665, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19844, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19859, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19874, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19889, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20068, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20083, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20098, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20113, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150525_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150535_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150545_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157897_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150635, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150631, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye150190_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_113, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_112, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_111, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0x11b440b78a8fa74f33eb0bb614ceee80, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8]
    Name: __mye157361_myl89_117, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl89_118, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_110, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157963_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_112, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_111, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_113, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_109, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150966_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl89_119, LayerType: kgen, Inputs: [ { Name: __mye150966_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_115, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl89_120, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_106, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_115, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155246_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153689_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153696zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_1226_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_116, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl89_121, LayerType: kgen, Inputs: [ { Name: __mye152620_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152610_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157967_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_116, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_118, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_2/linear1/MatMul_myl89_122, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157421_xformed___mye156303dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153711_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153707zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153720_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_119, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.2/linear1/Add]
    Name: __myl_FcAdd_myl89_123, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_118, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_119, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156308dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153727_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153734zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_linear2_bias _ ONNXTRT_Broadcast_1248_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_120, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl89_124, LayerType: kgen, Inputs: [ { Name: __mye152584_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157971_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152578_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_120, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x3f53c92c8e85fb99f9934c06da28da1c, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157363_myl89_125, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157365_myl89_126, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_score_head_2/MatMul_myl89_127, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156313dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153738_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153745zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_dec_score_head_2_bias _ ONNXTRT_Broadcast_1296_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,300,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/dec_score_head.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/Add]
    Name: __myl_GatResNegExpAddDivRes_myl89_128, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,1,300,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_123, Dimensions: [1,24000], Format/Datatype: Float }], TacticName: __myl_GatResNegExpAddDivRes_0x1d563258c32f843400fb4233ccab3fa6, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/Sigmoid][ONNX Layer: /postprocessor/Flatten][ONNX Layer: /model/decoder/Gather_8]
    Name: __myl_Top_myl89_129, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_123, Dimensions: [1,24000], Format/Datatype: Float }], Outputs: [ { Name: scores, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_125, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x1c85ccd1fad109f046189f0d3e8dff44, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/TopK]
    Name: __mye157367_myl89_130, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/MatMul_myl89_131, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157425_xformed___mye156318dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153760_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153756zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153769_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_126, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_1/MatMul_myl89_132, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_126, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157429_xformed___mye156323dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153787_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153783zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153796_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_127, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/MatMul_myl89_133, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_127, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156328dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153803_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153810zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_2_layers_2_bias _ ONNXTRT_Broadcast_1278_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_128, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add]
    Name: __mye157369_myl89_134, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_myl89_135, LayerType: kgen, Inputs: [ { Name: orig_target_sizes, Dimensions: [1,2], Format/Datatype: Int64 }, { Name: __myln_k_arg__bb1_125, Dimensions: [1,300], Format/Datatype: Int32 }, { Name: __mye150647, Dimensions: [1,1], Format/Datatype: Int64 }, { Name: __mye150651, Dimensions: [1,1], Format/Datatype: Float }, { Name: __mye150655, Dimensions: [1,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_128, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_95, Dimensions: [1,300,4], Format/Datatype: Float }], Outputs: [ { Name: labels, Dimensions: [1,300], Format/Datatype: Int64 }, { Name: boxes, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_0x046287ea34a14bdbbd780dcf069cdb4a, StreamId: 0, Metadata: [ONNX Layer: Cast_3039][ONNX Layer: /model/decoder/decoder/Clip_6][ONNX Layer: /model/decoder/decoder/Sub_2][ONNX Layer: /model/decoder/decoder/Div_2][ONNX Layer: /model/decoder/decoder/Sigmoid_3][ONNX Layer: /model/decoder/Gather_9][ONNX Layer: /model/decoder/decoder/Unsqueeze_3][ONNX Layer: /model/decoder/decoder/Add_2][ONNX Layer: /model/decoder/decoder/Log_2][ONNX Layer: /postprocessor/Split][ONNX Layer: /postprocessor/Squeeze_1][ONNX Layer: /postprocessor/Squeeze_2][ONNX Layer: /postprocessor/Mul][ONNX Layer: /postprocessor/Add][ONNX Layer: /postprocessor/Unsqueeze_2][ONNX Layer: /postprocessor/Sub][ONNX Layer: /postprocessor/Unsqueeze][ONNX Layer: /postprocessor/Concat][ONNX Layer: /postprocessor/Mul_2][ONNX Layer: /postprocessor/GatherElements][ONNX Layer: /postprocessor/Unsqueeze_5][ONNX Layer: /postprocessor/Unsqueeze_3][ONNX Layer: /postprocessor/Add_1][ONNX Layer: /postprocessor/Squeeze][ONNX Layer: /postprocessor/Unsqueeze_1][ONNX Layer: /postprocessor/Sub_1][ONNX Layer: /postprocessor/Mul_1][ONNX Layer: /postprocessor/Squeeze_3][ONNX Layer: /postprocessor/Mul_3][ONNX Layer: /postprocessor/Sub_2][ONNX Layer: /postprocessor/Div][ONNX Layer: /postprocessor/Unsqueeze_4][ONNX Layer: /postprocessor/Tile]
    
    Bindings:
    images
    orig_target_sizes
    /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
    /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
    /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
    labels
    boxes
    scores
[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 301 MiB
[X] Adding 1 engine(s) to plan file.
[X] Adding 1 engine weights(s) to plan file.
[I] Finished engine building in 25.623 seconds
[V] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.
[X] Plugin creator already registered - ::ROIAlign_TRT version 2
[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1
[X] Plugin creator already registered - ::BatchedNMS_TRT version 1
[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1
[X] Plugin creator already registered - ::Clip_TRT version 1
[X] Plugin creator already registered - ::CoordConvAC version 1
[X] Plugin creator already registered - ::CropAndResizeDynamic version 1
[X] Plugin creator already registered - ::CropAndResize version 1
[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1
[X] Plugin creator already registered - ::DetectionLayer_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1
[X] Plugin creator already registered - ::EfficientNMS_TRT version 1
[X] Plugin creator already registered - ::FlattenConcat_TRT version 1
[X] Plugin creator already registered - ::GenerateDetection_TRT version 1
[X] Plugin creator already registered - ::GridAnchor_TRT version 1
[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2
[X] Plugin creator already registered - ::InstanceNormalization_TRT version 3
[X] Plugin creator already registered - ::LReLU_TRT version 1
[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1
[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1
[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1
[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1
[X] Plugin creator already registered - ::NMSDynamic_TRT version 1
[X] Plugin creator already registered - ::NMS_TRT version 1
[X] Plugin creator already registered - ::Normalize_TRT version 1
[X] Plugin creator already registered - ::PillarScatterPlugin version 1
[X] Plugin creator already registered - ::PriorBox_TRT version 1
[X] Plugin creator already registered - ::ProposalDynamic version 1
[X] Plugin creator already registered - ::ProposalLayer_TRT version 1
[X] Plugin creator already registered - ::Proposal version 1
[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1
[X] Plugin creator already registered - ::Region_TRT version 1
[X] Plugin creator already registered - ::Reorg_TRT version 2
[X] Plugin creator already registered - ::Reorg_TRT version 1
[X] Plugin creator already registered - ::ResizeNearest_TRT version 1
[X] Plugin creator already registered - ::ROIAlign_TRT version 1
[X] Plugin creator already registered - ::RPROI_TRT version 1
[X] Plugin creator already registered - ::ScatterElements version 1
[X] Plugin creator already registered - ::ScatterElements version 2
[X] Plugin creator already registered - ::ScatterND version 1
[X] Plugin creator already registered - ::SpecialSlice_TRT version 1
[X] Plugin creator already registered - ::Split version 1
[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1
[V] Loaded engine size: 29 MiB
[X] Deserialization required 8154 microseconds.
[X] Adding 1 engine(s) to plan file.
[X] Adding 1 engine weights(s) to plan file.
[I] Saving engine to q_config_3break_fusion-output_modified.engine
[V] [MS] Running engine with multi stream info
[V] [MS] Number of aux streams is 1
[V] [MS] Number of total worker streams is 2
[V] [MS] The main stream provided by execute/enqueue calls is the first worker stream
[X] Total per-runner device persistent memory is 38912
[X] Total per-runner host persistent memory is 307440
[X] Allocated device scratch memory of size 63692800
[X] - Runner scratch: 63692800 bytes
[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +61, now: CPU 0, GPU 83 (MiB)
[X] CUDA lazy loading is enabled.
[V] Loading inputs from data loader
[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. 
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] trt-runner-N8-07/03/25-18:26:23    
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[X] trt-runner-N8-07/03/25-18:26:23     | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}
[V] trt-runner-N8-07/03/25-18:26:23     | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 to: tensor([  0,   0,   0,  ..., 199,  12, 191], device='cuda:0',
           dtype=torch.uint8)
[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 to: tensor([  0,   0,   0,  ...,  75, 240, 191], device='cuda:0',
           dtype=torch.uint8)
[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 to: tensor([  0,   0,   0,  ...,  61,   0, 191], device='cuda:0',
           dtype=torch.uint8)
[X] Reallocated output tensor: labels to: tensor([  0,   0,   0,  ..., 229,  29,  62], device='cuda:0',
           dtype=torch.uint8)
[X] Reallocated output tensor: boxes to: tensor([  0,   0,   0,  ..., 165,   1,  64], device='cuda:0',
           dtype=torch.uint8)
[X] Reallocated output tensor: scores to: tensor([  0,   0,   0,  ...,  79, 151,  60], device='cuda:0',
           dtype=torch.uint8)
[I] trt-runner-N8-07/03/25-18:26:23    
    ---- Inference Output(s) ----
    {/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[X] trt-runner-N8-07/03/25-18:26:23     | Inference Time: 7.581 ms | Received outputs:
        {'/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 0.4316, -0.3674,  1.2109,  ..., -2.4562,  4.7800, -3.1244],
                 [ 0.4487, -0.3178,  1.2108,  ..., -2.0842,  4.8041, -2.6748],
                 [ 1.4643,  0.1176,  1.5716,  ..., -1.8169,  1.5938, -2.4584],
                 ...,
                 [ 1.5525, -0.1242,  1.9707,  ..., -2.0602,  2.7392, -2.5490],
                 [ 0.5669,  0.0583,  1.1875,  ..., -2.1596,  6.0211, -2.1684],
                 [ 0.2932, -0.1182,  1.4120,  ..., -3.0419,  6.2847, -4.9691]]]), '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 2.6528,  1.5311,  2.6102,  ..., -1.4036,  2.5926, -2.6285],
                 [ 2.5577,  1.0807,  2.5447,  ..., -1.5526,  2.3486, -2.8261],
                 [ 2.9017,  1.4335,  3.2413,  ..., -1.7719,  1.6068, -2.1192],
                 ...,
                 [ 2.2386,  1.1959,  2.6401,  ..., -2.0415,  1.4249, -1.7451],
                 [ 2.4426,  1.7146,  2.2050,  ..., -2.5656,  0.0447, -2.8893],
                 [ 2.1967,  0.3839,  1.9957,  ..., -1.7783,  2.9338, -2.0423]]]), '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0': tensor([[[-0.0838, -1.5191,  0.3148,  ..., -2.4632,  2.7313, -4.1327],
                 [-0.5353, -1.7380, -0.0057,  ..., -2.5524,  3.0500, -4.0854],
                 [ 0.3636, -0.6534,  0.4637,  ..., -2.5760,  3.5717, -4.0373],
                 ...,
                 [-0.5541, -0.7204,  0.8832,  ..., -2.0863,  1.8275, -3.1611],
                 [-0.5803, -1.4158,  0.1699,  ..., -2.6334,  3.0617, -3.6630],
                 [-0.5597, -0.8800,  0.8625,  ..., -4.0711,  2.3900, -4.3777]]]), 'labels': tensor([[ 0,  0,  0, 26, 13, 67,  0, 13, 13,  8,  8, 13, 13, 13,  8,  8,  8, 36,
                  8, 67, 28,  8, 13,  0, 28, 24,  0,  8, 67,  8, 27,  8, 13, 13,  0, 24,
                 26, 28, 13, 13, 58,  8,  0, 24, 26,  2, 26,  0, 13, 13, 26, 28,  0,  0,
                  0,  2, 13, 13,  0,  0,  0, 13,  8, 13,  8,  0, 26, 24,  8,  0,  8, 13,
                 26,  8, 24,  8,  0,  0, 24, 28, 13,  2, 24, 13, 13, 13, 13, 13,  0,  2,
                  8, 13, 13, 31, 24,  8, 13,  2, 24,  8, 13,  2,  8,  8,  0,  2,  8,  8,
                 13, 26,  9, 56, 24,  8, 13, 13, 13, 58, 13, 13,  8, 13,  0,  8, 24,  0,
                  8, 26, 26, 13, 13,  8, 24,  8,  0, 58,  8, 67,  0, 13, 24,  0, 24,  8,
                 13, 13,  0,  2,  8,  8,  0,  0,  7,  8,  3,  0, 58,  8, 13, 56, 13,  7,
                  8, 26,  2, 24,  2,  0,  8, 13, 28,  8, 13,  8, 26, 67, 41, 24,  8, 24,
                 14,  8,  8,  8,  0, 56, 13, 26, 24,  9, 26, 39, 26, 24, 13, 24, 13, 26,
                  2, 56, 24,  2, 24,  2, 58, 56,  8, 13,  0, 26,  8,  0, 67, 26, 58, 13,
                 76, 67, 67,  8, 13, 58,  8, 26, 26, 13,  0,  0,  8, 25, 56,  2, 26, 13,
                 13, 73,  0, 24, 56,  8,  8, 24,  8,  2,  8, 26, 13,  0, 26, 36,  0, 13,
                 13,  1, 67,  0,  0, 58, 27, 39,  2,  0, 26, 56, 13, 24,  0, 36, 56,  8,
                 39,  0, 13,  0,  0, 60, 73, 13, 58,  8,  2,  0, 56, 13,  8, 26, 13, 60,
                  8,  2, 26,  0,  8, 24, 26, 67, 67, 13, 56, 13]]), 'boxes': tensor([[[264.5414,  65.7668, 553.2275, 475.2036],
                 [138.4842, 156.1638, 223.6809, 283.3659],
                 [221.9758, 158.8307, 285.1899, 276.6480],
                 ...,
                 [362.2994, 394.5497, 604.3275, 479.0206],
                 [361.6497, 318.6481, 640.5162, 475.5358],
                 [286.0392, 192.9799, 310.2411, 202.8626]]]), 'scores': tensor([[0.9638, 0.9258, 0.9101, 0.8503, 0.7216, 0.7023, 0.6242, 0.6046, 0.5814,
                 0.5730, 0.5536, 0.5310, 0.5297, 0.5250, 0.5104, 0.4812, 0.4811, 0.4529,
                 0.4524, 0.4521, 0.4371, 0.3776, 0.3646, 0.3610, 0.3609, 0.3558, 0.3397,
                 0.3314, 0.3174, 0.3170, 0.3050, 0.3007, 0.2879, 0.2779, 0.2736, 0.2711,
                 0.2707, 0.2603, 0.2591, 0.2495, 0.2481, 0.2450, 0.2430, 0.2362, 0.2346,
                 0.2308, 0.2269, 0.2257, 0.2237, 0.2168, 0.2140, 0.2097, 0.2071, 0.2051,
                 0.2034, 0.2009, 0.1993, 0.1992, 0.1987, 0.1983, 0.1916, 0.1915, 0.1912,
                 0.1887, 0.1885, 0.1837, 0.1816, 0.1789, 0.1770, 0.1755, 0.1733, 0.1715,
                 0.1712, 0.1684, 0.1681, 0.1652, 0.1646, 0.1645, 0.1636, 0.1630, 0.1620,
                 0.1613, 0.1610, 0.1603, 0.1586, 0.1583, 0.1576, 0.1573, 0.1569, 0.1556,
                 0.1551, 0.1526, 0.1514, 0.1511, 0.1496, 0.1490, 0.1489, 0.1483, 0.1469,
                 0.1458, 0.1453, 0.1443, 0.1419, 0.1407, 0.1404, 0.1393, 0.1393, 0.1378,
                 0.1370, 0.1367, 0.1351, 0.1349, 0.1347, 0.1338, 0.1337, 0.1335, 0.1330,
                 0.1327, 0.1307, 0.1306, 0.1299, 0.1298, 0.1280, 0.1266, 0.1241, 0.1233,
                 0.1232, 0.1227, 0.1214, 0.1214, 0.1206, 0.1195, 0.1190, 0.1189, 0.1176,
                 0.1174, 0.1174, 0.1166, 0.1162, 0.1157, 0.1124, 0.1107, 0.1105, 0.1104,
                 0.1103, 0.1096, 0.1088, 0.1084, 0.1083, 0.1079, 0.1079, 0.1078, 0.1073,
                 0.1072, 0.1068, 0.1068, 0.1063, 0.1059, 0.1056, 0.1049, 0.1042, 0.1042,
                 0.1040, 0.1022, 0.1020, 0.1019, 0.1014, 0.1008, 0.0995, 0.0990, 0.0989,
                 0.0986, 0.0985, 0.0980, 0.0976, 0.0970, 0.0965, 0.0963, 0.0963, 0.0961,
                 0.0958, 0.0951, 0.0946, 0.0944, 0.0943, 0.0941, 0.0925, 0.0923, 0.0920,
                 0.0920, 0.0919, 0.0916, 0.0912, 0.0910, 0.0904, 0.0904, 0.0897, 0.0895,
                 0.0891, 0.0884, 0.0884, 0.0875, 0.0875, 0.0872, 0.0867, 0.0863, 0.0861,
                 0.0858, 0.0858, 0.0854, 0.0853, 0.0850, 0.0849, 0.0846, 0.0846, 0.0843,
                 0.0838, 0.0835, 0.0830, 0.0824, 0.0823, 0.0822, 0.0821, 0.0817, 0.0816,
                 0.0815, 0.0814, 0.0813, 0.0810, 0.0809, 0.0809, 0.0806, 0.0798, 0.0791,
                 0.0787, 0.0784, 0.0784, 0.0777, 0.0775, 0.0771, 0.0769, 0.0769, 0.0766,
                 0.0766, 0.0765, 0.0765, 0.0763, 0.0745, 0.0744, 0.0744, 0.0740, 0.0740,
                 0.0738, 0.0736, 0.0733, 0.0732, 0.0732, 0.0731, 0.0729, 0.0728, 0.0727,
                 0.0725, 0.0717, 0.0714, 0.0703, 0.0700, 0.0696, 0.0695, 0.0695, 0.0695,
                 0.0692, 0.0690, 0.0689, 0.0688, 0.0687, 0.0686, 0.0683, 0.0681, 0.0681,
                 0.0675, 0.0668, 0.0668, 0.0668, 0.0667, 0.0661, 0.0660, 0.0660, 0.0658,
                 0.0657, 0.0657, 0.0657, 0.0657, 0.0649, 0.0648, 0.0645, 0.0644, 0.0641,
                 0.0640, 0.0640, 0.0637]])}
[I] trt-runner-N8-07/03/25-18:26:23     | Completed 1 iteration(s) in 7.581 ms | Average inference time: 7.581 ms.
[I] onnxrt-runner-N8-07/03/25-18:26:23  | Activating and starting inference
[I] Creating ONNX-Runtime Inference Session with providers: ['CPUExecutionProvider']
[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. 
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] onnxrt-runner-N8-07/03/25-18:26:23 
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[X] onnxrt-runner-N8-07/03/25-18:26:23  | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}
[V] onnxrt-runner-N8-07/03/25-18:26:23  | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[I] onnxrt-runner-N8-07/03/25-18:26:23 
    ---- Inference Output(s) ----
    {/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[X] onnxrt-runner-N8-07/03/25-18:26:23  | Inference Time: 141.833 ms | Received outputs:
        {'/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 0.4873, -0.2872,  1.2477,  ..., -1.9035,  4.6459, -2.5865],
                 [ 0.5295, -0.3420,  1.2929,  ..., -2.2817,  4.8796, -3.0140],
                 [ 1.3832,  0.0874,  1.5528,  ..., -1.8717,  1.6218, -2.4840],
                 ...,
                 [ 0.1996,  0.3820,  1.3124,  ..., -2.7857,  5.7248, -5.0676],
                 [ 0.6862,  0.3825,  1.7086,  ..., -3.7300,  6.0249, -7.4908],
                 [ 0.6994,  0.2702,  1.3387,  ..., -2.1122,  6.5988, -3.5191]]]), '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 2.5207,  1.1581,  2.4749,  ..., -1.4153,  2.1369, -2.6800],
                 [ 2.5109,  1.3888,  2.5375,  ..., -1.3492,  2.4295, -2.5769],
                 [ 3.0133,  1.5753,  3.2022,  ..., -1.7849,  1.5140, -2.3457],
                 ...,
                 [ 1.9741,  0.6161,  1.4928,  ..., -0.3968,  3.9285, -3.1247],
                 [-0.4951,  0.0801,  0.4984,  ..., -1.9319,  4.9938, -3.5214],
                 [ 1.8562,  1.6371,  1.9030,  ..., -1.4491,  2.4908, -1.8194]]]), '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0': tensor([[[-0.5195, -1.7893,  0.0354,  ..., -2.6352,  3.0571, -4.1036],
                 [-0.1792, -1.4569,  0.2640,  ..., -2.5316,  2.7575, -3.9642],
                 [ 0.4061, -0.6149,  0.5129,  ..., -2.4448,  3.4591, -4.0444],
                 ...,
                 [-1.6378, -0.6653, -0.0822,  ..., -1.8648,  3.9121, -4.4858],
                 [ 0.8325, -0.4050,  0.5578,  ..., -3.3823,  3.6902, -5.1366],
                 [-0.4194, -0.6062,  0.8846,  ..., -0.1627,  5.3451, -3.3694]]]), 'labels': tensor([[ 0,  0,  0, 26, 13,  0,  8, 13, 67, 13,  8, 36, 13, 13,  8,  8, 28, 67,
                  8,  8, 13,  0, 67,  8, 13, 24, 28, 27,  0, 13,  8, 28, 58,  8,  8,  0,
                 13, 26, 24, 24, 13, 13,  0, 13, 13, 26, 26,  2, 13, 13,  8,  2, 26, 13,
                 24, 26,  0, 26, 67,  0,  0,  0,  0, 24,  8,  0, 13, 13,  0,  2,  8,  0,
                  0, 26,  8, 13,  8,  8, 13, 13, 24, 13, 28, 13, 26,  0, 13, 24, 24, 13,
                 13, 13, 24, 26,  8,  8,  2,  8, 13,  8, 13, 24,  0,  2, 67, 26, 24, 58,
                  8, 31,  8,  0,  8,  8,  0, 24,  8, 13,  8, 26, 58,  8,  0,  9, 13, 13,
                  0, 13, 24, 13, 26,  0, 13,  8,  8, 26,  8,  9,  0,  8,  0, 13,  2,  0,
                  2,  8, 56, 56, 14,  0, 24, 26,  8,  2,  3, 13,  8,  2,  0, 24, 13,  8,
                 13,  8, 58, 26,  8, 28, 13,  0, 39, 13, 56,  0, 13, 67,  2, 26, 13, 28,
                  8, 26,  0, 25, 56, 13, 13, 13,  8,  8, 67, 26,  0,  8,  8, 76,  0, 26,
                  2,  0, 26, 13,  2, 13, 13, 13, 26, 13, 67,  0, 13, 13, 13,  2,  7, 58,
                 56,  0,  8,  0, 58,  0, 67, 60, 13, 58, 24, 26, 13, 26, 13, 14, 58, 13,
                 13,  8,  0, 24, 13, 26, 26, 36,  2, 13, 24, 24, 36, 73, 56, 56,  2,  0,
                  8, 24, 24, 60, 24, 58,  8, 45, 24, 24,  1,  8,  8, 56,  0, 12, 26, 39,
                 13, 26,  2,  8, 26, 56, 24, 28, 36,  8, 67,  5, 26, 24, 26, 13,  8, 26,
                  1, 13,  0,  8, 24, 73,  8,  0, 56, 26, 60,  6]]), 'boxes': tensor([[[264.7241,  65.3608, 553.2991, 475.9188],
                 [222.0591, 159.3543, 285.3618, 276.5153],
                 [138.5406, 156.0868, 223.9909, 283.3219],
                 ...,
                 [221.9039, 171.6946, 234.6225, 204.3119],
                 [362.2125, 322.4501, 606.7469, 477.4002],
                 [278.8448, 195.1371, 323.1751, 225.4758]]]), 'scores': tensor([[0.9616, 0.9148, 0.9139, 0.8343, 0.6973, 0.6629, 0.6341, 0.6328, 0.6262,
                 0.5367, 0.5253, 0.5241, 0.5100, 0.4903, 0.4867, 0.4690, 0.4631, 0.4488,
                 0.4225, 0.4212, 0.4086, 0.3659, 0.3536, 0.3489, 0.3404, 0.3302, 0.3145,
                 0.3105, 0.3065, 0.3040, 0.2982, 0.2894, 0.2834, 0.2805, 0.2786, 0.2513,
                 0.2488, 0.2468, 0.2426, 0.2405, 0.2392, 0.2344, 0.2289, 0.2248, 0.2181,
                 0.2155, 0.2134, 0.2103, 0.2074, 0.2032, 0.2030, 0.2022, 0.1986, 0.1981,
                 0.1976, 0.1973, 0.1960, 0.1948, 0.1935, 0.1885, 0.1877, 0.1861, 0.1810,
                 0.1797, 0.1784, 0.1777, 0.1749, 0.1744, 0.1739, 0.1730, 0.1722, 0.1715,
                 0.1708, 0.1686, 0.1680, 0.1663, 0.1651, 0.1648, 0.1644, 0.1610, 0.1581,
                 0.1546, 0.1537, 0.1517, 0.1502, 0.1485, 0.1477, 0.1477, 0.1456, 0.1449,
                 0.1448, 0.1427, 0.1424, 0.1420, 0.1391, 0.1384, 0.1379, 0.1374, 0.1342,
                 0.1341, 0.1335, 0.1331, 0.1323, 0.1322, 0.1318, 0.1307, 0.1294, 0.1294,
                 0.1291, 0.1285, 0.1279, 0.1260, 0.1259, 0.1256, 0.1251, 0.1245, 0.1224,
                 0.1197, 0.1192, 0.1189, 0.1189, 0.1188, 0.1173, 0.1171, 0.1166, 0.1158,
                 0.1149, 0.1142, 0.1112, 0.1112, 0.1110, 0.1109, 0.1108, 0.1106, 0.1104,
                 0.1096, 0.1092, 0.1088, 0.1087, 0.1081, 0.1071, 0.1067, 0.1066, 0.1059,
                 0.1058, 0.1057, 0.1056, 0.1047, 0.1046, 0.1041, 0.1034, 0.1030, 0.1022,
                 0.1020, 0.1016, 0.1016, 0.1010, 0.1002, 0.0994, 0.0990, 0.0988, 0.0985,
                 0.0979, 0.0969, 0.0969, 0.0967, 0.0963, 0.0958, 0.0949, 0.0948, 0.0940,
                 0.0938, 0.0933, 0.0929, 0.0928, 0.0920, 0.0907, 0.0907, 0.0906, 0.0901,
                 0.0899, 0.0899, 0.0897, 0.0893, 0.0890, 0.0887, 0.0887, 0.0884, 0.0883,
                 0.0881, 0.0878, 0.0867, 0.0857, 0.0854, 0.0842, 0.0842, 0.0841, 0.0839,
                 0.0835, 0.0835, 0.0834, 0.0832, 0.0824, 0.0824, 0.0823, 0.0823, 0.0818,
                 0.0815, 0.0812, 0.0805, 0.0797, 0.0790, 0.0789, 0.0786, 0.0785, 0.0783,
                 0.0783, 0.0783, 0.0781, 0.0780, 0.0777, 0.0766, 0.0765, 0.0764, 0.0759,
                 0.0755, 0.0755, 0.0752, 0.0752, 0.0750, 0.0742, 0.0740, 0.0739, 0.0735,
                 0.0735, 0.0734, 0.0733, 0.0732, 0.0732, 0.0730, 0.0730, 0.0725, 0.0723,
                 0.0722, 0.0721, 0.0719, 0.0718, 0.0717, 0.0716, 0.0712, 0.0711, 0.0706,
                 0.0700, 0.0700, 0.0698, 0.0697, 0.0692, 0.0690, 0.0690, 0.0689, 0.0687,
                 0.0686, 0.0679, 0.0678, 0.0675, 0.0674, 0.0673, 0.0672, 0.0669, 0.0669,
                 0.0666, 0.0663, 0.0662, 0.0662, 0.0661, 0.0658, 0.0657, 0.0654, 0.0653,
                 0.0651, 0.0649, 0.0649, 0.0646, 0.0646, 0.0645, 0.0644, 0.0644, 0.0643,
                 0.0643, 0.0638, 0.0638, 0.0635, 0.0634, 0.0633, 0.0633, 0.0632, 0.0626,
                 0.0626, 0.0625, 0.0618]])}
[I] onnxrt-runner-N8-07/03/25-18:26:23  | Completed 1 iteration(s) in 141.8 ms | Average inference time: 141.8 ms.
[V] Successfully ran: ['trt-runner-N8-07/03/25-18:26:23', 'onnxrt-runner-N8-07/03/25-18:26:23']
[I] Accuracy Comparison | trt-runner-N8-07/03/25-18:26:23 vs. onnxrt-runner-N8-07/03/25-18:26:23
[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N8-07/03/25-18:26:23 vs. onnxrt-runner-N8-07/03/25-18:26:23
[X]             trt-runner-N8-07/03/25-18:26:23     | Mismatched values:
                tensor([ 0.4316, -0.3674,  1.2109,  ..., -3.0419,  6.2847, -4.9691])
[X]             onnxrt-runner-N8-07/03/25-18:26:23  | Mismatched values:
                tensor([ 0.4873, -0.2872,  1.2477,  ..., -2.1122,  6.5988, -3.5191])
[I]         trt-runner-N8-07/03/25-18:26:23: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.15055, std-dev=2.0813, var=4.3319, median=0.13813, min=-7.7755 at (0, 59, 191), max=7.6648 at (0, 184, 190), avg-magnitude=1.7079, p90=2.947, p95=3.6524, p99=4.2705
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-8.37 , -6.74 ) |       14.0 | 
                (-6.74 , -5.1  ) |      116.0 | 
                (-5.1  , -3.47 ) |     2194.0 | #####
                (-3.47 , -1.84 ) |     8487.0 | #####################
                (-1.84 , -0.206) |    14341.0 | ####################################
                (-0.206, 1.43  ) |    15916.0 | ########################################
                (1.43  , 3.06  ) |    11249.0 | ############################
                (3.06  , 4.69  ) |     4938.0 | ############
                (4.69  , 6.33  ) |      284.0 | 
                (6.33  , 7.96  ) |       61.0 | 
[I]         onnxrt-runner-N8-07/03/25-18:26:23: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.15709, std-dev=2.0893, var=4.3651, median=0.14377, min=-8.3695 at (0, 285, 191), max=7.9581 at (0, 56, 190), avg-magnitude=1.7118, p90=2.9767, p95=3.6649, p99=4.3001
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-8.37 , -6.74 ) |       21.0 | 
                (-6.74 , -5.1  ) |      120.0 | 
                (-5.1  , -3.47 ) |     2164.0 | #####
                (-3.47 , -1.84 ) |     8528.0 | #####################
                (-1.84 , -0.206) |    14240.0 | ###################################
                (-0.206, 1.43  ) |    15931.0 | ########################################
                (1.43  , 3.06  ) |    11209.0 | ############################
                (3.06  , 4.69  ) |     5034.0 | ############
                (4.69  , 6.33  ) |      292.0 | 
                (6.33  , 7.96  ) |       61.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=7.3095] OR [rel=18342] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.73438, std-dev=0.75091, var=0.56386, median=0.50277, min=0 at (0, 10, 127), max=7.3095 at (0, 144, 104), avg-magnitude=0.73438, p90=1.6948, p95=2.22, p99=3.4751
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (0    , 0.731) |    36310.0 | ########################################
                    (0.731, 1.46 ) |    13356.0 | ##############
                    (1.46 , 2.19 ) |     4943.0 | #####
                    (2.19 , 2.92 ) |     1830.0 | ##
                    (2.92 , 3.65 ) |      699.0 | 
                    (3.65 , 4.39 ) |      280.0 | 
                    (4.39 , 5.12 ) |      104.0 | 
                    (5.12 , 5.85 ) |       46.0 | 
                    (5.85 , 6.58 ) |       26.0 | 
                    (6.58 , 7.31 ) |        6.0 | 
[I]             Relative Difference | Stats: mean=3.2675, std-dev=113.77, var=12943, median=0.38604, min=0 at (0, 10, 127), max=18342 at (0, 250, 40), avg-magnitude=3.2675, p90=2.5065, p95=5.0638, p99=26.278
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0       , 1.83e+03) |    57587.0 | ########################################
                    (1.83e+03, 3.67e+03) |        8.0 | 
                    (3.67e+03, 5.5e+03 ) |        2.0 | 
                    (5.5e+03 , 7.34e+03) |        1.0 | 
                    (7.34e+03, 9.17e+03) |        0.0 | 
                    (9.17e+03, 1.1e+04 ) |        0.0 | 
                    (1.1e+04 , 1.28e+04) |        0.0 | 
                    (1.28e+04, 1.47e+04) |        0.0 | 
                    (1.47e+04, 1.65e+04) |        0.0 | 
                    (1.65e+04, 1.83e+04) |        2.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N8-07/03/25-18:26:23] and '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N8-07/03/25-18:26:23]
[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N8-07/03/25-18:26:23 vs. onnxrt-runner-N8-07/03/25-18:26:23
[X]             trt-runner-N8-07/03/25-18:26:23     | Mismatched values:
                tensor([ 2.6528,  1.5311,  2.6102,  ..., -1.7783,  2.9338, -2.0423])
[X]             onnxrt-runner-N8-07/03/25-18:26:23  | Mismatched values:
                tensor([ 2.5207,  1.1581,  2.4749,  ..., -1.4491,  2.4908, -1.8194])
[I]         trt-runner-N8-07/03/25-18:26:23: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.062986, std-dev=2.5864, var=6.6897, median=0.13066, min=-33.048 at (0, 170, 183), max=12.167 at (0, 224, 47), avg-magnitude=1.9628, p90=3.2053, p95=3.6994, p99=4.7205
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-35.5, -30.8) |        5.0 | 
                (-30.8, -26  ) |       21.0 | 
                (-26  , -21.2) |       37.0 | 
                (-21.2, -16.4) |       63.0 | 
                (-16.4, -11.7) |       83.0 | 
                (-11.7, -6.91) |       68.0 | 
                (-6.91, -2.14) |    10446.0 | ###########
                (-2.14, 2.63 ) |    37592.0 | ########################################
                (2.63 , 7.4  ) |     9142.0 | #########
                (7.4  , 12.2 ) |      143.0 | 
[I]         onnxrt-runner-N8-07/03/25-18:26:23: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.051423, std-dev=2.584, var=6.6772, median=0.11873, min=-35.523 at (0, 275, 183), max=12.039 at (0, 222, 47), avg-magnitude=1.9569, p90=3.1917, p95=3.6942, p99=4.7464
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-35.5, -30.8) |        7.0 | 
                (-30.8, -26  ) |       17.0 | 
                (-26  , -21.2) |       34.0 | 
                (-21.2, -16.4) |       64.0 | 
                (-16.4, -11.7) |       82.0 | 
                (-11.7, -6.91) |       70.0 | 
                (-6.91, -2.14) |    10494.0 | ###########
                (-2.14, 2.63 ) |    37644.0 | ########################################
                (2.63 , 7.4  ) |     9041.0 | #########
                (7.4  , 12.2 ) |      147.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=28.387] OR [rel=15187] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.92788, std-dev=1.0742, var=1.1538, median=0.64655, min=3.5763e-07 at (0, 40, 187), max=28.387 at (0, 172, 183), avg-magnitude=0.92788, p90=2.0655, p95=2.6194, p99=4.1407
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (3.58e-07, 2.84) |    55359.0 | ########################################
                    (2.84    , 5.68) |     1991.0 | #
                    (5.68    , 8.52) |      122.0 | 
                    (8.52    , 11.4) |       48.0 | 
                    (11.4    , 14.2) |       31.0 | 
                    (14.2    , 17  ) |       31.0 | 
                    (17      , 19.9) |       10.0 | 
                    (19.9    , 22.7) |        4.0 | 
                    (22.7    , 25.5) |        2.0 | 
                    (25.5    , 28.4) |        2.0 | 
[I]             Relative Difference | Stats: mean=3.1402, std-dev=83.824, var=7026.5, median=0.42242, min=2.304e-07 at (0, 40, 187), max=15187 at (0, 36, 8), avg-magnitude=3.1402, p90=2.9163, p95=5.7083, p99=28.305
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (2.3e-07 , 1.52e+03) |    57593.0 | ########################################
                    (1.52e+03, 3.04e+03) |        4.0 | 
                    (3.04e+03, 4.56e+03) |        0.0 | 
                    (4.56e+03, 6.07e+03) |        0.0 | 
                    (6.07e+03, 7.59e+03) |        1.0 | 
                    (7.59e+03, 9.11e+03) |        1.0 | 
                    (9.11e+03, 1.06e+04) |        0.0 | 
                    (1.06e+04, 1.21e+04) |        0.0 | 
                    (1.21e+04, 1.37e+04) |        0.0 | 
                    (1.37e+04, 1.52e+04) |        1.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N8-07/03/25-18:26:23] and '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N8-07/03/25-18:26:23]
[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N8-07/03/25-18:26:23 vs. onnxrt-runner-N8-07/03/25-18:26:23
[X]             trt-runner-N8-07/03/25-18:26:23     | Mismatched values:
                tensor([-0.0838, -1.5191,  0.3148,  ..., -4.0711,  2.3900, -4.3777])
[X]             onnxrt-runner-N8-07/03/25-18:26:23  | Mismatched values:
                tensor([-0.5195, -1.7893,  0.0354,  ..., -0.1627,  5.3451, -3.3694])
[I]         trt-runner-N8-07/03/25-18:26:23: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.086785, std-dev=2.2967, var=5.2749, median=0.14519, min=-9.6005 at (0, 179, 183), max=8.9231 at (0, 263, 184), avg-magnitude=1.8904, p90=3.2069, p95=3.8582, p99=4.4169
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.65 , -7.8  ) |       64.0 | 
                (-7.8  , -5.94 ) |      145.0 | 
                (-5.94 , -4.08 ) |      648.0 | #
                (-4.08 , -2.22 ) |     9555.0 | ######################
                (-2.22 , -0.365) |    13757.0 | ################################
                (-0.365, 1.49  ) |    16901.0 | ########################################
                (1.49  , 3.35  ) |    11437.0 | ###########################
                (3.35  , 5.21  ) |     4958.0 | ###########
                (5.21  , 7.07  ) |      131.0 | 
                (7.07  , 8.92  ) |        4.0 | 
[I]         onnxrt-runner-N8-07/03/25-18:26:23: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.090388, std-dev=2.2926, var=5.2562, median=0.14476, min=-9.6536 at (0, 285, 183), max=8.2856 at (0, 264, 184), avg-magnitude=1.8848, p90=3.214, p95=3.8515, p99=4.398
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.65 , -7.8  ) |       61.0 | 
                (-7.8  , -5.94 ) |      146.0 | 
                (-5.94 , -4.08 ) |      622.0 | #
                (-4.08 , -2.22 ) |     9490.0 | ######################
                (-2.22 , -0.365) |    13778.0 | ################################
                (-0.365, 1.49  ) |    17072.0 | ########################################
                (1.49  , 3.35  ) |    11340.0 | ##########################
                (3.35  , 5.21  ) |     4948.0 | ###########
                (5.21  , 7.07  ) |      135.0 | 
                (7.07  , 8.92  ) |        8.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=9.9742] OR [rel=63297] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.91983, std-dev=0.88394, var=0.78135, median=0.66069, min=5.9605e-08 at (0, 273, 82), max=9.9742 at (0, 263, 184), avg-magnitude=0.91983, p90=2.1388, p95=2.6805, p99=3.8562
[I]                 ---- Histogram ----
                    Bin Range         |  Num Elems | Visualization
                    (5.96e-08, 0.997) |    37102.0 | ########################################
                    (0.997   , 1.99 ) |    13668.0 | ##############
                    (1.99    , 2.99 ) |     4908.0 | #####
                    (2.99    , 3.99 ) |     1440.0 | #
                    (3.99    , 4.99 ) |      355.0 | 
                    (4.99    , 5.98 ) |       94.0 | 
                    (5.98    , 6.98 ) |       21.0 | 
                    (6.98    , 7.98 ) |        9.0 | 
                    (7.98    , 8.98 ) |        1.0 | 
                    (8.98    , 9.97 ) |        2.0 | 
[I]             Relative Difference | Stats: mean=4.1724, std-dev=269.11, var=72418, median=0.45277, min=7.6303e-08 at (0, 103, 170), max=63297 at (0, 1, 24), avg-magnitude=4.1724, p90=3.2588, p95=6.5565, p99=30.851
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (7.63e-08, 6.33e+03) |    57599.0 | ########################################
                    (6.33e+03, 1.27e+04) |        0.0 | 
                    (1.27e+04, 1.9e+04 ) |        0.0 | 
                    (1.9e+04 , 2.53e+04) |        0.0 | 
                    (2.53e+04, 3.16e+04) |        0.0 | 
                    (3.16e+04, 3.8e+04 ) |        0.0 | 
                    (3.8e+04 , 4.43e+04) |        0.0 | 
                    (4.43e+04, 5.06e+04) |        0.0 | 
                    (5.06e+04, 5.7e+04 ) |        0.0 | 
                    (5.7e+04 , 6.33e+04) |        1.0 | 
[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N8-07/03/25-18:26:23] and '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N8-07/03/25-18:26:23]
[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: 'labels' (dtype=int64, shape=torch.Size([1, 300])) with 'labels' (dtype=int64, shape=torch.Size([1, 300]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N8-07/03/25-18:26:23 vs. onnxrt-runner-N8-07/03/25-18:26:23
[X]             trt-runner-N8-07/03/25-18:26:23     | Mismatched values:
                tensor([67,  0, 13,  8, 13,  8, 36, 67, 28,  8, 13,  0, 28,  0,  8, 67,  8, 27,
                         8, 13, 13,  0, 24, 26, 28, 13, 13, 58,  8, 24, 26,  2,  0, 26, 28,  0,
                         0,  0,  2, 13, 13,  0, 13,  8, 13, 26, 24,  8,  0, 13, 26,  8, 24,  8,
                         0,  0, 24, 28, 13,  2, 24, 13, 13, 13,  0,  2,  8, 13, 31, 24, 13,  2,
                        24,  2,  8,  8,  0,  2,  8,  8, 13, 26,  9, 56, 24, 13, 13, 13, 58, 13,
                        13,  8, 13,  8, 24,  0,  8, 26, 26, 13,  8, 24,  0, 58, 67, 13, 24,  0,
                        24,  8, 13, 13,  0,  2,  8,  8,  0,  0,  7,  8,  0, 58,  8, 13, 56,  7,
                         8, 26,  2, 24,  2,  0,  8, 13, 28,  8, 13,  8, 26, 41, 24,  8, 24, 14,
                         8,  8,  8,  0, 56, 26, 24,  9, 26, 39, 26, 24, 13, 24, 13, 56, 24,  2,
                        24,  2, 58, 56,  8,  0, 26,  8,  0, 67, 26, 58, 13, 76, 67, 67,  8, 13,
                        58,  8, 26, 26, 13,  0,  0,  8, 25, 56,  2, 26, 73, 56,  8,  8, 24,  8,
                         2,  8, 26, 13,  0, 26, 36,  0, 13, 13,  1, 67,  0,  0, 27, 39,  2,  0,
                        26, 56, 13, 24, 36, 56,  8, 39,  0, 13,  0,  0, 60, 73, 13, 58,  2,  0,
                        56, 13,  8, 26, 13, 60,  8,  2, 26,  0,  8, 24, 26, 67, 67, 13, 56, 13])
[X]             onnxrt-runner-N8-07/03/25-18:26:23  | Mismatched values:
                tensor([ 0,  8, 67, 13, 36, 28, 67,  8, 13,  0, 67,  8, 13, 28, 27,  0, 13,  8,
                        28, 58,  8,  8,  0, 13, 26, 24, 24, 13, 13, 13, 13, 26,  2,  8,  2, 26,
                        13, 24, 26,  0, 26, 67,  0,  0, 24, 13, 13,  0,  2,  0,  0, 26,  8, 13,
                         8,  8, 13, 13, 24, 13, 28, 26,  0, 24, 24, 13, 13, 24, 26,  8,  2,  8,
                        13, 24,  0,  2, 67, 26, 24, 58,  8, 31,  8,  0,  8,  0, 24,  8, 13,  8,
                        26, 58,  8,  9, 13, 13,  0, 13, 24, 26,  0, 13,  8, 26,  9,  8,  0, 13,
                         2,  0,  2,  8, 56, 56, 14,  0, 24, 26,  8,  2, 13,  8,  2,  0, 24,  8,
                        13,  8, 58, 26,  8, 28, 13,  0, 39, 13, 56,  0, 13,  2, 26, 13, 28,  8,
                        26,  0, 25, 56, 13, 13,  8,  8, 67, 26,  0,  8,  8, 76,  0,  0, 26, 13,
                         2, 13, 13, 13, 26, 67,  0, 13, 13, 13,  2,  7, 58, 56,  0,  8,  0, 58,
                         0, 67, 60, 13, 58, 24, 26, 13, 26, 13, 14, 58,  8, 13, 26, 26, 36,  2,
                        13, 24, 24, 36, 73, 56, 56,  2,  0,  8, 24, 24, 60, 24,  8, 45, 24, 24,
                         1,  8,  8, 56, 12, 26, 39, 13, 26,  2,  8, 26, 56, 24, 28, 36, 67,  5,
                        26, 24, 26, 13,  8, 26,  1, 13,  0,  8, 24, 73,  8,  0, 56, 26, 60,  6])
[I]         trt-runner-N8-07/03/25-18:26:23: labels | Stats: mean=18.187, std-dev=18.669, var=348.55, median=13, min=0 at (0, 0), max=76 at (0, 216), avg-magnitude=18.187, p90=56, p95=60, p99=67
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         70 | ######################
                (7 , 15) |        124 | ########################################
                (15, 22) |          0 | 
                (22, 30) |         62 | ####################
                (30, 38) |          4 | #
                (38, 45) |          4 | #
                (45, 53) |          0 | 
                (53, 60) |         22 | #######
                (60, 68) |         11 | ###
                (68, 76) |          3 | 
[I]         onnxrt-runner-N8-07/03/25-18:26:23: labels | Stats: mean=18.583, std-dev=18.363, var=337.21, median=13, min=0 at (0, 0), max=76 at (0, 195), avg-magnitude=18.583, p90=56, p95=60, p99=67
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         66 | #####################
                (7 , 15) |        122 | ########################################
                (15, 22) |          0 | 
                (22, 30) |         69 | ######################
                (30, 38) |          5 | #
                (38, 45) |          3 | 
                (45, 53) |          0 | 
                (53, 60) |         22 | #######
                (60, 68) |         10 | ###
                (68, 76) |          3 | 
[I]         Error Metrics: labels
[I]             Minimum Required Tolerance: elemwise error | [abs=73] OR [rel=nan] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=17.77, std-dev=18.108, var=327.9, median=13, min=0 at (0, 0), max=73 at (0, 247), avg-magnitude=17.77, p90=49, p95=58, p99=67
[I]                 ---- Histogram ----
                    Bin Range|  Num Elems | Visualization
                    (0 , 7 ) |        103 | ########################################
                    (7 , 14) |         74 | ############################
                    (14, 21) |         29 | ###########
                    (21, 29) |         35 | #############
                    (29, 36) |         11 | ####
                    (36, 43) |          8 | ###
                    (43, 51) |         14 | #####
                    (51, 58) |         12 | ####
                    (58, 65) |          6 | ##
                    (65, 73) |          8 | ###
[I]             Relative Difference | Stats: mean=nan, std-dev=nan, var=nan, median=nan, min=nan at (0, 0), max=nan at (0, 0), avg-magnitude=nan, p90=nan, p95=nan, p99=nan
[V]                 Could not generate histogram. Note: Error was: torch.histogramdd: dimension 0's range [-nan, -nan] is not finite
[I]                 
[X]         Finished comparing: 'labels' (dtype=int64, shape=torch.Size([1, 300])) [trt-runner-N8-07/03/25-18:26:23] and 'labels' (dtype=int64, shape=torch.Size([1, 300])) [onnxrt-runner-N8-07/03/25-18:26:23]
[E]         FAILED | Output: 'labels' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) with 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N8-07/03/25-18:26:23 vs. onnxrt-runner-N8-07/03/25-18:26:23
[X]             trt-runner-N8-07/03/25-18:26:23     | Mismatched values:
                tensor([264.5414,  65.7668, 553.2275,  ..., 192.9799, 310.2411, 202.8626])
[X]             onnxrt-runner-N8-07/03/25-18:26:23  | Mismatched values:
                tensor([264.7241,  65.3608, 553.2991,  ..., 195.1371, 323.1751, 225.4758])
[I]         trt-runner-N8-07/03/25-18:26:23: boxes | Stats: mean=275.72, std-dev=148.03, var=21912, median=231.2, min=-0.58401 at (0, 38, 0), max=645.32 at (0, 194, 2), avg-magnitude=275.73, p90=534.29, p95=602.85, p99=639.41
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-0.977, 63.7) |       54.0 | #####
                (63.7  , 128 ) |       65.0 | #######
                (128   , 193 ) |      205.0 | ######################
                (193   , 258 ) |      370.0 | ########################################
                (258   , 322 ) |      178.0 | ###################
                (322   , 387 ) |      107.0 | ###########
                (387   , 451 ) |       33.0 | ###
                (451   , 516 ) |       57.0 | ######
                (516   , 581 ) |       42.0 | ####
                (581   , 645 ) |       89.0 | #########
[I]         onnxrt-runner-N8-07/03/25-18:26:23: boxes | Stats: mean=271.02, std-dev=146.07, var=21337, median=228.6, min=-0.97731 at (0, 90, 0), max=641.26 at (0, 40, 2), avg-magnitude=271.03, p90=525.5, p95=595.62, p99=639.07
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-0.977, 63.7) |       60.0 | ######
                (63.7  , 128 ) |       69.0 | #######
                (128   , 193 ) |      198.0 | ####################
                (193   , 258 ) |      389.0 | ########################################
                (258   , 322 ) |      169.0 | #################
                (322   , 387 ) |      104.0 | ##########
                (387   , 451 ) |       32.0 | ###
                (451   , 516 ) |       52.0 | #####
                (516   , 581 ) |       50.0 | #####
                (581   , 645 ) |       77.0 | #######
[I]         Error Metrics: boxes
[I]             Minimum Required Tolerance: elemwise error | [abs=620.49] OR [rel=4886.8] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=132.5, std-dev=135.77, var=18434, median=79.604, min=0.0048218 at (0, 42, 2), max=620.49 at (0, 144, 0), avg-magnitude=132.5, p90=324.05, p95=435.66, p99=531.4
[I]                 ---- Histogram ----
                    Bin Range       |  Num Elems | Visualization
                    (0.00482, 62.1) |      520.0 | ########################################
                    (62.1   , 124 ) |      212.0 | ################
                    (124    , 186 ) |      124.0 | #########
                    (186    , 248 ) |       97.0 | #######
                    (248    , 310 ) |      111.0 | ########
                    (310    , 372 ) |       56.0 | ####
                    (372    , 434 ) |       19.0 | #
                    (434    , 496 ) |       31.0 | ##
                    (496    , 558 ) |       25.0 | #
                    (558    , 620 ) |        5.0 | 
[I]             Relative Difference | Stats: mean=22.483, std-dev=243.94, var=59509, median=0.3409, min=8.101e-06 at (0, 42, 2), max=4886.8 at (0, 210, 0), avg-magnitude=22.483, p90=1.5253, p95=4.3459, p99=490.2
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (8.1e-06 , 489     ) |     1187.0 | ########################################
                    (489     , 977     ) |        5.0 | 
                    (977     , 1.47e+03) |        2.0 | 
                    (1.47e+03, 1.95e+03) |        2.0 | 
                    (1.95e+03, 2.44e+03) |        0.0 | 
                    (2.44e+03, 2.93e+03) |        1.0 | 
                    (2.93e+03, 3.42e+03) |        0.0 | 
                    (3.42e+03, 3.91e+03) |        1.0 | 
                    (3.91e+03, 4.4e+03 ) |        1.0 | 
                    (4.4e+03 , 4.89e+03) |        1.0 | 
[X]         Finished comparing: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [trt-runner-N8-07/03/25-18:26:23] and 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [onnxrt-runner-N8-07/03/25-18:26:23]
[E]         FAILED | Output: 'boxes' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[I]     Comparing Output: 'scores' (dtype=float32, shape=torch.Size([1, 300])) with 'scores' (dtype=float32, shape=torch.Size([1, 300]))
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[X]         Note: Comparing trt-runner-N8-07/03/25-18:26:23 vs. onnxrt-runner-N8-07/03/25-18:26:23
[X]             trt-runner-N8-07/03/25-18:26:23     | Mismatched values:
                tensor([0.9638, 0.9258, 0.9101, 0.8503, 0.7216, 0.7023, 0.6242, 0.6046, 0.5814,
                        0.5730, 0.5536, 0.5310, 0.5297, 0.5250, 0.5104, 0.4812, 0.4811, 0.4529,
                        0.4524, 0.4521, 0.4371, 0.3776, 0.3646, 0.3610, 0.3609, 0.3558, 0.3397,
                        0.3314, 0.3174, 0.3170, 0.3050, 0.3007, 0.2879, 0.2779, 0.2736, 0.2711,
                        0.2707, 0.2603, 0.2591, 0.2495, 0.2481, 0.2450, 0.2430, 0.2362, 0.2346,
                        0.2308, 0.2269, 0.2257, 0.2237, 0.2168, 0.2140, 0.2097, 0.2071, 0.2051,
                        0.2034, 0.2009, 0.1993, 0.1992, 0.1987, 0.1983, 0.1916, 0.1915, 0.1912,
                        0.1887, 0.1885, 0.1837, 0.1816, 0.1789, 0.1770, 0.1755, 0.1733, 0.1715,
                        0.1712, 0.1684, 0.1681, 0.1652, 0.1646, 0.1645, 0.1636, 0.1630, 0.1620,
                        0.1613, 0.1610, 0.1603, 0.1586, 0.1583, 0.1576, 0.1573, 0.1569, 0.1556,
                        0.1551, 0.1526, 0.1514, 0.1511, 0.1496, 0.1490, 0.1489, 0.1483, 0.1469,
                        0.1458, 0.1453, 0.1443, 0.1419, 0.1407, 0.1404, 0.1393, 0.1393, 0.1378,
                        0.1370, 0.1367, 0.1351, 0.1349, 0.1347, 0.1338, 0.1337, 0.1335, 0.1330,
                        0.1327, 0.1307, 0.1306, 0.1299, 0.1298, 0.1280, 0.1266, 0.1241, 0.1233,
                        0.1232, 0.1227, 0.1214, 0.1214, 0.1206, 0.1195, 0.1190, 0.1189, 0.1176,
                        0.1174, 0.1174, 0.1166, 0.1162, 0.1157, 0.1124, 0.1107, 0.1105, 0.1104,
                        0.1103, 0.1096, 0.1088, 0.1084, 0.1083, 0.1079, 0.1079, 0.1078, 0.1073,
                        0.1072, 0.1068, 0.1068, 0.1063, 0.1059, 0.1056, 0.1049, 0.1042, 0.1042,
                        0.1040, 0.1022, 0.1020, 0.1019, 0.1014, 0.1008, 0.0995, 0.0990, 0.0989,
                        0.0986, 0.0985, 0.0980, 0.0976, 0.0970, 0.0965, 0.0963, 0.0963, 0.0961,
                        0.0958, 0.0951, 0.0946, 0.0944, 0.0943, 0.0941, 0.0925, 0.0923, 0.0920,
                        0.0920, 0.0919, 0.0916, 0.0912, 0.0910, 0.0904, 0.0904, 0.0897, 0.0895,
                        0.0891, 0.0884, 0.0884, 0.0875, 0.0875, 0.0872, 0.0867, 0.0863, 0.0861,
                        0.0858, 0.0858, 0.0854, 0.0853, 0.0850, 0.0849, 0.0846, 0.0846, 0.0843,
                        0.0838, 0.0835, 0.0830, 0.0824, 0.0823, 0.0822, 0.0821, 0.0817, 0.0816,
                        0.0815, 0.0814, 0.0813, 0.0810, 0.0809, 0.0809, 0.0806, 0.0798, 0.0791,
                        0.0787, 0.0784, 0.0784, 0.0777, 0.0775, 0.0771, 0.0769, 0.0769, 0.0766,
                        0.0766, 0.0765, 0.0765, 0.0763, 0.0745, 0.0744, 0.0744, 0.0740, 0.0740,
                        0.0738, 0.0736, 0.0733, 0.0732, 0.0732, 0.0731, 0.0729, 0.0728, 0.0727,
                        0.0725, 0.0717, 0.0714, 0.0703, 0.0700, 0.0696, 0.0695, 0.0695, 0.0695,
                        0.0692, 0.0690, 0.0689, 0.0688, 0.0687, 0.0686, 0.0683, 0.0681, 0.0681,
                        0.0675, 0.0668, 0.0668, 0.0668, 0.0667, 0.0661, 0.0660, 0.0660, 0.0658,
                        0.0657, 0.0657, 0.0657, 0.0657, 0.0649, 0.0648, 0.0645, 0.0644, 0.0641,
                        0.0640, 0.0640, 0.0637])
[X]             onnxrt-runner-N8-07/03/25-18:26:23  | Mismatched values:
                tensor([0.9616, 0.9148, 0.9139, 0.8343, 0.6973, 0.6629, 0.6341, 0.6328, 0.6262,
                        0.5367, 0.5253, 0.5241, 0.5100, 0.4903, 0.4867, 0.4690, 0.4631, 0.4488,
                        0.4225, 0.4212, 0.4086, 0.3659, 0.3536, 0.3489, 0.3404, 0.3302, 0.3145,
                        0.3105, 0.3065, 0.3040, 0.2982, 0.2894, 0.2834, 0.2805, 0.2786, 0.2513,
                        0.2488, 0.2468, 0.2426, 0.2405, 0.2392, 0.2344, 0.2289, 0.2248, 0.2181,
                        0.2155, 0.2134, 0.2103, 0.2074, 0.2032, 0.2030, 0.2022, 0.1986, 0.1981,
                        0.1976, 0.1973, 0.1960, 0.1948, 0.1935, 0.1885, 0.1877, 0.1861, 0.1810,
                        0.1797, 0.1784, 0.1777, 0.1749, 0.1744, 0.1739, 0.1730, 0.1722, 0.1715,
                        0.1708, 0.1686, 0.1680, 0.1663, 0.1651, 0.1648, 0.1644, 0.1610, 0.1581,
                        0.1546, 0.1537, 0.1517, 0.1502, 0.1485, 0.1477, 0.1477, 0.1456, 0.1449,
                        0.1448, 0.1427, 0.1424, 0.1420, 0.1391, 0.1384, 0.1379, 0.1374, 0.1342,
                        0.1341, 0.1335, 0.1331, 0.1323, 0.1322, 0.1318, 0.1307, 0.1294, 0.1294,
                        0.1291, 0.1285, 0.1279, 0.1260, 0.1259, 0.1256, 0.1251, 0.1245, 0.1224,
                        0.1197, 0.1192, 0.1189, 0.1189, 0.1188, 0.1173, 0.1171, 0.1166, 0.1158,
                        0.1149, 0.1142, 0.1112, 0.1112, 0.1110, 0.1109, 0.1108, 0.1106, 0.1104,
                        0.1096, 0.1092, 0.1088, 0.1087, 0.1081, 0.1071, 0.1067, 0.1066, 0.1059,
                        0.1058, 0.1057, 0.1056, 0.1047, 0.1046, 0.1041, 0.1034, 0.1030, 0.1022,
                        0.1020, 0.1016, 0.1016, 0.1010, 0.1002, 0.0994, 0.0990, 0.0988, 0.0985,
                        0.0979, 0.0969, 0.0969, 0.0967, 0.0963, 0.0958, 0.0949, 0.0948, 0.0940,
                        0.0938, 0.0933, 0.0929, 0.0928, 0.0920, 0.0907, 0.0907, 0.0906, 0.0901,
                        0.0899, 0.0899, 0.0897, 0.0893, 0.0890, 0.0887, 0.0887, 0.0884, 0.0883,
                        0.0881, 0.0878, 0.0867, 0.0857, 0.0854, 0.0842, 0.0842, 0.0841, 0.0839,
                        0.0835, 0.0835, 0.0834, 0.0832, 0.0824, 0.0824, 0.0823, 0.0823, 0.0818,
                        0.0815, 0.0812, 0.0805, 0.0797, 0.0790, 0.0789, 0.0786, 0.0785, 0.0783,
                        0.0783, 0.0783, 0.0781, 0.0780, 0.0777, 0.0766, 0.0765, 0.0764, 0.0759,
                        0.0755, 0.0755, 0.0752, 0.0752, 0.0750, 0.0742, 0.0740, 0.0739, 0.0735,
                        0.0735, 0.0734, 0.0733, 0.0732, 0.0732, 0.0730, 0.0730, 0.0725, 0.0723,
                        0.0722, 0.0721, 0.0719, 0.0718, 0.0717, 0.0716, 0.0712, 0.0711, 0.0706,
                        0.0700, 0.0700, 0.0698, 0.0697, 0.0692, 0.0690, 0.0690, 0.0689, 0.0687,
                        0.0686, 0.0679, 0.0678, 0.0675, 0.0674, 0.0673, 0.0672, 0.0669, 0.0669,
                        0.0666, 0.0663, 0.0662, 0.0662, 0.0661, 0.0658, 0.0657, 0.0654, 0.0653,
                        0.0651, 0.0649, 0.0649, 0.0646, 0.0646, 0.0645, 0.0644, 0.0644, 0.0643,
                        0.0643, 0.0638, 0.0638, 0.0635, 0.0634, 0.0633, 0.0633, 0.0632, 0.0626,
                        0.0626, 0.0625, 0.0618])
[I]         trt-runner-N8-07/03/25-18:26:23: scores | Stats: mean=0.1613, std-dev=0.14597, var=0.021307, median=0.10793, min=0.063683 at (0, 299), max=0.96384 at (0, 0), avg-magnitude=0.1613, p90=0.30623, p95=0.48264, p99=0.85088
[I]             ---- Histogram ----
                Bin Range       |  Num Elems | Visualization
                (0.0618, 0.152) |      208.0 | ########################################
                (0.152 , 0.242) |       49.0 | #########
                (0.242 , 0.332) |       16.0 | ###
                (0.332 , 0.423) |        6.0 | #
                (0.423 , 0.513) |        7.0 | #
                (0.513 , 0.603) |        6.0 | #
                (0.603 , 0.693) |        2.0 | 
                (0.693 , 0.783) |        2.0 | 
                (0.783 , 0.874) |        1.0 | 
                (0.874 , 0.964) |        3.0 | 
[I]         onnxrt-runner-N8-07/03/25-18:26:23: scores | Stats: mean=0.15455, std-dev=0.14384, var=0.02069, median=0.10375, min=0.061773 at (0, 299), max=0.96157 at (0, 0), avg-magnitude=0.15455, p90=0.29878, p95=0.46987, p99=0.83513
[I]             ---- Histogram ----
                Bin Range       |  Num Elems | Visualization
                (0.0618, 0.152) |      217.0 | ########################################
                (0.152 , 0.242) |       44.0 | ########
                (0.242 , 0.332) |       14.0 | ##
                (0.332 , 0.423) |        7.0 | #
                (0.423 , 0.513) |        6.0 | #
                (0.513 , 0.603) |        3.0 | 
                (0.603 , 0.693) |        4.0 | 
                (0.693 , 0.783) |        1.0 | 
                (0.783 , 0.874) |        1.0 | 
                (0.874 , 0.964) |        3.0 | 
[I]         Error Metrics: scores
[I]             Minimum Required Tolerance: elemwise error | [abs=0.044752] OR [rel=0.10845] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.0073964, std-dev=0.0064139, var=4.1138e-05, median=0.0054786, min=6.5416e-06 at (0, 71), max=0.044752 at (0, 8), avg-magnitude=0.0073964, p90=0.012923, p95=0.020454, p99=0.034677
[I]                 ---- Histogram ----
                    Bin Range           |  Num Elems | Visualization
                    (6.54e-06, 0.00448) |      103.0 | #################################
                    (0.00448 , 0.00896) |      122.0 | ########################################
                    (0.00896 , 0.0134 ) |       46.0 | ###############
                    (0.0134  , 0.0179 ) |       10.0 | ###
                    (0.0179  , 0.0224 ) |        6.0 | #
                    (0.0224  , 0.0269 ) |        4.0 | #
                    (0.0269  , 0.0313 ) |        5.0 | #
                    (0.0313  , 0.0358 ) |        1.0 | 
                    (0.0358  , 0.0403 ) |        2.0 | 
                    (0.0403  , 0.0448 ) |        1.0 | 
[I]             Relative Difference | Stats: mean=0.052791, std-dev=0.021615, var=0.0004672, median=0.055879, min=3.8137e-05 at (0, 71), max=0.10845 at (0, 117), avg-magnitude=0.052791, p90=0.077591, p95=0.087143, p99=0.094574
[I]                 ---- Histogram ----
                    Bin Range          |  Num Elems | Visualization
                    (3.81e-05, 0.0109) |       13.0 | ########
                    (0.0109  , 0.0217) |       14.0 | ########
                    (0.0217  , 0.0326) |       27.0 | #################
                    (0.0326  , 0.0434) |       45.0 | ############################
                    (0.0434  , 0.0542) |       42.0 | ##########################
                    (0.0542  , 0.0651) |       63.0 | ########################################
                    (0.0651  , 0.0759) |       57.0 | ####################################
                    (0.0759  , 0.0868) |       23.0 | ##############
                    (0.0868  , 0.0976) |       14.0 | ########
                    (0.0976  , 0.108 ) |        2.0 | #
[X]         Finished comparing: 'scores' (dtype=float32, shape=torch.Size([1, 300])) [trt-runner-N8-07/03/25-18:26:23] and 'scores' (dtype=float32, shape=torch.Size([1, 300])) [onnxrt-runner-N8-07/03/25-18:26:23]
[E]         FAILED | Output: 'scores' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)
[E]     FAILED | Mismatched outputs: ['/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0', '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0', '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0', 'labels', 'boxes', 'scores']
[X]     Finished comparing trt-runner-N8-07/03/25-18:26:23 with onnxrt-runner-N8-07/03/25-18:26:23
[E] Accuracy Summary | trt-runner-N8-07/03/25-18:26:23 vs. onnxrt-runner-N8-07/03/25-18:26:23 | Passed: 0/1 iterations | Pass Rate: 0.0%
