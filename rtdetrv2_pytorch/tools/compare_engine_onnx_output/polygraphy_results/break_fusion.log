[96m[INFO] Origianl output [0m
['labels', 'boxes', 'scores']
[38;5;13m[V] Marking all ONNX tensors as outputs[0m
[96m[INFO] output to be compared [0m
['/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0',
 '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0',
 '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0',
 '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0',
 '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0',
 '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0',
 'labels',
 'boxes',
 'scores']
[38;5;14m[I] trt-runner-N2-05/19/25-15:43:09     | Activating and starting inference[0m
[38;5;13m[V] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.[0m
[38;5;104m[X] CUDA lazy loading is enabled.[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Clip_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CoordConvAC version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResizeDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResize version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DetectionLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::FlattenConcat_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GenerateDetection_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchor_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 3[0m
[38;5;104m[X] Plugin creator already registered - ::LReLU_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Normalize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PillarScatterPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PriorBox_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Proposal version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Region_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ResizeNearest_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::RPROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 2[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterND version 1[0m
[38;5;104m[X] Plugin creator already registered - ::SpecialSlice_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Split version 1[0m
[38;5;104m[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1[0m
[38;5;13m[V] ----------------------------------------------------------------[0m
[38;5;13m[V] Input filename:   default_mtq_int8_q_qint8break_fusion-output_modified.onnx[0m
[38;5;13m[V] ONNX IR version:  0.0.8[0m
[38;5;13m[V] Opset version:    17[0m
[38;5;13m[V] Producer name:    pytorch[0m
[38;5;13m[V] Producer version: 2.5.0[0m
[38;5;13m[V] Domain:           [0m
[38;5;13m[V] Model version:    0[0m
[38;5;13m[V] Doc string:       [0m
[38;5;13m[V] ----------------------------------------------------------------[0m
[38;5;104m[X] Adding network input: images with dtype: float32, dimensions: (1, 3, 640, 640)[0m
[38;5;104m[X] Registering tensor: images for ONNX tensor: images[0m
[38;5;104m[X] Adding network input: orig_target_sizes with dtype: int64, dimensions: (1, 2)[0m
[38;5;104m[X] Registering tensor: orig_target_sizes for ONNX tensor: orig_target_sizes[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.anchors[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_score_head.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_score_head.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_score_head.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: onnx::Add_3614[0m
[38;5;104m[X] Importing initializer: onnx::Add_3616[0m
[38;5;104m[X] Importing initializer: onnx::Add_3618[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3619[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3620[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3621[0m
[38;5;104m[X] Importing initializer: onnx::Mul_3692[0m
[38;5;104m[X] Importing initializer: onnx::Add_3731[0m
[38;5;104m[X] Importing initializer: onnx::Add_3733[0m
[38;5;104m[X] Importing initializer: onnx::Add_3735[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3736[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3737[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3738[0m
[38;5;104m[X] Importing initializer: onnx::Mul_3755[0m
[38;5;104m[X] Importing initializer: onnx::Add_3803[0m
[38;5;104m[X] Importing initializer: onnx::Add_3805[0m
[38;5;104m[X] Importing initializer: onnx::Add_3807[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3808[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3809[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3810[0m
[38;5;104m[X] Importing initializer: onnx::Add_3875[0m
[38;5;104m[X] Importing initializer: onnx::Add_3877[0m
[38;5;104m[X] Importing initializer: onnx::Add_3879[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3880[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3881[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3882[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/Constant_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Split_2305[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /postprocessor/Constant_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Tile_3498[0m
[38;5;104m[X] Importing initializer: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] Importing initializer: _v_4326[0m
[38;5;104m[X] Importing initializer: _v_1997[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Concat_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Concat_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Importing initializer: _v_1846[0m
[38;5;104m[X] Importing initializer: _v_1848[0m
[38;5;104m[X] Importing initializer: _v_1850[0m
[38;5;104m[X] Importing initializer: _v_1749[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] Importing initializer: _v_1663[0m
[38;5;104m[X] Importing initializer: _v_1665[0m
[38;5;104m[X] Importing initializer: _v_1669[0m
[38;5;104m[X] Importing initializer: _v_1675[0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: images[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [images -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight for ONNX node: tmp_weight[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_0 for ONNX node: tmp_weight_0[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_1.conv.weight -> (32, 3, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1 for ONNX node: tmp_weight_1[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_2 for ONNX node: tmp_weight_2[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/Conv for ONNX node: /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_1.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_var -> (32)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/act/Relu for ONNX node: /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_3 for ONNX node: tmp_weight_3[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_4 for ONNX node: tmp_weight_4[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_2.conv.weight -> (32, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_5 for ONNX node: tmp_weight_5[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_6 for ONNX node: tmp_weight_6[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/Conv for ONNX node: /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_2.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_var -> (32)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/act/Relu for ONNX node: /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_7 for ONNX node: tmp_weight_7[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_8 for ONNX node: tmp_weight_8[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_3.conv.weight -> (64, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_9 for ONNX node: tmp_weight_9[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_10 for ONNX node: tmp_weight_10[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/Conv for ONNX node: /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_3.norm.weight -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.bias -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_mean -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/act/Relu for ONNX node: /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/MaxPool [MaxPool][0m
[38;5;104m[X] Parsing node: /model/backbone/MaxPool [MaxPool][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/MaxPool [MaxPool] inputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/MaxPool for ONNX node: /model/backbone/MaxPool[0m
[38;5;104m[X] Registering tensor: /model/backbone/MaxPool_output_0 for ONNX tensor: /model/backbone/MaxPool_output_0[0m
[38;5;104m[X] /model/backbone/MaxPool [MaxPool] outputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/MaxPool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_11 for ONNX node: tmp_weight_11[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_12 for ONNX node: tmp_weight_12[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_13 for ONNX node: tmp_weight_13[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_14 for ONNX node: tmp_weight_14[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_15 for ONNX node: tmp_weight_15[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_16 for ONNX node: tmp_weight_16[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_17 for ONNX node: tmp_weight_17[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_18 for ONNX node: tmp_weight_18[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.short.conv.weight -> (64, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.short.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_19 for ONNX node: tmp_weight_19[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_20 for ONNX node: tmp_weight_20[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/Add for ONNX node: /model/backbone/res_layers.0/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_21 for ONNX node: tmp_weight_21[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_22 for ONNX node: tmp_weight_22[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_23 for ONNX node: tmp_weight_23[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_24 for ONNX node: tmp_weight_24[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_25 for ONNX node: tmp_weight_25[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_26 for ONNX node: tmp_weight_26[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_27 for ONNX node: tmp_weight_27[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_28 for ONNX node: tmp_weight_28[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/Add for ONNX node: /model/backbone/res_layers.0/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_29 for ONNX node: tmp_weight_29[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_30 for ONNX node: tmp_weight_30[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2a.conv.weight -> (128, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_31 for ONNX node: tmp_weight_31[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_32 for ONNX node: tmp_weight_32[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_33 for ONNX node: tmp_weight_33[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_34 for ONNX node: tmp_weight_34[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_35 for ONNX node: tmp_weight_35[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_36 for ONNX node: tmp_weight_36[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_37 for ONNX node: tmp_weight_37[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_38 for ONNX node: tmp_weight_38[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.short.conv.conv.weight -> (128, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_39 for ONNX node: tmp_weight_39[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_40 for ONNX node: tmp_weight_40[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/Add for ONNX node: /model/backbone/res_layers.1/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_41 for ONNX node: tmp_weight_41[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_42 for ONNX node: tmp_weight_42[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2a.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_43 for ONNX node: tmp_weight_43[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_44 for ONNX node: tmp_weight_44[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_45 for ONNX node: tmp_weight_45[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_46 for ONNX node: tmp_weight_46[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_47 for ONNX node: tmp_weight_47[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_48 for ONNX node: tmp_weight_48[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/Add for ONNX node: /model/backbone/res_layers.1/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_49 for ONNX node: tmp_weight_49[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_50 for ONNX node: tmp_weight_50[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2a.conv.weight -> (256, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_51 for ONNX node: tmp_weight_51[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_52 for ONNX node: tmp_weight_52[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_53 for ONNX node: tmp_weight_53[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_54 for ONNX node: tmp_weight_54[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_55 for ONNX node: tmp_weight_55[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_56 for ONNX node: tmp_weight_56[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_57 for ONNX node: tmp_weight_57[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_58 for ONNX node: tmp_weight_58[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.short.conv.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_59 for ONNX node: tmp_weight_59[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_60 for ONNX node: tmp_weight_60[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/Add for ONNX node: /model/backbone/res_layers.2/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_61 for ONNX node: tmp_weight_61[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_62 for ONNX node: tmp_weight_62[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2a.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_63 for ONNX node: tmp_weight_63[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_64 for ONNX node: tmp_weight_64[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_65 for ONNX node: tmp_weight_65[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_66 for ONNX node: tmp_weight_66[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_67 for ONNX node: tmp_weight_67[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_68 for ONNX node: tmp_weight_68[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/Add for ONNX node: /model/backbone/res_layers.2/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_69 for ONNX node: tmp_weight_69[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_70 for ONNX node: tmp_weight_70[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2a.conv.weight -> (512, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_71 for ONNX node: tmp_weight_71[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_72 for ONNX node: tmp_weight_72[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_73 for ONNX node: tmp_weight_73[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_74 for ONNX node: tmp_weight_74[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_75 for ONNX node: tmp_weight_75[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_76 for ONNX node: tmp_weight_76[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_77 for ONNX node: tmp_weight_77[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_78 for ONNX node: tmp_weight_78[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.short.conv.conv.weight -> (512, 256, 1, 1)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_79 for ONNX node: tmp_weight_79[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_80 for ONNX node: tmp_weight_80[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/Add for ONNX node: /model/backbone/res_layers.3/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_81 for ONNX node: tmp_weight_81[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_82 for ONNX node: tmp_weight_82[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2a.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_83 for ONNX node: tmp_weight_83[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_84 for ONNX node: tmp_weight_84[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_85 for ONNX node: tmp_weight_85[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_86 for ONNX node: tmp_weight_86[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_87 for ONNX node: tmp_weight_87[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_88 for ONNX node: tmp_weight_88[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/Add for ONNX node: /model/backbone/res_layers.3/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.0.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_89 for ONNX node: tmp_weight_89[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_90 for ONNX node: tmp_weight_90[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/Conv for ONNX node: /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/Conv [Conv] outputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_91 for ONNX node: tmp_weight_91[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_92 for ONNX node: tmp_weight_92[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/Conv for ONNX node: /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/Conv [Conv] outputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_93 for ONNX node: tmp_weight_93[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_94 for ONNX node: tmp_weight_94[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.2.conv.weight -> (256, 512, 1, 1)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_95 for ONNX node: tmp_weight_95[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_96 for ONNX node: tmp_weight_96[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/Conv [Conv] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/Conv for ONNX node: /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/Conv [Conv] outputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/encoder/Reshape [Reshape] inputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/Reshape for ONNX node: /model/encoder/Reshape[0m
[38;5;104m[X] Registering tensor: /model/encoder/Reshape_output_0 for ONNX tensor: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/Reshape [Reshape] outputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/Transpose [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Transpose for ONNX node: /model/encoder/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/Transpose_output_0 for ONNX tensor: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/Transpose [Transpose] outputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/Constant_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add for ONNX node: /model/encoder/encoder.0/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3619[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3619 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3619 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_97 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3614[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3614 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3614 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_98 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_99 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3620[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3620 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3620 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_100 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_101 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3616[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3616 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3616 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_102 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_103 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3621[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3621 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3621 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_104 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_105 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3618[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3618 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3618 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_106 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_107 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_108 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_109 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_110 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_111 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_112 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_113 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_114 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_115 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_116 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_117 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/Add_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_120 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_121 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_122 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_123 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_124 for ONNX node: tmp_weight_124[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_125 for ONNX node: tmp_weight_125[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_126 for ONNX node: tmp_weight_126[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_127 for ONNX node: tmp_weight_127[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_128 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_129 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_130 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_131 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_132 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_133 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Div for ONNX node: /model/encoder/encoder.0/layers.0/activation/Div[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] outputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Div_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] inputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Erf for ONNX node: /model/encoder/encoder.0/layers.0/activation/Erf[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] outputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Erf_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] inputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_134 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_135 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Add for ONNX node: /model/encoder/encoder.0/layers.0/activation/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_136 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_137 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_138 for ONNX node: tmp_weight_138[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_139 for ONNX node: tmp_weight_139[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_140 for ONNX node: tmp_weight_140[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_141 for ONNX node: tmp_weight_141[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_142 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_143 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear2.bias -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_144 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_145 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/Add_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_2_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_148 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_149 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_150 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_151 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Transpose_1 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Transpose_1 for ONNX node: /model/encoder/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/Transpose_1_output_0 for ONNX tensor: /model/encoder/Transpose_1_output_0[0m
[38;5;104m[X] /model/encoder/Transpose_1 [Transpose] outputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_1_output_0[0m
[38;5;104m[X] /model/encoder/Reshape_1 [Reshape] inputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], [/model/encoder/Concat_1_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_152 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/Reshape_1 for ONNX node: /model/encoder/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/Reshape_1_output_0 for ONNX tensor: /model/encoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/Reshape_1 [Reshape] outputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_153 for ONNX node: tmp_weight_153[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_154 for ONNX node: tmp_weight_154[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.lateral_convs.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_155 for ONNX node: tmp_weight_155[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_156 for ONNX node: tmp_weight_156[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/Conv for ONNX node: /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.lateral_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/act/Mul for ONNX node: /model/encoder/lateral_convs.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Resize [Resize][0m
[38;5;104m[X] Parsing node: /model/encoder/Resize [Resize][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] /model/encoder/Resize [Resize] inputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Resize for ONNX node: /model/encoder/Resize[0m
[38;5;104m[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest[0m
[38;5;104m[X] Registering tensor: /model/encoder/Resize_output_0 for ONNX tensor: /model/encoder/Resize_output_0[0m
[38;5;104m[X] /model/encoder/Resize [Resize] outputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_2 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_2 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/Resize_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Concat_2 [Concat] inputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_2 for ONNX node: /model/encoder/Concat_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_2_output_0 for ONNX tensor: /model/encoder/Concat_2_output_0[0m
[38;5;104m[X] /model/encoder/Concat_2 [Concat] outputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_157 for ONNX node: tmp_weight_157[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_158 for ONNX node: tmp_weight_158[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_159 for ONNX node: tmp_weight_159[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_160 for ONNX node: tmp_weight_160[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_161 for ONNX node: tmp_weight_161[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_162 for ONNX node: tmp_weight_162[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_163 for ONNX node: tmp_weight_163[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_164 for ONNX node: tmp_weight_164[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_165 for ONNX node: tmp_weight_165[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_166 for ONNX node: tmp_weight_166[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_167 for ONNX node: tmp_weight_167[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_168 for ONNX node: tmp_weight_168[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_169 for ONNX node: tmp_weight_169[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_170 for ONNX node: tmp_weight_170[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_171 for ONNX node: tmp_weight_171[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_172 for ONNX node: tmp_weight_172[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_173 for ONNX node: tmp_weight_173[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_174 for ONNX node: tmp_weight_174[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/Add [Add] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/Add for ONNX node: /model/encoder/fpn_blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/Add [Add] outputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_175 for ONNX node: tmp_weight_175[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_176 for ONNX node: tmp_weight_176[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_177 for ONNX node: tmp_weight_177[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_178 for ONNX node: tmp_weight_178[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_179 for ONNX node: tmp_weight_179[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_180 for ONNX node: tmp_weight_180[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.lateral_convs.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_181 for ONNX node: tmp_weight_181[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_182 for ONNX node: tmp_weight_182[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/Conv for ONNX node: /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.lateral_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/act/Mul for ONNX node: /model/encoder/lateral_convs.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Resize_1 [Resize][0m
[38;5;104m[X] Parsing node: /model/encoder/Resize_1 [Resize][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] /model/encoder/Resize_1 [Resize] inputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Resize_1 for ONNX node: /model/encoder/Resize_1[0m
[38;5;104m[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest[0m
[38;5;104m[X] Registering tensor: /model/encoder/Resize_1_output_0 for ONNX tensor: /model/encoder/Resize_1_output_0[0m
[38;5;104m[X] /model/encoder/Resize_1 [Resize] outputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_3 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_3 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/Resize_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Concat_3 [Concat] inputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_3 for ONNX node: /model/encoder/Concat_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_3_output_0 for ONNX tensor: /model/encoder/Concat_3_output_0[0m
[38;5;104m[X] /model/encoder/Concat_3 [Concat] outputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_183 for ONNX node: tmp_weight_183[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_184 for ONNX node: tmp_weight_184[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_185 for ONNX node: tmp_weight_185[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_186 for ONNX node: tmp_weight_186[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_187 for ONNX node: tmp_weight_187[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_188 for ONNX node: tmp_weight_188[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_189 for ONNX node: tmp_weight_189[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_190 for ONNX node: tmp_weight_190[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_191 for ONNX node: tmp_weight_191[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_192 for ONNX node: tmp_weight_192[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_193 for ONNX node: tmp_weight_193[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_194 for ONNX node: tmp_weight_194[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_195 for ONNX node: tmp_weight_195[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_196 for ONNX node: tmp_weight_196[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_197 for ONNX node: tmp_weight_197[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_198 for ONNX node: tmp_weight_198[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_199 for ONNX node: tmp_weight_199[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_200 for ONNX node: tmp_weight_200[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/Add [Add] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/Add for ONNX node: /model/encoder/fpn_blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/Add_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/Add [Add] outputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_201 for ONNX node: tmp_weight_201[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_202 for ONNX node: tmp_weight_202[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_203 for ONNX node: tmp_weight_203[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_204 for ONNX node: tmp_weight_204[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_205 for ONNX node: tmp_weight_205[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_206 for ONNX node: tmp_weight_206[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.0.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.downsample_convs.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_207 for ONNX node: tmp_weight_207[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_208 for ONNX node: tmp_weight_208[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/Conv for ONNX node: /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.downsample_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/act/Mul for ONNX node: /model/encoder/downsample_convs.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_4 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_4 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/Concat_4 [Concat] inputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_4 for ONNX node: /model/encoder/Concat_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_4_output_0 for ONNX tensor: /model/encoder/Concat_4_output_0[0m
[38;5;104m[X] /model/encoder/Concat_4 [Concat] outputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_4_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_209 for ONNX node: tmp_weight_209[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_210 for ONNX node: tmp_weight_210[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_211 for ONNX node: tmp_weight_211[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_212 for ONNX node: tmp_weight_212[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_213 for ONNX node: tmp_weight_213[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_214 for ONNX node: tmp_weight_214[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_215 for ONNX node: tmp_weight_215[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_216 for ONNX node: tmp_weight_216[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_217 for ONNX node: tmp_weight_217[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_218 for ONNX node: tmp_weight_218[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_219 for ONNX node: tmp_weight_219[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_220 for ONNX node: tmp_weight_220[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_221 for ONNX node: tmp_weight_221[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_222 for ONNX node: tmp_weight_222[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_223 for ONNX node: tmp_weight_223[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_224 for ONNX node: tmp_weight_224[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_225 for ONNX node: tmp_weight_225[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_226 for ONNX node: tmp_weight_226[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/Add [Add] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/Add for ONNX node: /model/encoder/pan_blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/Add [Add] outputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_227 for ONNX node: tmp_weight_227[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_228 for ONNX node: tmp_weight_228[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_229 for ONNX node: tmp_weight_229[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_230 for ONNX node: tmp_weight_230[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_231 for ONNX node: tmp_weight_231[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_232 for ONNX node: tmp_weight_232[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.1.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.downsample_convs.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_233 for ONNX node: tmp_weight_233[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_234 for ONNX node: tmp_weight_234[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/Conv for ONNX node: /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.downsample_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/act/Mul for ONNX node: /model/encoder/downsample_convs.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_5 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_5 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/Concat_5 [Concat] inputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_5 for ONNX node: /model/encoder/Concat_5[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_5_output_0 for ONNX tensor: /model/encoder/Concat_5_output_0[0m
[38;5;104m[X] /model/encoder/Concat_5 [Concat] outputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_235 for ONNX node: tmp_weight_235[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_236 for ONNX node: tmp_weight_236[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_237 for ONNX node: tmp_weight_237[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_238 for ONNX node: tmp_weight_238[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_239 for ONNX node: tmp_weight_239[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_240 for ONNX node: tmp_weight_240[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_241 for ONNX node: tmp_weight_241[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_242 for ONNX node: tmp_weight_242[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_243 for ONNX node: tmp_weight_243[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_244 for ONNX node: tmp_weight_244[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_245 for ONNX node: tmp_weight_245[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_246 for ONNX node: tmp_weight_246[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_247 for ONNX node: tmp_weight_247[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_248 for ONNX node: tmp_weight_248[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_249 for ONNX node: tmp_weight_249[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_250 for ONNX node: tmp_weight_250[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_251 for ONNX node: tmp_weight_251[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_252 for ONNX node: tmp_weight_252[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/Add [Add] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/Add for ONNX node: /model/encoder/pan_blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/Add_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/Add [Add] outputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_253 for ONNX node: tmp_weight_253[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_254 for ONNX node: tmp_weight_254[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_255 for ONNX node: tmp_weight_255[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_256 for ONNX node: tmp_weight_256[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_257 for ONNX node: tmp_weight_257[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_258 for ONNX node: tmp_weight_258[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/Conv for ONNX node: /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/Conv [Conv] outputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.decoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_259 for ONNX node: tmp_weight_259[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_260 for ONNX node: tmp_weight_260[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/Conv for ONNX node: /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/Conv [Conv] outputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.decoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_261 for ONNX node: tmp_weight_261[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_262 for ONNX node: tmp_weight_262[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.2.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_263 for ONNX node: tmp_weight_263[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_264 for ONNX node: tmp_weight_264[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/Conv [Conv] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/Conv for ONNX node: /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/Conv [Conv] outputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.decoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape [Reshape] inputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_265 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape for ONNX node: /model/decoder/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_output_0 for ONNX tensor: /model/decoder/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/Reshape [Reshape] outputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/Transpose [Transpose] inputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose for ONNX node: /model/decoder/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_output_0 for ONNX tensor: /model/decoder/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/Transpose [Transpose] outputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape_1 [Reshape] inputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_266 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape_1 for ONNX node: /model/decoder/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_1_output_0 for ONNX tensor: /model/decoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/Reshape_1 [Reshape] outputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_1 [Transpose] inputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose_1 for ONNX node: /model/decoder/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_1_output_0 for ONNX tensor: /model/decoder/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_1 [Transpose] outputs: [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape_2 [Reshape] inputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_267 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape_2 for ONNX node: /model/decoder/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_2_output_0 for ONNX tensor: /model/decoder/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/Reshape_2 [Reshape] outputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_2 [Transpose] inputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose_2 for ONNX node: /model/decoder/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_2_output_0 for ONNX tensor: /model/decoder/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_2 [Transpose] outputs: [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Concat_3 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/Concat_3 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/Concat_3 [Concat] inputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Concat_3 for ONNX node: /model/decoder/Concat_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/Concat_3_output_0 for ONNX tensor: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] /model/decoder/Concat_3 [Concat] outputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/Mul [Mul][0m
[38;5;104m[X] Searching for input: onnx::Mul_3692[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] /model/decoder/Mul [Mul] inputs: [onnx::Mul_3692 -> (1, 8400, 1)[FLOAT]], [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Mul_3692 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Mul for ONNX node: /model/decoder/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/Mul_output_0 for ONNX tensor: /model/decoder/Mul_output_0[0m
[38;5;104m[X] /model/decoder/Mul [Mul] outputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_268 for ONNX node: tmp_weight_268[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_269 for ONNX node: tmp_weight_269[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_output.proj.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_270 for ONNX node: tmp_weight_270[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_271 for ONNX node: tmp_weight_271[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Transpose [Transpose] inputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/Transpose for ONNX node: /model/decoder/enc_output/proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/Transpose_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Transpose [Transpose] outputs: [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/MatMul [MatMul] inputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_272 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_273 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/MatMul for ONNX node: /model/decoder/enc_output/proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/MatMul_output_0 for ONNX tensor: /model/decoder/enc_output/proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/MatMul [MatMul] outputs: [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Add [Add] inputs: [model.decoder.enc_output.proj.bias -> (256)[FLOAT]], [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_274 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_275 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/Add for ONNX node: /model/decoder/enc_output/proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/Add_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Add [Add] outputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/Add_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] inputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [model.decoder.enc_output.norm.weight -> (256)[FLOAT]], [model.decoder.enc_output.norm.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.norm.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.norm.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_278 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_279 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_280 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_281 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/norm/LayerNormalization for ONNX node: /model/decoder/enc_output/norm/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0 for ONNX tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] outputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_282 for ONNX node: tmp_weight_282[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_283 for ONNX node: tmp_weight_283[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_score_head.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_score_head.weight -> (80, 256)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_score_head.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_284 for ONNX node: tmp_weight_284[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_285 for ONNX node: tmp_weight_285[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Transpose [Transpose] inputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/Transpose for ONNX node: /model/decoder/enc_score_head/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/Transpose_output_0 for ONNX tensor: /model/decoder/enc_score_head/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Transpose [Transpose] outputs: [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_286 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_287 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/MatMul for ONNX node: /model/decoder/enc_score_head/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/MatMul_output_0 for ONNX tensor: /model/decoder/enc_score_head/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/MatMul [MatMul] outputs: [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_score_head.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Add [Add] inputs: [model.decoder.enc_score_head.bias -> (80)[FLOAT]], [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_score_head.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_288 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_289 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/Add for ONNX node: /model/decoder/enc_score_head/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/Add_output_0 for ONNX tensor: /model/decoder/enc_score_head/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Add [Add] outputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_290 for ONNX node: tmp_weight_290[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_291 for ONNX node: tmp_weight_291[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_292 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_293 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.0.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_294 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_295 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Add for ONNX node: /model/decoder/enc_bbox_head/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/act/Relu for ONNX node: /model/decoder/enc_bbox_head/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/act/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_296 for ONNX node: tmp_weight_296[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_297 for ONNX node: tmp_weight_297[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_298 for ONNX node: tmp_weight_298[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_299 for ONNX node: tmp_weight_299[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_300 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_301 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_302 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_303 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Add for ONNX node: /model/decoder/enc_bbox_head/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/act_1/Relu for ONNX node: /model/decoder/enc_bbox_head/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_304 for ONNX node: tmp_weight_304[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_305 for ONNX node: tmp_weight_305[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_306 for ONNX node: tmp_weight_306[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_307 for ONNX node: tmp_weight_307[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_308 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_309 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.2.bias -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_310 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_311 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Add for ONNX node: /model/decoder/enc_bbox_head/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.anchors[0m
[38;5;104m[X] /model/decoder/Add [Add] inputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], [model.decoder.anchors -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.anchors required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Add for ONNX node: /model/decoder/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/Add_output_0 for ONNX tensor: /model/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/Add [Add] outputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/ReduceMax [ReduceMax][0m
[38;5;104m[X] Parsing node: /model/decoder/ReduceMax [ReduceMax][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/Add_output_0[0m
[38;5;104m[X] /model/decoder/ReduceMax [ReduceMax] inputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/ReduceMax for ONNX node: /model/decoder/ReduceMax[0m
[38;5;104m[X] Registering tensor: /model/decoder/ReduceMax_output_0 for ONNX tensor: /model/decoder/ReduceMax_output_0[0m
[38;5;104m[X] /model/decoder/ReduceMax [ReduceMax] outputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/TopK [TopK][0m
[38;5;104m[X] Parsing node: /model/decoder/TopK [TopK][0m
[38;5;104m[X] Searching for input: /model/decoder/ReduceMax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] /model/decoder/TopK [TopK] inputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Constant_18_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_convertToScalar required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/TopK for ONNX node: /model/decoder/TopK[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/TopK_output_0 for ONNX tensor: /model/decoder/TopK_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/TopK_output_1 for ONNX tensor: /model/decoder/TopK_output_1[0m
[38;5;104m[X] /model/decoder/TopK [TopK] outputs: [/model/decoder/TopK_output_0 -> (1, 300)[FLOAT]], [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/TopK_output_1[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/Unsqueeze [Unsqueeze] inputs: [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Constant_7_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Unsqueeze for ONNX node: /model/decoder/Unsqueeze[0m
[38;5;104m[X] Registering tensor: /model/decoder/Unsqueeze_output_0 for ONNX tensor: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] /model/decoder/Unsqueeze [Unsqueeze] outputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Tile [Tile][0m
[38;5;104m[X] Parsing node: /model/decoder/Tile [Tile][0m
[38;5;104m[X] Searching for input: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] /model/decoder/Tile [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Tile for ONNX node: /model/decoder/Tile[0m
[38;5;104m[X] Registering tensor: /model/decoder/Tile_output_0 for ONNX tensor: /model/decoder/Tile_output_0[0m
[38;5;104m[X] /model/decoder/Tile [Tile] outputs: [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/GatherElements [GatherElements][0m
[38;5;104m[X] Parsing node: /model/decoder/GatherElements [GatherElements][0m
[38;5;104m[X] Searching for input: /model/decoder/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Tile_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements [GatherElements] inputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_312 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/GatherElements for ONNX node: /model/decoder/GatherElements[0m
[38;5;104m[X] Registering tensor: /model/decoder/GatherElements_output_0 for ONNX tensor: /model/decoder/GatherElements_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements [GatherElements] outputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Tile_1 [Tile][0m
[38;5;104m[X] Parsing node: /model/decoder/Tile_1 [Tile][0m
[38;5;104m[X] Searching for input: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/Tile_1 [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_7_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_313 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Tile_1 for ONNX node: /model/decoder/Tile_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Tile_1_output_0 for ONNX tensor: /model/decoder/Tile_1_output_0[0m
[38;5;104m[X] /model/decoder/Tile_1 [Tile] outputs: [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/GatherElements_1 [GatherElements][0m
[38;5;104m[X] Parsing node: /model/decoder/GatherElements_1 [GatherElements][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Tile_1_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements_1 [GatherElements] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_314 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/GatherElements_1 for ONNX node: /model/decoder/GatherElements_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/GatherElements_1_output_0 for ONNX tensor: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements_1 [GatherElements] outputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid [Sigmoid] inputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid for ONNX node: /model/decoder/decoder/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_315 for ONNX node: tmp_weight_315[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_316 for ONNX node: tmp_weight_316[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.0.weight -> (512, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_317 for ONNX node: tmp_weight_317[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_318 for ONNX node: tmp_weight_318[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_319 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_320 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_321 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_322 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_323 for ONNX node: tmp_weight_323[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_324 for ONNX node: tmp_weight_324[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.1.weight -> (256, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_325 for ONNX node: tmp_weight_325[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_326 for ONNX node: tmp_weight_326[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_327 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_328 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_329 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_330 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add for ONNX node: /model/decoder/decoder/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add [Add] outputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3736[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3736 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3736 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_331 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_332 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3731[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3731 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3731 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_333 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_334 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3737[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3737 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3737 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_335 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_336 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3733[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3733 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3733 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_337 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_338 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3738[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3738 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3738 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_339 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_340 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3735[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3735 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3735 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_341 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_342 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_343 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_344 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_345 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_346 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_347 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_348 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_349 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.0/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_350 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_351 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_352 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_1 [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_1 for ONNX node: /model/decoder/decoder/layers.0/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_355 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_356 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_357 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_358 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_2 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_2 for ONNX node: /model/decoder/decoder/layers.0/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_359 for ONNX node: tmp_weight_359[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_360 for ONNX node: tmp_weight_360[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_361 for ONNX node: tmp_weight_361[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_362 for ONNX node: tmp_weight_362[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_363 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_364 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_365 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_366 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_367 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_368 for ONNX node: tmp_weight_368[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_369 for ONNX node: tmp_weight_369[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_370 for ONNX node: tmp_weight_370[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_371 for ONNX node: tmp_weight_371[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_372 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_373 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_374 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_375 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0_376 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_377 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0_378 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_379 for ONNX node: tmp_weight_379[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_380 for ONNX node: tmp_weight_380[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_381 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_382 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_383 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_384 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0_385 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_386 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0_387 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_388 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Mul_3755 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_389 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_390 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: _v_1997 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_391 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_392 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_395 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_396 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_398 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_399 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_400 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_402 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_403 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_404 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_406 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_407 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_409 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_410 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_411 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_412 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_414 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_415 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_416 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_417 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_419 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_420 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_421 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_422 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_423 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: onnx::Unsqueeze_1255 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_424 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_425 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_427 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_428 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_430 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_431 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_433 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_434 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_435 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_437 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_438 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_439 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_441 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_442 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_444 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_445 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_446 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_447 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_449 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_450 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_451 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_452 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_454 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_455 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_456 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_457 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_458 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_459 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_461 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_462 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_464 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_465 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_467 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_468 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_469 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_471 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_472 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_473 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_475 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_476 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_478 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_479 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_480 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_481 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_483 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_484 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_485 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_486 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_488 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_489 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_490 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_491 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_492 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_494 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_495 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_497 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_498 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_500 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_501 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_502 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_504 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_505 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_506 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_508 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_509 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_511 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_512 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_513 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_514 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_516 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_517 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_518 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_519 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_521 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_522 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_523 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_524 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_525 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_527 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_528 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_530 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_531 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_533 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_534 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_535 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_537 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_538 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_539 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_541 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_542 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_544 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_545 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_546 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_547 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_549 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_550 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_551 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_552 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_554 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_555 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_556 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_557 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_558 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_559 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_560 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_561 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_562 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_563 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_564 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_565 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_566 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_567 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_568 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_569 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_570 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_571 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_572 for ONNX node: tmp_weight_572[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_573 for ONNX node: tmp_weight_573[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_574 for ONNX node: tmp_weight_574[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_575 for ONNX node: tmp_weight_575[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_576 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_577 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_578 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_579 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_3 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_3 for ONNX node: /model/decoder/decoder/layers.0/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_3 [Add] outputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_582 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_583 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_584 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_585 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_586 for ONNX node: tmp_weight_586[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_587 for ONNX node: tmp_weight_587[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_588 for ONNX node: tmp_weight_588[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_589 for ONNX node: tmp_weight_589[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_590 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_591 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Add [Add] inputs: [model.decoder.decoder.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_592 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_593 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Add for ONNX node: /model/decoder/decoder/layers.0/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/activation/Relu for ONNX node: /model/decoder/decoder/layers.0/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_594 for ONNX node: tmp_weight_594[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_595 for ONNX node: tmp_weight_595[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_596 for ONNX node: tmp_weight_596[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_597 for ONNX node: tmp_weight_597[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_598 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_599 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Add [Add] inputs: [model.decoder.decoder.layers.0.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_600 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_601 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Add for ONNX node: /model/decoder/decoder/layers.0/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_4 [Add] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_4 for ONNX node: /model/decoder/decoder/layers.0/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_4 [Add] outputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_604 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_605 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_606 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_607 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_608 for ONNX node: tmp_weight_608[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_609 for ONNX node: tmp_weight_609[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_610 for ONNX node: tmp_weight_610[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_611 for ONNX node: tmp_weight_611[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_612 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_613 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_614 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_615 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_616 for ONNX node: tmp_weight_616[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_617 for ONNX node: tmp_weight_617[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_618 for ONNX node: tmp_weight_618[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_619 for ONNX node: tmp_weight_619[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_620 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_621 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_622 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_623 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_624 for ONNX node: tmp_weight_624[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_625 for ONNX node: tmp_weight_625[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_626 for ONNX node: tmp_weight_626[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_627 for ONNX node: tmp_weight_627[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_628 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_629 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_630 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_631 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip [Clip] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip for ONNX node: /model/decoder/decoder/Clip[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_output_0 for ONNX tensor: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip [Clip] outputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_1 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_1 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_1 [Clip] inputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Constant_3_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_633 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_634 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_635 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_636 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_1_output_0 for ONNX tensor: /model/decoder/decoder/Clip_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_1 [Clip] outputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_637 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_638 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub for ONNX node: /model/decoder/decoder/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_output_0 for ONNX tensor: /model/decoder/decoder/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub [Sub] outputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_2 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_2 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_2 [Clip] inputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_640 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_641 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_642 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_643 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_2_output_0 for ONNX tensor: /model/decoder/decoder/Clip_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_2 [Clip] outputs: [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div [Div] inputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div for ONNX node: /model/decoder/decoder/Div[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_output_0 for ONNX tensor: /model/decoder/decoder/Div_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div [Div] outputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log [Log] inputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log for ONNX node: /model/decoder/decoder/Log[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_output_0 for ONNX tensor: /model/decoder/decoder/Log_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log [Log] outputs: [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add [Add] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add for ONNX node: /model/decoder/decoder/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_output_0 for ONNX tensor: /model/decoder/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add [Add] outputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] inputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_1 for ONNX node: /model/decoder/decoder/Sigmoid_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_1_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_644 for ONNX node: tmp_weight_644[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_645 for ONNX node: tmp_weight_645[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_646 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_647 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_648 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_649 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act_1/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_650 for ONNX node: tmp_weight_650[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_651 for ONNX node: tmp_weight_651[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_652 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_653 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_654 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_655 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add for ONNX node: /model/decoder/decoder/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add [Add] outputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3808[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3808 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3808 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_656 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_657 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3803[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] inputs: [onnx::Add_3803 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3803 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_658 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_659 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3809[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3809 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3809 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_660 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_661 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3805[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] inputs: [onnx::Add_3805 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3805 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_662 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_663 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3810[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3810 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3810 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_664 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_665 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3807[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] inputs: [onnx::Add_3807 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3807 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_666 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_667 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_668 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_669 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_670 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_671 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_672 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_673 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_674 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.1/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_675 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_676 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: _v_1675[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1675 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_677 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_1 [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_1 for ONNX node: /model/decoder/decoder/layers.1/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_680 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_681 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_682 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_683 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_2 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_2 for ONNX node: /model/decoder/decoder/layers.1/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_684 for ONNX node: tmp_weight_684[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_685 for ONNX node: tmp_weight_685[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_686 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_687 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_688 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_689 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_690 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_691 for ONNX node: tmp_weight_691[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_692 for ONNX node: tmp_weight_692[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_693 for ONNX node: tmp_weight_693[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_694 for ONNX node: tmp_weight_694[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_695 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_696 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_697 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_698 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0_699 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_700 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0_701 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_702 for ONNX node: tmp_weight_702[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_703 for ONNX node: tmp_weight_703[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_704 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_705 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_706 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_707 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0_708 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_709 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0_710 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_711 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_712 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_713 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_714 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_715 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_717 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_718 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_720 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_721 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_723 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_724 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_725 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_727 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_728 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_729 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_731 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_732 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_734 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_735 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_736 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_737 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_739 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_740 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_741 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_742 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_744 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_745 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_746 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_747 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_748 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_749 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_750 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_752 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_753 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_755 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_756 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_758 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_759 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_760 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_762 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_763 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_764 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_766 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_767 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_769 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_770 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_771 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_772 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_774 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_775 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_776 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_777 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_779 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_780 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_781 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_782 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_783 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_784 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_786 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_787 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_789 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_790 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_792 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_793 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_794 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_796 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_797 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_798 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_800 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_801 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_803 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_804 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_805 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_806 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_808 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_809 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_810 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_811 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_813 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_814 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_815 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_816 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_817 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_819 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_820 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_822 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_823 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_825 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_826 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_827 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_829 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_830 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_831 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_833 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_834 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_836 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_837 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_838 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_839 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_841 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_842 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_843 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_844 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_846 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_847 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_848 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_849 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_850 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_852 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_853 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_855 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_856 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_858 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_859 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_860 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_862 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_863 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_864 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_866 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_867 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_869 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_870 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_871 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_872 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_874 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_875 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_876 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_877 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_879 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_880 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_881 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_882 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_883 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_884 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_885 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_886 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_887 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_888 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_889 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_890 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_891 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_892 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_893 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_894 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_895 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_896 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_897 for ONNX node: tmp_weight_897[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_898 for ONNX node: tmp_weight_898[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_899 for ONNX node: tmp_weight_899[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_900 for ONNX node: tmp_weight_900[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_901 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_902 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_903 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_904 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_3 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_3 for ONNX node: /model/decoder/decoder/layers.1/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_3 [Add] outputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_907 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_908 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_909 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_910 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_911 for ONNX node: tmp_weight_911[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_912 for ONNX node: tmp_weight_912[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_913 for ONNX node: tmp_weight_913[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_914 for ONNX node: tmp_weight_914[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_915 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_916 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Add [Add] inputs: [model.decoder.decoder.layers.1.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_917 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_918 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Add for ONNX node: /model/decoder/decoder/layers.1/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/activation/Relu for ONNX node: /model/decoder/decoder/layers.1/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_919 for ONNX node: tmp_weight_919[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_920 for ONNX node: tmp_weight_920[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_921 for ONNX node: tmp_weight_921[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_922 for ONNX node: tmp_weight_922[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_923 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_924 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Add [Add] inputs: [model.decoder.decoder.layers.1.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_925 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_926 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Add for ONNX node: /model/decoder/decoder/layers.1/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_4 [Add] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_4 for ONNX node: /model/decoder/decoder/layers.1/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_4 [Add] outputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_929 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_930 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_931 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_932 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_933 for ONNX node: tmp_weight_933[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_934 for ONNX node: tmp_weight_934[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_935 for ONNX node: tmp_weight_935[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_936 for ONNX node: tmp_weight_936[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_937 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_938 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_939 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_940 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_941 for ONNX node: tmp_weight_941[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_942 for ONNX node: tmp_weight_942[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_943 for ONNX node: tmp_weight_943[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_944 for ONNX node: tmp_weight_944[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_945 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_946 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_947 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_948 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_949 for ONNX node: tmp_weight_949[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_950 for ONNX node: tmp_weight_950[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_951 for ONNX node: tmp_weight_951[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_952 for ONNX node: tmp_weight_952[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_953 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_954 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_955 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_956 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_3 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_3 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_3 [Clip] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip_3 for ONNX node: /model/decoder/decoder/Clip_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_3_output_0 for ONNX tensor: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_3 [Clip] outputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_4 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_4 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_4 [Clip] inputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_958 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_959 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_960 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_961 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_4_output_0 for ONNX tensor: /model/decoder/decoder/Clip_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_4 [Clip] outputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub_1 [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub_1 [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_1 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_962 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_963 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub_1 for ONNX node: /model/decoder/decoder/Sub_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_1_output_0 for ONNX tensor: /model/decoder/decoder/Sub_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_1 [Sub] outputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_5 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_5 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_5 [Clip] inputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_965 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_966 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_967 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_968 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_5_output_0 for ONNX tensor: /model/decoder/decoder/Clip_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_5 [Clip] outputs: [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div_1 [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div_1 [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_1 [Div] inputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div_1 for ONNX node: /model/decoder/decoder/Div_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_1_output_0 for ONNX tensor: /model/decoder/decoder/Div_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_1 [Div] outputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log_1 [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log_1 [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_1 [Log] inputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log_1 for ONNX node: /model/decoder/decoder/Log_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_1_output_0 for ONNX tensor: /model/decoder/decoder/Log_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_1 [Log] outputs: [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_1 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add_1 for ONNX node: /model/decoder/decoder/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_1 [Add] outputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] inputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_2 for ONNX node: /model/decoder/decoder/Sigmoid_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_2_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_969 for ONNX node: tmp_weight_969[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_970 for ONNX node: tmp_weight_970[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_971 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_972 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_973 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_974 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act_2/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_2/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_975 for ONNX node: tmp_weight_975[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_976 for ONNX node: tmp_weight_976[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_977 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_978 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_979 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_980 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add for ONNX node: /model/decoder/decoder/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add [Add] outputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3880[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3880 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3880 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_981 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_982 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3875[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] inputs: [onnx::Add_3875 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3875 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_983 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_984 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3881[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3881 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3881 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_985 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_986 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3877[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] inputs: [onnx::Add_3877 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3877 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_987 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_988 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3882[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3882 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3882 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_989 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_990 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3879[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] inputs: [onnx::Add_3879 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3879 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_991 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_992 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_993 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_994 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_995 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_996 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_997 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_998 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_999 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.2/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1000 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1001 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: _v_1675[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1675 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1002 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_1 [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_1 for ONNX node: /model/decoder/decoder/layers.2/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1005 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1006 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1007 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1008 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_2 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_2 for ONNX node: /model/decoder/decoder/layers.2/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1009 for ONNX node: tmp_weight_1009[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1010 for ONNX node: tmp_weight_1010[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1011 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1012 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1013 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1014 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1015 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1016 for ONNX node: tmp_weight_1016[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1017 for ONNX node: tmp_weight_1017[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1018 for ONNX node: tmp_weight_1018[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1019 for ONNX node: tmp_weight_1019[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1020 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1021 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1022 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1023 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0_1024 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1025 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0_1026 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1027 for ONNX node: tmp_weight_1027[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1028 for ONNX node: tmp_weight_1028[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1029 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1030 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1031 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1032 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0_1033 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1034 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0_1035 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1036 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1037 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1038 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1039 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1040 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1042 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1043 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1045 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1046 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1048 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1049 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1050 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1052 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1053 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1054 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1056 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1057 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1059 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1060 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1061 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1062 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1064 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1065 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1066 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1067 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1069 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1070 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1071 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1072 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1073 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1074 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1075 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1077 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1078 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1080 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1081 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1083 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1084 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1085 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1087 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1088 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1089 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1091 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1092 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1094 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1095 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1096 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1097 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1099 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1100 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1101 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1102 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1104 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1105 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1106 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1107 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1108 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1109 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1111 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1112 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1114 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1115 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1117 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1118 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1119 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1121 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1122 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1123 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1125 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1126 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1128 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1129 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1130 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1131 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1133 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1134 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1135 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1136 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1138 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1139 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1140 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1141 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1142 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1144 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1145 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1147 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1148 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1150 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1151 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1152 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1154 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1155 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1156 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1158 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1159 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1161 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1162 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1163 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1164 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1166 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1167 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1168 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1169 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1171 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1172 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1173 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1174 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1175 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1177 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1178 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1180 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1181 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1183 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1184 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1185 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1187 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1188 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1189 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1191 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1192 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1194 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1195 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1196 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1197 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1199 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1200 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1201 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1202 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1204 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1205 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1206 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1207 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1208 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1209 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1210 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1211 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1212 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1213 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1214 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1215 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1216 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1217 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1218 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1219 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1220 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1221 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1222 for ONNX node: tmp_weight_1222[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1223 for ONNX node: tmp_weight_1223[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1224 for ONNX node: tmp_weight_1224[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1225 for ONNX node: tmp_weight_1225[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1226 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1227 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1228 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1229 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_3 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_3 for ONNX node: /model/decoder/decoder/layers.2/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_3 [Add] outputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1232 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1233 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1234 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1235 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1236 for ONNX node: tmp_weight_1236[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1237 for ONNX node: tmp_weight_1237[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1238 for ONNX node: tmp_weight_1238[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1239 for ONNX node: tmp_weight_1239[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1240 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1241 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Add [Add] inputs: [model.decoder.decoder.layers.2.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1242 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1243 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Add for ONNX node: /model/decoder/decoder/layers.2/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/activation/Relu for ONNX node: /model/decoder/decoder/layers.2/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1244 for ONNX node: tmp_weight_1244[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1245 for ONNX node: tmp_weight_1245[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1246 for ONNX node: tmp_weight_1246[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1247 for ONNX node: tmp_weight_1247[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1248 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1249 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Add [Add] inputs: [model.decoder.decoder.layers.2.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1250 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1251 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Add for ONNX node: /model/decoder/decoder/layers.2/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_4 [Add] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_4 for ONNX node: /model/decoder/decoder/layers.2/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_4 [Add] outputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1254 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1255 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1256 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1257 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1258 for ONNX node: tmp_weight_1258[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1259 for ONNX node: tmp_weight_1259[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1260 for ONNX node: tmp_weight_1260[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1261 for ONNX node: tmp_weight_1261[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1262 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1263 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1264 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1265 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1266 for ONNX node: tmp_weight_1266[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1267 for ONNX node: tmp_weight_1267[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1268 for ONNX node: tmp_weight_1268[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1269 for ONNX node: tmp_weight_1269[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1270 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1271 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1272 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1273 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1274 for ONNX node: tmp_weight_1274[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1275 for ONNX node: tmp_weight_1275[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1276 for ONNX node: tmp_weight_1276[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1277 for ONNX node: tmp_weight_1277[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1278 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1279 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1280 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1281 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_6 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_6 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_6 [Clip] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip_6 for ONNX node: /model/decoder/decoder/Clip_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_6_output_0 for ONNX tensor: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_6 [Clip] outputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_7 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_7 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_7 [Clip] inputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1283 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1284 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1285 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1286 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_7_output_0 for ONNX tensor: /model/decoder/decoder/Clip_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_7 [Clip] outputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub_2 [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub_2 [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_2 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1287 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1288 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub_2 for ONNX node: /model/decoder/decoder/Sub_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_2_output_0 for ONNX tensor: /model/decoder/decoder/Sub_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_2 [Sub] outputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_8 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_8 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_8 [Clip] inputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1290 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1291 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1292 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1293 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_8_output_0 for ONNX tensor: /model/decoder/decoder/Clip_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_8 [Clip] outputs: [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div_2 [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div_2 [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_2 [Div] inputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div_2 for ONNX node: /model/decoder/decoder/Div_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_2_output_0 for ONNX tensor: /model/decoder/decoder/Div_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_2 [Div] outputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log_2 [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log_2 [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_2 [Log] inputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log_2 for ONNX node: /model/decoder/decoder/Log_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_2_output_0 for ONNX tensor: /model/decoder/decoder/Log_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_2 [Log] outputs: [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_2 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add_2 for ONNX node: /model/decoder/decoder/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_2 [Add] outputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] inputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_3 for ONNX node: /model/decoder/decoder/Sigmoid_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_3_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_score_head.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_score_head.2.weight -> (80, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_score_head.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1294 for ONNX node: tmp_weight_1294[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1295 for ONNX node: tmp_weight_1295[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Transpose for ONNX node: /model/decoder/decoder/dec_score_head.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1296 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1297 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/MatMul for ONNX node: /model/decoder/decoder/dec_score_head.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Add [Add] inputs: [model.decoder.dec_score_head.2.bias -> (80)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_score_head.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1298 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1299 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Add for ONNX node: /model/decoder/decoder/dec_score_head.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Add [Add] outputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_3_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Unsqueeze_3 for ONNX node: /model/decoder/decoder/Unsqueeze_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Unsqueeze_3_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Add_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] inputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Unsqueeze_4 for ONNX node: /model/decoder/decoder/Unsqueeze_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Unsqueeze_4_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Gather_8 [Gather][0m
[38;5;104m[X] Parsing node: /model/decoder/Gather_8 [Gather][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Unsqueeze_4_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/Gather_8 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Constant_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Using Gather axis: 0[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1300 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Gather_8 for ONNX node: /model/decoder/Gather_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/Gather_8_output_0 for ONNX tensor: /model/decoder/Gather_8_output_0[0m
[38;5;104m[X] /model/decoder/Gather_8 [Gather] outputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Gather_9 [Gather][0m
[38;5;104m[X] Parsing node: /model/decoder/Gather_9 [Gather][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Unsqueeze_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/Gather_9 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Using Gather axis: 0[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1301 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Gather_9 for ONNX node: /model/decoder/Gather_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/Gather_9_output_0 for ONNX tensor: /model/decoder/Gather_9_output_0[0m
[38;5;104m[X] /model/decoder/Gather_9 [Gather] outputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Split [Split][0m
[38;5;104m[X] Parsing node: /postprocessor/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/Gather_9_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_output_0[0m
[38;5;104m[X] /postprocessor/Split [Split] inputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Constant_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1302 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1303 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1304 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1305 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1306 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1307 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1308 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_0 for ONNX tensor: /postprocessor/Split_output_0[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_1 for ONNX tensor: /postprocessor/Split_output_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_2 for ONNX tensor: /postprocessor/Split_output_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_3 for ONNX tensor: /postprocessor/Split_output_3[0m
[38;5;104m[X] /postprocessor/Split [Split] outputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze [Squeeze] inputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze for ONNX node: /postprocessor/Squeeze[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_output_0 for ONNX tensor: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze [Squeeze] outputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_1 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_1 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_1[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_1 [Squeeze] inputs: [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_1 for ONNX node: /postprocessor/Squeeze_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_1_output_0 for ONNX tensor: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_1 [Squeeze] outputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_2 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_2 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_2[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_2 [Squeeze] inputs: [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_2 for ONNX node: /postprocessor/Squeeze_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_2_output_0 for ONNX tensor: /postprocessor/Squeeze_2_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_2 [Squeeze] outputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_3 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_3 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_3[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_3 [Squeeze] inputs: [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_3 for ONNX node: /postprocessor/Squeeze_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_3_output_0 for ONNX tensor: /postprocessor/Squeeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_3 [Squeeze] outputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul [Mul] inputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1309 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1310 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul for ONNX node: /postprocessor/Mul[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_output_0 for ONNX tensor: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Mul [Mul] outputs: [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Sub [Sub] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub for ONNX node: /postprocessor/Sub[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sub_output_0 for ONNX tensor: /postprocessor/Sub_output_0[0m
[38;5;104m[X] /postprocessor/Sub [Sub] outputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul_1 [Mul] inputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1311 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1312 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_1 for ONNX node: /postprocessor/Mul_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_1_output_0 for ONNX tensor: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Mul_1 [Mul] outputs: [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub_1 [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub_1 [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Sub_1 [Sub] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub_1 for ONNX node: /postprocessor/Sub_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sub_1_output_0 for ONNX tensor: /postprocessor/Sub_1_output_0[0m
[38;5;104m[X] /postprocessor/Sub_1 [Sub] outputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Add [Add][0m
[38;5;104m[X] Parsing node: /postprocessor/Add [Add][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Add [Add] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Add for ONNX node: /postprocessor/Add[0m
[38;5;104m[X] Registering tensor: /postprocessor/Add_output_0 for ONNX tensor: /postprocessor/Add_output_0[0m
[38;5;104m[X] /postprocessor/Add [Add] outputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /postprocessor/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Add_1 [Add] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Add_1 for ONNX node: /postprocessor/Add_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Add_1_output_0 for ONNX tensor: /postprocessor/Add_1_output_0[0m
[38;5;104m[X] /postprocessor/Add_1 [Add] outputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Sub_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze [Unsqueeze] inputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze for ONNX node: /postprocessor/Unsqueeze[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_output_0 for ONNX tensor: /postprocessor/Unsqueeze_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze [Unsqueeze] outputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Sub_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_1 [Unsqueeze] inputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_1 for ONNX node: /postprocessor/Unsqueeze_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_1_output_0 for ONNX tensor: /postprocessor/Unsqueeze_1_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_1 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_2 [Unsqueeze] inputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_2 for ONNX node: /postprocessor/Unsqueeze_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_2_output_0 for ONNX tensor: /postprocessor/Unsqueeze_2_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_2 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_3 [Unsqueeze] inputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_3 for ONNX node: /postprocessor/Unsqueeze_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_3_output_0 for ONNX tensor: /postprocessor/Unsqueeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_3 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Concat [Concat][0m
[38;5;104m[X] Parsing node: /postprocessor/Concat [Concat][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_2_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Concat [Concat] inputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Concat for ONNX node: /postprocessor/Concat[0m
[38;5;104m[X] Registering tensor: /postprocessor/Concat_output_0 for ONNX tensor: /postprocessor/Concat_output_0[0m
[38;5;104m[X] /postprocessor/Concat [Concat] outputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Tile [Tile][0m
[38;5;104m[X] Parsing node: /postprocessor/Tile [Tile][0m
[38;5;104m[X] Searching for input: orig_target_sizes[0m
[38;5;104m[X] Searching for input: onnx::Tile_3498[0m
[38;5;104m[X] /postprocessor/Tile [Tile] inputs: [orig_target_sizes -> (1, 2)[INT64]], [onnx::Tile_3498 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1313 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Tile for ONNX node: /postprocessor/Tile[0m
[38;5;104m[X] Registering tensor: /postprocessor/Tile_output_0 for ONNX tensor: /postprocessor/Tile_output_0[0m
[38;5;104m[X] /postprocessor/Tile [Tile] outputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Tile_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_4 [Unsqueeze] inputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Constant_21_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_4 for ONNX node: /postprocessor/Unsqueeze_4[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_4_output_0 for ONNX tensor: /postprocessor/Unsqueeze_4_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_4 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: Cast_3039 [Cast][0m
[38;5;104m[X] Parsing node: Cast_3039 [Cast][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_4_output_0[0m
[38;5;104m[X] Cast_3039 [Cast] inputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], [0m
[38;5;104m[X] Casting to type: float32[0m
[38;5;104m[X] Registering layer: Cast_3039 for ONNX node: Cast_3039[0m
[38;5;104m[X] Registering tensor: onnx::Mul_3505 for ONNX tensor: onnx::Mul_3505[0m
[38;5;104m[X] Cast_3039 [Cast] outputs: [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Concat_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3505[0m
[38;5;104m[X] /postprocessor/Mul_2 [Mul] inputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_2 for ONNX node: /postprocessor/Mul_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_2_output_0 for ONNX tensor: /postprocessor/Mul_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul_2 [Mul] outputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /postprocessor/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/Gather_8_output_0[0m
[38;5;104m[X] /postprocessor/Sigmoid [Sigmoid] inputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sigmoid for ONNX node: /postprocessor/Sigmoid[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sigmoid_output_0 for ONNX tensor: /postprocessor/Sigmoid_output_0[0m
[38;5;104m[X] /postprocessor/Sigmoid [Sigmoid] outputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Flatten [Flatten][0m
[38;5;104m[X] Parsing node: /postprocessor/Flatten [Flatten][0m
[38;5;104m[X] Searching for input: /postprocessor/Sigmoid_output_0[0m
[38;5;104m[X] /postprocessor/Flatten [Flatten] inputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1314 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Flatten for ONNX node: /postprocessor/Flatten[0m
[38;5;104m[X] Registering tensor: /postprocessor/Flatten_output_0 for ONNX tensor: /postprocessor/Flatten_output_0[0m
[38;5;104m[X] /postprocessor/Flatten [Flatten] outputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/TopK [TopK][0m
[38;5;104m[X] Parsing node: /postprocessor/TopK [TopK][0m
[38;5;104m[X] Searching for input: /postprocessor/Flatten_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] /postprocessor/TopK [TopK] inputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_convertToScalar_1315 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/TopK for ONNX node: /postprocessor/TopK[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1316 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: scores_1317 for ONNX tensor: scores[0m
[38;5;104m[X] Registering tensor: /postprocessor/TopK_output_1 for ONNX tensor: /postprocessor/TopK_output_1[0m
[38;5;104m[X] /postprocessor/TopK [TopK] outputs: [scores -> (1, 300)[FLOAT]], [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Div [Div][0m
[38;5;104m[X] Parsing node: /postprocessor/Div [Div][0m
[38;5;104m[X] Searching for input: /postprocessor/TopK_output_1[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] /postprocessor/Div [Div] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Constant_14_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1318 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1319 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Div for ONNX node: /postprocessor/Div[0m
[38;5;104m[X] Registering tensor: /postprocessor/Div_output_0 for ONNX tensor: /postprocessor/Div_output_0[0m
[38;5;104m[X] /postprocessor/Div [Div] outputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Div_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] /postprocessor/Mul_3 [Mul] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1320 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1321 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_3 for ONNX node: /postprocessor/Mul_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_3_output_0 for ONNX tensor: /postprocessor/Mul_3_output_0[0m
[38;5;104m[X] /postprocessor/Mul_3 [Mul] outputs: [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub_2 [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub_2 [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/TopK_output_1[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_3_output_0[0m
[38;5;104m[X] /postprocessor/Sub_2 [Sub] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub_2 for ONNX node: /postprocessor/Sub_2[0m
[38;5;104m[X] Registering tensor: labels_1322 for ONNX tensor: labels[0m
[38;5;104m[X] /postprocessor/Sub_2 [Sub] outputs: [labels -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Div_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_5 [Unsqueeze] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_5 for ONNX node: /postprocessor/Unsqueeze_5[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_5_output_0 for ONNX tensor: /postprocessor/Unsqueeze_5_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_5 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Tile_1 [Tile][0m
[38;5;104m[X] Parsing node: /postprocessor/Tile_1 [Tile][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] /postprocessor/Tile_1 [Tile] inputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1323 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Tile_1 for ONNX node: /postprocessor/Tile_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Tile_1_output_0 for ONNX tensor: /postprocessor/Tile_1_output_0[0m
[38;5;104m[X] /postprocessor/Tile_1 [Tile] outputs: [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/GatherElements [GatherElements][0m
[38;5;104m[X] Parsing node: /postprocessor/GatherElements [GatherElements][0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_2_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Tile_1_output_0[0m
[38;5;104m[X] /postprocessor/GatherElements [GatherElements] inputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1324 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/GatherElements for ONNX node: /postprocessor/GatherElements[0m
[38;5;104m[X] Registering tensor: boxes_1325 for ONNX tensor: boxes[0m
[38;5;104m[X] /postprocessor/GatherElements [GatherElements] outputs: [boxes -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0_378 as output: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0_701 as output: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0_1026 as output: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0_387 as output: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0_710 as output: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0_1035 as output: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0_385 as output: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0_708 as output: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0_1033 as output: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0_376 as output: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0_699 as output: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Marking /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0_1024 as output: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Marking labels_1322 as output: labels[0m
[38;5;104m[X] Marking boxes_1325 as output: boxes[0m
[38;5;104m[X] Marking scores_1317 as output: scores[0m
[38;5;14m[I] Building engine with configuration:
    Flags                  | [TF32]
    Engine Capability      | EngineCapability.STANDARD
    Memory Pools           | [WORKSPACE: 1024.00 MiB, TACTIC_DRAM: 24105.06 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]
    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]
    Profiling Verbosity    | ProfilingVerbosity.DETAILED
    Preview Features       | [PROFILE_SHARING_0806][0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Original: 1920 layers[0m
[38;5;104m[X] After dead-layer removal: 1920 layers[0m
[38;5;104m[X] Graph construction completed in 0.0190569 seconds.[0m
[38;5;104m[X] After adding DebugOutput nodes: 1920 layers[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3619[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3619 with ONNXTRT_Broadcast[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3614[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3614 with ONNXTRT_Broadcast_99[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3620[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3620 with ONNXTRT_Broadcast_101[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3616[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3616 with ONNXTRT_Broadcast_103[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3621[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3621 with ONNXTRT_Broadcast_105[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3618[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3618 with ONNXTRT_Broadcast_107[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_116[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.weight with ONNXTRT_Broadcast_121[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.bias with ONNXTRT_Broadcast_123[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear1.bias with ONNXTRT_Broadcast_131[0m
[38;5;104m[X] Running: ConstShuffleFusion on /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] ConstShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/activation/Constant_output_0 with ONNXTRT_Broadcast_133[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear2.bias with ONNXTRT_Broadcast_145[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.weight with ONNXTRT_Broadcast_149[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.bias with ONNXTRT_Broadcast_151[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.proj.bias with ONNXTRT_Broadcast_275[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.weight with ONNXTRT_Broadcast_279[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.bias with ONNXTRT_Broadcast_281[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_score_head.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_score_head.bias with ONNXTRT_Broadcast_289[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.0.bias with ONNXTRT_Broadcast_295[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.1.bias with ONNXTRT_Broadcast_303[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.2.bias with ONNXTRT_Broadcast_311[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3736[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3736 with ONNXTRT_Broadcast_332[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3731[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3731 with ONNXTRT_Broadcast_334[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3737[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3737 with ONNXTRT_Broadcast_336[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3733[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3733 with ONNXTRT_Broadcast_338[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3738[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3738 with ONNXTRT_Broadcast_340[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3735[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3735 with ONNXTRT_Broadcast_342[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_351[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.weight with ONNXTRT_Broadcast_356[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.bias with ONNXTRT_Broadcast_358[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.value_proj.bias with ONNXTRT_Broadcast_366[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_375[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_384[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.output_proj.bias with ONNXTRT_Broadcast_579[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.weight with ONNXTRT_Broadcast_583[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.bias with ONNXTRT_Broadcast_585[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear1.bias with ONNXTRT_Broadcast_593[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear2.bias with ONNXTRT_Broadcast_601[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.weight with ONNXTRT_Broadcast_605[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.bias with ONNXTRT_Broadcast_607[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.0.bias with ONNXTRT_Broadcast_615[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.1.bias with ONNXTRT_Broadcast_623[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.2.bias with ONNXTRT_Broadcast_631[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1426) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1426) [Constant] with ONNXTRT_Broadcast_636[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1433) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1433) [Constant] with ONNXTRT_Broadcast_643[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3808[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3808 with ONNXTRT_Broadcast_657[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3803[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3803 with ONNXTRT_Broadcast_659[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3809[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3809 with ONNXTRT_Broadcast_661[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3805[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3805 with ONNXTRT_Broadcast_663[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3810[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3810 with ONNXTRT_Broadcast_665[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3807[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3807 with ONNXTRT_Broadcast_667[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.self_attn.out_proj.bias with ONNXTRT_Broadcast_676[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.weight with ONNXTRT_Broadcast_681[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.bias with ONNXTRT_Broadcast_683[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.value_proj.bias with ONNXTRT_Broadcast_689[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_698[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_707[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.output_proj.bias with ONNXTRT_Broadcast_904[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.weight with ONNXTRT_Broadcast_908[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.bias with ONNXTRT_Broadcast_910[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear1.bias with ONNXTRT_Broadcast_918[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear2.bias with ONNXTRT_Broadcast_926[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.weight with ONNXTRT_Broadcast_930[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.bias with ONNXTRT_Broadcast_932[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.0.bias with ONNXTRT_Broadcast_940[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.1.bias with ONNXTRT_Broadcast_948[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.2.bias with ONNXTRT_Broadcast_956[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1834) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1834) [Constant] with ONNXTRT_Broadcast_961[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1841) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1841) [Constant] with ONNXTRT_Broadcast_968[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3880[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3880 with ONNXTRT_Broadcast_982[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3875[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3875 with ONNXTRT_Broadcast_984[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3881[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3881 with ONNXTRT_Broadcast_986[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3877[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3877 with ONNXTRT_Broadcast_988[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3882[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3882 with ONNXTRT_Broadcast_990[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3879[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3879 with ONNXTRT_Broadcast_992[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.self_attn.out_proj.bias with ONNXTRT_Broadcast_1001[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.weight with ONNXTRT_Broadcast_1006[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.bias with ONNXTRT_Broadcast_1008[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.value_proj.bias with ONNXTRT_Broadcast_1014[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_1023[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_1032[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.output_proj.bias with ONNXTRT_Broadcast_1229[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.weight with ONNXTRT_Broadcast_1233[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.bias with ONNXTRT_Broadcast_1235[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear1.bias with ONNXTRT_Broadcast_1243[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear2.bias with ONNXTRT_Broadcast_1251[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.weight with ONNXTRT_Broadcast_1255[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.bias with ONNXTRT_Broadcast_1257[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.0.bias with ONNXTRT_Broadcast_1265[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.1.bias with ONNXTRT_Broadcast_1273[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.2.bias with ONNXTRT_Broadcast_1281[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 2242) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2242) [Constant] with ONNXTRT_Broadcast_1286[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 2249) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2249) [Constant] with ONNXTRT_Broadcast_1293[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_score_head.2.bias with ONNXTRT_Broadcast_1299[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear1/Transpose with ONNXTRT_Broadcast_129[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear2/Transpose with ONNXTRT_Broadcast_143[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_output/proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_output/proj/Transpose with ONNXTRT_Broadcast_273[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_score_head/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_score_head/Transpose with ONNXTRT_Broadcast_287[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.0/Transpose with ONNXTRT_Broadcast_293[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.1/Transpose with ONNXTRT_Broadcast_301[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.2/Transpose with ONNXTRT_Broadcast_309[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_364[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_373[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_382[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_577[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear1/Transpose with ONNXTRT_Broadcast_591[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear2/Transpose with ONNXTRT_Broadcast_599[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose with ONNXTRT_Broadcast_613[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose with ONNXTRT_Broadcast_621[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose with ONNXTRT_Broadcast_629[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_687[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_696[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_705[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_902[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear1/Transpose with ONNXTRT_Broadcast_916[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear2/Transpose with ONNXTRT_Broadcast_924[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose with ONNXTRT_Broadcast_938[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose with ONNXTRT_Broadcast_946[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose with ONNXTRT_Broadcast_954[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_1012[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_1021[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_1030[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_1227[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear1/Transpose with ONNXTRT_Broadcast_1241[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear2/Transpose with ONNXTRT_Broadcast_1249[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose with ONNXTRT_Broadcast_1263[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose with ONNXTRT_Broadcast_1271[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose with ONNXTRT_Broadcast_1279[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_score_head.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_score_head.2/Transpose with ONNXTRT_Broadcast_1297[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape with /model/encoder/encoder.0/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_113[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_113[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 with /model/encoder/encoder.0/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/Transpose_1 with /model/encoder/Reshape_1[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape with /model/decoder/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_1 with /model/decoder/Transpose_1[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_2 with /model/decoder/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape with /model/decoder/decoder/layers.0/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape with /model/decoder/decoder/layers.1/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape with /model/decoder/decoder/layers.2/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_2 with /model/decoder/decoder/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape with /model/decoder/decoder/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_1 with /model/decoder/decoder/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_348[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_348[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Transpose_5 with /model/decoder/decoder/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_4 with /model/decoder/decoder/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_388[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_388 with /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_388 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_388 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2 with /model/decoder/decoder/layers.0/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Transpose_1 with /model/decoder/decoder/layers.0/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape_10 with /model/decoder/decoder/layers.0/cross_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_2 with /model/decoder/decoder/layers.1/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape with /model/decoder/decoder/layers.1/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_1 with /model/decoder/decoder/layers.1/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_673[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_673[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Transpose_5 with /model/decoder/decoder/layers.1/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_4 with /model/decoder/decoder/layers.1/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_711[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_711 with /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_711 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_711 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2 with /model/decoder/decoder/layers.1/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Transpose_1 with /model/decoder/decoder/layers.1/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape_10 with /model/decoder/decoder/layers.1/cross_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_2 with /model/decoder/decoder/layers.2/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape with /model/decoder/decoder/layers.2/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_1 with /model/decoder/decoder/layers.2/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_998[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_998[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Transpose_5 with /model/decoder/decoder/layers.2/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_4 with /model/decoder/decoder/layers.2/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1036[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1036 with /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1036 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1036 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2 with /model/decoder/decoder/layers.2/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Transpose_1 with /model/decoder/decoder/layers.2/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape_10 with /model/decoder/decoder/layers.2/cross_attn/Transpose_3[0m
[38;5;104m[X] QDQ graph optimizer - constant folding of Q/DQ initializers[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_5[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_9[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_13[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_17[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_19[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_23[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_27[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_31[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_35[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_39[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_43[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_47[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_51[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_55[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_59[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_63[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_67[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_71[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_75[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_79[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_83[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_87[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_89[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_91[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_95[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_126[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_140[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_155[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_159[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_163[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_167[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_171[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_173[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_177[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_181[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_185[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_189[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_193[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_197[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_199[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_203[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_207[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_211[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_215[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_219[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_223[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_225[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_229[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_233[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_237[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_241[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_245[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_249[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_251[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_255[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_257[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_259[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_263[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_270[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_284[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_290[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_298[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_306[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_317[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_325[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_361[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_370[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_379[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_574[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_588[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_596[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_610[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_618[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_626[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_684[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_693[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_702[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_899[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_913[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_921[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_935[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_943[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_951[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1009[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1018[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1027[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1224[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1238[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1246[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1260[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1268[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1276[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1294[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_2[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_10[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_18[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_24[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_32[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_40[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_48[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_56[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_64[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_72[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_80[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_88[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_92[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_127[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_156[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_164[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_172[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_178[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_186[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_194[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_200[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_208[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_216[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_224[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_230[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_238[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_246[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_252[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_258[0m
[38;5;104m[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_264[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_285[0m
[38;5;104m[X] Removing /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_299[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_318[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_362[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_380[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_589[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_611[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_627[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_694[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_900[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_922[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_944[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1010[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1028[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1239[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1261[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1277[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_3[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_4[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_7[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_8[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_11[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_12[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_15[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_16[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_21[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_22[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_25[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_26[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_29[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_30[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_37[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_38[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_34[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_41[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_42[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_45[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_46[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_49[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_50[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_57[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_58[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_54[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_61[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_62[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_65[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_66[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_69[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_70[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_77[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_78[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_74[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_81[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_82[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_85[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_86[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_93[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_94[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_124[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_125[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_138[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_139[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_153[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_154[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_157[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_158[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_161[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_162[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_165[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_166[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_169[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_170[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_175[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_176[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_179[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_180[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_183[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_184[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_187[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_188[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_191[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_192[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_195[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_196[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_201[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_202[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_205[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_206[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_209[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_210[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_213[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_214[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_217[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_218[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_221[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_222[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_227[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_228[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_231[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_232[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_235[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_236[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_239[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_240[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_243[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_244[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_247[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_248[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_253[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_254[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_261[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_262[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_359[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_268[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_360[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_269[0m
[38;5;104m[X] Removing /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_282[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_283[0m
[38;5;104m[X] Removing /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_296[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_297[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_304[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_305[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_315[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_316[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_323[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_324[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_368[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_369[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_572[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_573[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_586[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_587[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_594[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_595[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_608[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_609[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_616[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_617[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_624[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_625[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_644[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_645[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_650[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_651[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_691[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_692[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_897[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_898[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_911[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_912[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_919[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_920[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_933[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_934[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_941[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_942[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_949[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_950[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_969[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_970[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_975[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_976[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1016[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1017[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1222[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1223[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1236[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1237[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1244[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1245[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1258[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1259[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1266[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1267[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1274[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1275[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_6[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_14[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_20[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_28[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_36[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_44[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_52[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_60[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_68[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_76[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_84[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_90[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_96[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_141[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_160[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_168[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_174[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_182[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_190[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_198[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_204[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_212[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_220[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_226[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_234[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_242[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_250[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_256[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_260[0m
[38;5;104m[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_271[0m
[38;5;104m[X] Removing /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_291[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_307[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_326[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_371[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_575[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_597[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_619[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_685[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_703[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_914[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_936[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_952[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1019[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1225[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1247[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1269[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1295[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_33[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_53[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_73[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found and reassigned Myelin backends for Self-Attention nodes[0m
[38;5;104m[X] After Myelin optimization: 464 layers[0m
[38;5;104m[X] QDQ graph optimizer - constant folding of Q/DQ initializers[0m
[38;5;104m[X] QDQ graph optimizer forward pass - DQ motions and fusions[0m
[38;5;104m[X] QDQ graph optimizer backward pass[0m
[38;5;104m[X] QDQ graph optimizer quantization pass - Generate quantized ops[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.0/Add with /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.1/Add with /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.0/Add with /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.1/Add with /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.0/Add with /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.1/Add with /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.0/Add with /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.1/Add with /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_1/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_1/norm/BatchNormalization with /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_2/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_2/norm/BatchNormalization with /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_3/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_3/norm/BatchNormalization with /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_1.conv.weight with /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_2.conv.weight with /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_3.conv.weight with /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight with /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.2.conv.weight with /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.0.conv.weight with /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.1.conv.weight with /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.0.conv.weight with /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight with /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight with /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight with /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.1.conv.weight with /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight with /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight with /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight with /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.0.conv.weight with /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight with /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight with /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight with /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.1.conv.weight with /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight with /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight with /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight with /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.0.conv.weight with /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.1.conv.weight with /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.2.conv.weight with /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/backbone/MaxPool[0m
[38;5;104m[X] Swapping /model/backbone/MaxPool with /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_2[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_3[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_4[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_5[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize[0m
[38;5;104m[X] Swapping /model/encoder/Resize with /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize_1[0m
[38;5;104m[X] Swapping /model/encoder/Resize_1 with /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Running: HorizontalMergeQNodes on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Eliminating /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 which duplicates (Q) /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/lateral_convs.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.0/act/Sigmoid with /model/encoder/lateral_convs.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv1/act/Sigmoid with /model/encoder/fpn_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv2/act/Sigmoid with /model/encoder/fpn_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul) with /model/encoder/fpn_blocks.0/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv3/act/Sigmoid with /model/encoder/fpn_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/lateral_convs.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.1/act/Sigmoid with /model/encoder/lateral_convs.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv1/act/Sigmoid with /model/encoder/fpn_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv2/act/Sigmoid with /model/encoder/fpn_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul) with /model/encoder/fpn_blocks.1/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv3/act/Sigmoid with /model/encoder/fpn_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/downsample_convs.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.0/act/Sigmoid with /model/encoder/downsample_convs.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv1/act/Sigmoid with /model/encoder/pan_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv2/act/Sigmoid with /model/encoder/pan_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul) with /model/encoder/pan_blocks.0/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv3/act/Sigmoid with /model/encoder/pan_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/downsample_convs.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.1/act/Sigmoid with /model/encoder/downsample_convs.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv1/act/Sigmoid with /model/encoder/pan_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv2/act/Sigmoid with /model/encoder/pan_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul) with /model/encoder/pan_blocks.1/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv3/act/Sigmoid with /model/encoder/pan_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/norm/BatchNormalization + /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/norm/BatchNormalization + /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/norm/BatchNormalization + /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/lateral_convs.1/conv/Conv with PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.0/conv/Conv with PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.1/conv/Conv with PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1 and /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2 and /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2 and /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv with /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] QDQ graph optimizer quantization epilogue pass[0m
[38;5;104m[X] QDQ optimization pass[0m
[38;5;104m[X] QDQ graph optimizer constant fold dangling QDQ pass[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Swap the layer type of /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Swap the layer type of /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 from QUANTIZE to kQDQ[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] After vertical fusions: 86 layers[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] After slice removal: 86 layers[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_5[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_5_/model/encoder/lateral_convs.0/act/Mul_output_0_clone_1 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_4[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Generating copy for /model/encoder/Resize_1_output_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_3[0m
[38;5;104m[X] Generating copy for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_2[0m
[38;5;104m[X] Generating copy for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] After concat removal: 85 layers[0m
[38;5;104m[X] Trying to split Reshape and strided tensor[0m
[38;5;104m[X] Graph optimization time: 0.0827581 seconds.[0m
[38;5;104m[X] Building graph using backend strategy 2[0m
[38;5;13m[V] Local timing cache in use. Profiling results in this builder pass will not be stored.[0m
[38;5;104m[X] Constructing optimization profile number 0 [1/1].[0m
[38;5;104m[X] Applying generic optimizations to the graph for inference.[0m
[38;5;104m[X] Reserving memory for host IO tensors. Host: 0 bytes[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(3276800,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_chw_int8int8_tilesize16x16_k32_fltsteps1_threadspercta256_r3s3_u2v2_scalebias_relu Tactic: 0x11764d94950382f8 Time: 0.00577012[0m
[38;5;104m[X] Tactic Name: ampere_first_layer_filter3x3_imma_fwd Tactic: 0x9ae0c0d2fb3a01e5 Time: 0.00641026[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00632625 seconds. Fastest Tactic: 0x11764d94950382f8 Time: 0.00577012[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x11764d94950382f8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,409600:4,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize16x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x3d988d07a78b0918 Time: 0.0050699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x5cc792a989a1d1a6 Time: 0.00496878[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00978621[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00965668[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0115629 seconds. Fastest Tactic: 0x5cc792a989a1d1a6 Time: 0.00496878[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5cc792a989a1d1a6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1:16,640,1) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0390684[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0179105[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0145442[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0149338[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0299947[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0253745[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0373487[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0140893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0447187[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0269153[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0289956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0158022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0298756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0156349[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0305358[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0306114[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0337824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0401861[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0111858[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0151105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.014052[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0111588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0434213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0142356[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0189043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0120484[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0171915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0233472[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0367776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0308674[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0301129[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0308034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.045208[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0263713[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0143898[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0140182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0183949[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0112324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.04328[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0141164[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0141093[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0449253[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0437093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0172603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0111751[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0162215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0112448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0148925[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0315656[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0156097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0181081[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0196681[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0146824[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0110944[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0414625[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0131947[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0143036[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0130638[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0136597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0239269[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.028488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0398957[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0131061[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.028872[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0375301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0145248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0141951[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0107613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0322938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.030056[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0313901[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0119276[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0324625[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0286018[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0408486[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0111278[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0295031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0133687[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0102888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0166805[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0373108[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0181474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0124077[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0124788[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0369675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0425147[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0154958[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0281369[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0155433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0173696[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0171419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0239253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0477943[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.025229[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0111204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.017664[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0277678[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.016544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0501166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0112796[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0256812[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0396658[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0100013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0148777[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0139667[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0247573[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0160996[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0185677[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0409067[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0151072[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0290409[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.028536[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.029232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0138735[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0133194[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0110665[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0144093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.0396041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0181698[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0132903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.012753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0281093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0112501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0220907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0165293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0151239[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0146987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0138748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0258913[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0165862[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.0100913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0495482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0135027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.028392[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0402086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0283022[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0164135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0369781[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0282187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.048643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0142449[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0118919[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0106877[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0166796[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.017806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0128591[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0138564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0112046[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.445005 seconds. Fastest Tactic: 0xd805711548cbfb38 Time: 0.0100013[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd805711548cbfb38[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,409600:32,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0260669[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0197597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0342859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0356928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0141396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.016976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0280836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0177112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0261957[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.020315[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0294676[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0196386[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0163865[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0245516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0273559[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0238171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0276193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0137958[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0229376[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0164455[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0198406[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.015649[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0150493[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.021288[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0274732[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0434133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0297582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.018573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0195899[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0143569[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0233614[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0278391[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0118415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0451613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0129112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0271926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0154759[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0485638[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0193244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0273567[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0145915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0287369[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0260176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.027698[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0300542[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0122053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0198626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0356661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.024899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0211313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0267692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0131729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0198017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0195881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0383265[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0436893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0161666[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0197503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0267495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0194833[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0157775[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0495756[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0171541[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0122781[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0127866[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0186821[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0144169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0136687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0441613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0261719[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0315045[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.018733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0265313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0180284[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0271992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0172544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0449893[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.02183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0150265[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0271836[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0154032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0169701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0270178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0345568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0160874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0264993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0203727[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.241134 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.0118415[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(3276800,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0205446[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0201901[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00564305 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0201901[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0349707[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0217547[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0166105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0166461[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0403496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0323539[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0511939[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.015344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0382649[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0226439[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.037568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0205327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0412255[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0196888[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0223196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0418147[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0298089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0359573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0132927[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0163236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0138364[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0125871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0283173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0182765[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0259528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0126704[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0228871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0297787[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0505234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.023621[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0408735[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0427613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0394145[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0220187[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0147654[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0133815[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0209107[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0114763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0279129[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0137805[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.01448[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0307025[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0283253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0199203[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0125124[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0189499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0133161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0169333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0434227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0166784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0259602[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0258371[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0159777[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0137216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0638862[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.017842[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0173307[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0175253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0163449[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0342709[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0473027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0602489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0140196[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0413381[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0374708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0161615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.019408[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0140822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0506834[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0278969[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0303796[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0142209[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0509638[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0228331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0294942[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0140307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0268661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0167317[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0116123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0256518[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0372077[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0263122[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0152015[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0152495[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0369291[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0644196[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0230428[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0452453[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0189316[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0259536[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0224327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0318739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0732245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0238804[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0132094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0262999[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0450413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0205823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0760491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0132139[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0349024[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0278942[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.012024[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0189819[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0177645[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0366219[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0215633[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0266535[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0305813[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0178476[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.046476[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.0461827[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0481402[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0185399[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.015537[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0136311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0208987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.059968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0263885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0160686[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0152926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0413428[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0140836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0320039[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.020635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0200006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0185221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0166939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0251703[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0256145[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.0117944[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0747008[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0165369[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.043544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0287849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0247406[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0260546[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0369227[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0433573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0739349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0158284[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0143813[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0143427[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0220113[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0265534[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0163789[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0167051[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0133466[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.396714 seconds. Fastest Tactic: 0xa60c3259c62a72b2 Time: 0.0114763[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa60c3259c62a72b2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xde3a6a3727f31f34 Time: 0.946176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0255779[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0189535[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbbff0ceb48c87bac Time: 0.919477[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x9c9fd7d74c020c9d Time: 0.488715[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0347339[0m
[38;5;104m[X] Fast skip Tactic:0xb97409e537081e4c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb97409e537081e4c Time: 3.56982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0347477[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe5f40c565f9c8a09 Time: 0.0477973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0127403[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x4e679e1c8dcfbe3c Time: 0.941739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0161788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0274946[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x645c57c8d2bdcafa Time: 0.0531962[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x13cb22041bcdf2f5 Time: 0.858112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0164109[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0255512[0m
[38;5;104m[X] Fast skip Tactic:0xa1513318dd2f5314 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa1513318dd2f5314 Time: 2.36339[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0199831[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0288978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.017504[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0154453[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0238842[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2d0a836ca3b48b55 Time: 0.617813[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x307c1c762709b00e Time: 0.833195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0266585[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0230613[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9826a9122a4e1bac Time: 0.40832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0266519[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0127842[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.022002[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0158778[0m
[38;5;104m[X] Fast skip Tactic:0x9c391ea4731a473c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9c391ea4731a473c Time: 1.85814[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe6fb49f176c8ac20 Time: 0.241275[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.019621[0m
[38;5;104m[X] Fast skip Tactic:0x136deb7724d5b954 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x136deb7724d5b954 Time: 1.0025[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x718a86dfcb201f10 Time: 0.210011[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0148165[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x415d0d459f475d7a Time: 0.0705984[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0144404[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x7f9cac2d273e24da Time: 0.0398471[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x730183d1b4e07af0 Time: 0.0604693[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.021084[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xab1e201ca705d0dd Time: 0.612928[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x2c80f3b4623a1878 Time: 0.213333[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4dc022b90c990350 Time: 0.0377138[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0269949[0m
[38;5;104m[X] Fast skip Tactic:0x8286a69028b3e3f0 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x8286a69028b3e3f0 Time: 4.03104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0431013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0292[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0176775[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0187455[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0138556[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4480741a5fe7a6b0 Time: 0.0309081[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x8c7efb20a3cfa7ec Time: 0.58224[0m
[38;5;104m[X] Fast skip Tactic:0xb7622317db162586 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb7622317db162586 Time: 1.06682[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0227435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0270777[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0100006[0m
[38;5;104m[X] Fast skip Tactic:0x89a3827f636f5c26 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x89a3827f636f5c26 Time: 1.04448[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xdb35ad3ee7e5f3a9 Time: 0.0635538[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xfe34f7b3aa1f6429 Time: 0.0415502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.044564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0112174[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0268636[0m
[38;5;104m[X] Fast skip Tactic:0x3836d6c48cd9dbee which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3836d6c48cd9dbee Time: 1.20806[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x0f57663c97a6a89c Time: 0.478549[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5801f8b0fa1b5d5c Time: 0.807936[0m
[38;5;104m[X] Fast skip Tactic:0x48fd1cd53c4157a8 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x48fd1cd53c4157a8 Time: 4.66944[0m
[38;5;104m[X] Fast skip Tactic:0x31768067dfa77e0e which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x31768067dfa77e0e Time: 1.10998[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x0a6a5850a77efc64 Time: 0.951637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0137847[0m
[38;5;104m[X] Fast skip Tactic:0xe47e7c8e9e121924 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xe47e7c8e9e121924 Time: 3.70483[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x09cde4f526284108 Time: 0.111161[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x244ad5cff0ca2eb5 Time: 0.509269[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x853ead83f0b1020c Time: 0.755317[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0481234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0188735[0m
[38;5;104m[X] Fast skip Tactic:0x3620fc3660c7e024 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3620fc3660c7e024 Time: 1.91693[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xaca2d8f22e95cba6 Time: 0.584363[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0267438[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x23f62d21795a35ce Time: 0.888555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0133636[0m
[38;5;104m[X] Fast skip Tactic:0x6b2a895dc9dde74c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x6b2a895dc9dde74c Time: 2.05414[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0269177[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5600d95cf91aed70 Time: 0.0372729[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x032a0ef3f4005984 Time: 0.758976[0m
[38;5;104m[X] Fast skip Tactic:0xdf8cd3fb81a9e498 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xdf8cd3fb81a9e498 Time: 3.95075[0m
[38;5;104m[X] Fast skip Tactic:0x2e05c6cb8ae0ad7c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2e05c6cb8ae0ad7c Time: 4.19123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0254255[0m
[38;5;104m[X] Fast skip Tactic:0xd916513230270d0c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xd916513230270d0c Time: 1.16326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0261079[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0296018[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0111737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0194939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0351915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0238766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0206733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0267692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.011762[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xecb45af50ce22fe9 Time: 0.0353995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0195692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0193256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0375016[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0432667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0158448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.017719[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0262384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.019357[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0150052[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0491078[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0167403[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0104933[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0118893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.017216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0128174[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.012035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.043936[0m
[38;5;104m[X] Fast skip Tactic:0x5642a4e167e8f364 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x5642a4e167e8f364 Time: 2.10739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0253097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.030913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0182445[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0258371[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0168741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0263844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0162174[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9efe56660532ec2c Time: 0.475136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0445813[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0214927[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9ebc2bdb9bc0f238 Time: 0.123349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0138086[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa25e76bff47b753d Time: 0.765611[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0264673[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x11aaa3b552fd1244 Time: 0.644779[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xb99e8d5a01f89b1d Time: 0.49664[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbe2275b488688066 Time: 0.867669[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x34abf9381f0785c4 Time: 0.515755[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0144951[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0162667[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xc52cdc7983541cc4 Time: 0.421888[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x06f777ac34a0a24e Time: 0.650891[0m
[38;5;104m[X] Fast skip Tactic:0xc1336bcfda004054 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xc1336bcfda004054 Time: 1.73757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0262129[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x36ca788956376575 Time: 0.448171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0339915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.014384[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x76dcfa8e7440813a Time: 0.0385339[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x54c7919e8f324660 Time: 0.111013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0260529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.020182[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.731442 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.0100006[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(6553600,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0324305[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0319709[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0055738 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0319709[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(409600,1:16,1280,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0357365[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0382424[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0292204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0354667[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0681301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0364768[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.094528[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0278267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0405286[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0247459[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0729237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.035072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0718464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0346069[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0277481[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.074272[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0307132[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0367584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0233899[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0335605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.027159[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0203068[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0392391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0318623[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0477775[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0235029[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0385173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0360885[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0929307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0285689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0681941[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0733376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0416893[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.024829[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0308393[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0281778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0402726[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.021422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0390495[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0312349[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0278071[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0400853[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0391502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0262129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0196951[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.022188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0243528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0192444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0745195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0282596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.047388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.046932[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0330007[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.024752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0702464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.031201[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0305862[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0333952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0221133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0387449[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0860747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.108237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0309101[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0706304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0378856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0345067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0332247[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0240739[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.094224[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0304349[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0312776[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0265452[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0948827[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0280729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0399893[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0256435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0296942[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0190791[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0225906[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.04636[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.037523[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0477714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.027255[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0273321[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0371153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0778048[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0375585[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0835413[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0375099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0484023[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0414471[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.047172[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0792299[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.025357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0239284[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0481935[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.082432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0369717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0810325[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0246232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0480274[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0389535[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0217207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0237269[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0337611[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0684907[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0364747[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0489265[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0398756[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0333033[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0850213[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.084008[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0877227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0354987[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0335573[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0238133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.036496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.107379[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0485775[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0285929[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.027703[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0700011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0257715[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.037107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0342592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0233216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0271147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0263844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0265444[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.046732[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.020827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0883413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0224597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0738453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0396373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0288151[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0477364[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0372765[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0750357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0814293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0323568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0266437[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0249676[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0387449[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0480122[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0293609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0311418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0252853[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.409462 seconds. Fastest Tactic: 0xbe784bf72795274c Time: 0.0190791[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbe784bf72795274c[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0265945[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0194536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0341408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.034928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0145173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0183377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0288924[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0167493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0140871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0256358[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0200922[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.010632[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0289867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.030208[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.016063[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0240983[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0270187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0284213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0268808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0130556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.023424[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0165577[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0198695[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0151602[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.014893[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0212633[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0279902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0430333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0131947[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0299111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.0155273[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0183882[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0188148[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0142444[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0229746[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0297431[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0153959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.044828[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0171771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0269522[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0157164[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.04816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0196725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0296898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0139364[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0270302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0165039[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.025635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0262539[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0296889[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0184371[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0202089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0355168[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0249714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0217327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0268144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0183203[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0199937[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0194353[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.037587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0434587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0160411[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0303118[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0261973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0193517[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0152926[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0491642[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0172027[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0161407[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0191775[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0176601[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0204461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0191437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0439453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0258256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0316296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0197459[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0292684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0288524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0265017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0280151[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0446[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0217307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0223708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0137199[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.026601[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0149621[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0167333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0264927[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0342795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.024285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0111772[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0263221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0355797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0134229[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.25281 seconds. Fastest Tactic: 0x214f03e23f252333 Time: 0.010632[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x214f03e23f252333[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/MaxPool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,102400:4,320,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0xfa45342d0e1d409a Time: 0.011534[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0xa88280db27a5d09f Time: 0.0100502[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0x3acbbd865df539ed Time: 0.0104513[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0x3d35b618fd3968f1 Time: 0.0101676[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0xbeae815d02985cde Time: 0.0127356[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0xe09c44661dba1b5c Time: 0.0089673[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0xb331e89337ca2bad Time: 0.0101463[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0xc4335d27b08156bf Time: 0.0124132[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0xbadabf84bb0f736c Time: 0.00826641[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x80d8e857bc044afb Time: 0.0125381[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0x199aaf5950022512 Time: 0.0128291[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x67734dfa5b8c00c1 Time: 0.00784025[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0xf307ae442c39b4a3 Time: 0.0138705[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0x2eae5c3accbac70e Time: 0.0128299[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x9fe4504b077a26fb Time: 0.0113568[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0x63077323e21b2f73 Time: 0.0117245[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x9dff93820f6f4021 Time: 0.0103234[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x923de46f0f4ddb62 Time: 0.0112068[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0xc9170fb073b2e283 Time: 0.0094234[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0xd3ce7ffb6015b945 Time: 0.0117929[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x072517eca2b23ed1 Time: 0.0119169[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0x1340f65033fdc032 Time: 0.00889404[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0x47a86a624f206290 Time: 0.00982525[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x9a01981cafa3113d Time: 0.00977402[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x69dd2a2a81e4ca53 Time: 0.00885025[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x4a8c38f58c13d6ac Time: 0.0119867[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0x5d711a295c873956 Time: 0.0123219[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0x1dee9180e9950aa0 Time: 0.00835653[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x899a723e9e20bec2 Time: 0.0134878[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0xedb816f1de89af60 Time: 0.0160828[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0xa01139e8f028471d Time: 0.0172629[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x898e8c271f222050 Time: 0.0143609[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0xc04763fe0916790d Time: 0.00925232[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0x6e2321b421289b4f Time: 0.012251[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0x27ecc653ee9e3337 Time: 0.0125069[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x9351f452d5078ab3 Time: 0.00916612[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0xbee85cb73ffdd634 Time: 0.0111964[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x0165782a59f89027 Time: 0.00772218[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0xcee9042ed37eb39f Time: 0.0103011[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0xb474d8546167b9fe Time: 0.00976945[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0x74fa51ff328fc089 Time: 0.0154827[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0x543380407ea3cd6f Time: 0.0140689[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0x3465da56879df37f Time: 0.00899228[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kMAX Tactic: 0x1f6c40e3e09ec730 Time: 0.00618687[0m
[38;5;104m[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.114892 seconds. Fastest Tactic: 0x1f6c40e3e09ec730 Time: 0.00618687[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x1f6c40e3e09ec730[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,102400:32,320,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX Tactic: 0x94215b398b8eb3ba Time: 0.00650359[0m
[38;5;104m[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.00248862 seconds. Fastest Tactic: 0x94215b398b8eb3ba Time: 0.00650359[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x94215b398b8eb3ba[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0208553[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0204505[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00583385 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0204505[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0128767[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0148434[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00995827[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00985286[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0116447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0131762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0129596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0107153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0152436[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0100493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0127913[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0138483[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0130043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0115101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0121112[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0118665[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0100637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.013896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0118264[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.011904[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0116885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00872752[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0129399[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0135957[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00899621[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0135812[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0109856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0115847[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0724868 seconds. Fastest Tactic: 0x5f5aa01645d48746 Time: 0.00872752[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5f5aa01645d48746[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0148077[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00956099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0153959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0182552[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.009424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.011052[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0156722[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00783107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0085072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0160132[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00949807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00736812[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0124223[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0164861[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00873381[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.012738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0128985[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0155913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.011801[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00956922[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0132185[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.010554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0123919[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0084992[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00880898[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0124041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0152349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0201387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00704711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0136951[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00968594[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0098927[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0127455[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00906753[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0124839[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0163322[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00927348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0232597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0100041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0127854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00969204[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0219853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0108655[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0162245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0099018[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0118157[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00991623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0159893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0114702[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0125566[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0112188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0124365[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0165948[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0136026[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.013017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0152315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0104793[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0124832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0122034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0193393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0225721[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00954636[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0166933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0163098[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0122377[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00906003[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0222322[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.01104[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00958811[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0117455[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00798197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00926074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00888225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0202792[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0125985[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0141902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.011408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0160554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0132509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0164891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0131118[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0206105[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.012499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0103522[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00693986[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0116605[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00878316[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00986133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0115266[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0161859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0120103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00673003[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.014873[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0166853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00697755[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.257219 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.00673003[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0218927[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0208873[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00585226 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0208873[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0103215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0168085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0117933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0145025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0123662[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146503 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.0103215[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0177982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0134609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0240571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0160863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0167968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0141249[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0169723[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0137152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0147019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0164109[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0155971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0166212[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0139089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0171573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0136777[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0135044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0125033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0169589[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0136687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0156509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.013705[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0176876[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0244[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0679991 seconds. Fastest Tactic: 0xb936321f82fd390c Time: 0.0125033[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb936321f82fd390c[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00826355[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00842613[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00843947[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00857737[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0114821 seconds. Fastest Tactic: 0xa4ae2d82115c3e83 Time: 0.00826355[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa4ae2d82115c3e83[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.011413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0103389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0124393[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00711907 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.0103389[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00949155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00785017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00806959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00893165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00820995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00886372[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.011941[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00857819[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0100411[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00946844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00951348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00937718[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00800381[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00832208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00892997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0107754[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0110627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.0081054[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00858503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00820449[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00873463[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0103341[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00982682[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0090756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00807365[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00824065[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.0078338[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0085168[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0107675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00967924[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00905398[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00877754[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0109447[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0854223 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.0078338[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x0f47434ace2a7d18, 0.0204505 ms[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4)] got cached result: CaskConvolution, tactic 0x5f5aa01645d48746, 0.00872752 ms[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x9dafb2758560cc1d, 0.00673003 ms[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0226012[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0216473[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0054591 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0216473[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0122903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0215173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0140573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0175556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0143844[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0118508 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.0122903[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0149236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0146987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.022192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0176006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0176387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0113156[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0150145[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0149259[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0119238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0169899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0169904[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0171563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0110624[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0154521[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0119158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0134784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.013702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0178436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0136184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0130798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0120861[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0147664[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0227236[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0535181 seconds. Fastest Tactic: 0x0e07dc8353bf7e9f Time: 0.0110624[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0e07dc8353bf7e9f[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(102400,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00318141[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00321283 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00318141[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(12800,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00305009[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.003123 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00305009[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0160721[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0152349[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00566069 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0152349[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.00735838[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00648595[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00866215[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00603619[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.00733472[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00751218[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00728278[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00611258[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00652267[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0091987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00718638[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00665412[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00723631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.011206[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0068726[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00742756[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00614963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00758642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00672277[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0116403[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0113515[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00763273[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00712737[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00671125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0081214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00753161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00623131[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0111797[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0742374 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00603619[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00758012[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0060738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00622538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.00810667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00662044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0066675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00884884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00757262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.00936385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0060736[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0120632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00956587[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00712828[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00957044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00763636[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00899593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0116484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00632574[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00759927[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.00803708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0064958[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00681861[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0069305[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00739223[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00778543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0112217[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00611026[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00581315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.008368[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00678037[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.00917304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00958263[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00769915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00957348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00822868[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00754868[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00680881[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0121383[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00686345[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00958842[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00673856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0118209[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.00924223[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0114809[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0123387[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00815038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.007688[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00942429[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00766497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0079708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00705622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00847093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00933155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00649887[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00829268[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.00936741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0063994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00993788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00932593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.00948296[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00878148[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0122766[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0082586[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00799568[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0084328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00781172[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00683385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00664052[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0113131[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.00742187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0062637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00696711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00912375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00864438[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00949452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00819148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0113504[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00751905[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.007006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0114759[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00842347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00665976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0113326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.00922666[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0081548[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00680446[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0104048[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.239432 seconds. Fastest Tactic: 0x705baf38e41eee0b Time: 0.00581315[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x705baf38e41eee0b[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0280871[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0219013[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0057351 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0219013[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00894709[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0135808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00910155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0115895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00930489[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0129511 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.00894709[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0210767[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0117466[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0203664[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0104019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0128849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0138022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0132394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0110452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.015034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0125503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0103635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0128257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0135381[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0136764[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00974842[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00950311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0102895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0133542[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00962133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0117039[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00991718[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.021184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0205666[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0562776 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00950311[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00715325[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00699[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00729948[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00716255[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0118429 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00699[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00692419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00668202[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00804521[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00858298 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00668202[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00941452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00534536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00615796[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00612422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00627417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00886569[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00629193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00678315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00654599[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00608679[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00605048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00634244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00551852[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00549106[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00688043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00590915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00600934[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00552586[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00588613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.0055888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00861675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00672832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00646257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0061378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00542125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00565135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00544396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00600419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00652718[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00633439[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00592898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00595619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00704755[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.100746 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00534536[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0294853[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0266404[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00551454 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0266404[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.010667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00864492[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0133247[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00808457[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0105917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0107307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0101052[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00847387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00857245[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0144649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0100254[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00850453[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00991121[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.01776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00913643[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0107123[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0083568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.010422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00911856[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.018377[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0182164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0110338[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00982274[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00862386[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0117852[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0103014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00856971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0177229[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.081173 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00808457[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0102093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00745529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00798502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0116609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00886737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00805435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0122674[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0106037[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00680773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0141507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00779659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00638209[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0191064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0132021[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00962804[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0137711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0108153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0124014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0179907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00875351[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.010176[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0114286[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00906292[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00928829[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00926014[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0104377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.00869634[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0103864[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0176786[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00957348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00772315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00729832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0072152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0162839[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.012096[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00911135[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0131922[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.013017[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0100888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.00989271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00770788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0144662[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00935881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0109144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00906263[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0191828[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00824377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.00976853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0129321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00908541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0180458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00772024[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0140178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.017719[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0194495[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0110255[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.010729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0142533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0102946[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0110369[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00946933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0096384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.014324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00896814[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0120423[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0141671[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00868622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0133589[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0141938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0141778[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.012546[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0192871[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0114752[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0103602[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0112395[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0108597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00762303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00728742[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0177347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0105453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0079487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00844907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0125171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0105127[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0143502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0105417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0180261[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0105397[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00880365[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00947141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0180502[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0119779[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00901564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.017842[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0140871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00998933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00952415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00935674[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0138496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00945955[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.284579 seconds. Fastest Tactic: 0x214f03e23f252333 Time: 0.00638209[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x214f03e23f252333[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0287947[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0226126[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00539289 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0226126[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00985726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0164145[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00969234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0130014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0101624[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0120567 seconds. Fastest Tactic: 0xa8b56a226b057463 Time: 0.00969234[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa8b56a226b057463[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0216847[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0116075[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0193268[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0098971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0132263[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0128747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0125041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0112501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0141413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0127032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00977676[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0130638[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0126333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.012717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00886541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0096387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.010642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0137502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00980768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0104162[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00911856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0215473[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0197001[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0540959 seconds. Fastest Tactic: 0xad886d4d69834922 Time: 0.00886541[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xad886d4d69834922[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00287489[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00296518 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00287489[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00278551[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00282056 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00278551[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0276349[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0259249[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00551987 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0259249[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0105233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00710286[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0131516[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00693551[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.010504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0106183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00667541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00696022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00702067[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0142013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0065943[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00695[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0065531[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0178554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00769309[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.010585[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00698933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00693115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00756456[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.018281[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.018158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0108872[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00652369[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00706267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0117425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00687216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00717027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.017719[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.071102 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00652369[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00864[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00732058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00751834[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0115697[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00617076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00753351[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0107833[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0142742[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00824924[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0192421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0083624[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0102374[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0139209[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0107995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0077103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0181889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00871986[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00681077[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.00850667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00872533[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00925722[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0083912[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00983215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00876089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0178987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00696289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00699444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0122731[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0083904[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0133034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00772339[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00855877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0145187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00683341[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0109991[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00627615[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0192693[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0060261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00749535[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00902256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0184977[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0141436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0180901[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0194116[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00895607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00710264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0144369[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00699155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00718525[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00890358[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00692027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00879102[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00861566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0122469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0142551[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00850293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00834187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0143302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.00868677[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0127471[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0194684[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00871467[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00883004[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.009216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0111367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00794489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00757025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0179099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.010488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00706533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00605048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00766715[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00718411[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0144084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00698644[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0181968[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0100216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00920447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0182608[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0123583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00888309[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0179716[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0141649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00689894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00868978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.010252[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.280489 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.00584[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0501425[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0380634[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00528554 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0380634[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0116013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0201487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00932029[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.009216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0120347[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0119156 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.009216[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0344096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.010502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0341675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0115031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0110889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0188883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0184342[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00985474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0215353[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0109096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0114251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0110692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0186056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0188207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0113365[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00860472[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00871494[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0112498[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00958567[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0151049[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0115833[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0341579[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0344[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0560638 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00860472[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00811429[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00776508[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00834507[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00792633[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0119253 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00776508[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00670443[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00552498[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00554877[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00852106 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00552498[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.01043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00422116[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00609067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00470881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00629937[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00975268[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00523533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00696711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00954423[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00446208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00453823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00482819[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00428266[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00493396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00718343[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00481173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00491969[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00424601[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00456679[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00445369[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00949866[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00962499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00658824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00615583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00483339[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00422643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00434923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00483138[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0049869[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00610056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00544275[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.0060381[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00994698[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0943817 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00422116[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0538072[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0487573[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00549424 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0487573[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0171376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0103984[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.022395[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0102397[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0170795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0172917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00952148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.010315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0103754[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0244114[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00942726[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.010272[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.009464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0309692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0110551[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0172203[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0104777[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00976823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0109178[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0320233[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0318798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.017719[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00943555[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0103893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0192699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00973349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0105177[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.030881[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0863474 seconds. Fastest Tactic: 0x19e870769dcaba51 Time: 0.00942726[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x19e870769dcaba51[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0135996[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0104803[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0113326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.018896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0084928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00737809[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0104253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0172299[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00938281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0239771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0117418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00932296[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0332577[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0109763[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0148601[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0222364[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0173387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0105943[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0312892[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0136021[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00932444[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.013232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0136414[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0141613[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0127554[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0157474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0101514[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0138035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0310759[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0160284[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0103312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00753564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0104423[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0293982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0200835[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0127759[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0215867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0107892[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0128287[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0162519[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00966247[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0244465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00925895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0179587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00867446[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0335061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00840693[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.0161402[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.010587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0138924[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0312669[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0083344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0241074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0309207[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0335712[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.013127[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0101317[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0242507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0095427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.010467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0138615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00913153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0137233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0135535[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0201732[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0242476[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0134293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0110627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0241562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0136755[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.020293[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0335435[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0135121[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0130638[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0135514[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0175056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0110321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.01072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0310652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0171061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0104507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00843013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0106417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00894035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0242949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00923358[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0316451[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0157198[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0138394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0160437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0317159[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0195917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0137711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.031072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0242476[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00803835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0160091[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0150521[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0160935[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.286662 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.00737809[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0508069[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.038784[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0053468 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.038784[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0126214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0228772[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0101498[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0100298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0130589[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0145986 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0100298[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0349845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0100922[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0346229[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0115314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0112316[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0190643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0185404[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00936326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.021594[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.010687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0114165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0108948[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0187787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0188409[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0113995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00828592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0084472[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0113916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00941837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0147362[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0115954[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0347285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0349451[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0649395 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00828592[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00263181[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00316885 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00263181[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00293015[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00280299 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00293015[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0518781[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0480274[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00542654 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0480274[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0170789[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0103195[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0223381[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0101476[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0171152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0175826[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00793778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0102183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0103269[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0242796[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00936178[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0105377[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0081306[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0308936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0109849[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0172896[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0104467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00866188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0109206[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0316664[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0317731[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0176387[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00796648[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0103121[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.019251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00833041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0104697[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0309479[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0914443 seconds. Fastest Tactic: 0x22ebff09f6ab32eb Time: 0.00793778[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x22ebff09f6ab32eb[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0134357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0105173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0113013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.019008[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00876937[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00741096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00719637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0173456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0241623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0119402[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0332926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00804089[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0151898[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0228302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0175495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00730087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0314221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0136367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00921311[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0120762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0137489[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0145377[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0128919[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0158918[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0135927[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0312194[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0102642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.010283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0201682[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0128578[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.02208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00860855[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0123587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0245006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00918746[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0182299[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00879327[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00851147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00919092[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0140067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0316994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0242118[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0311632[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0335157[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.011808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0100822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.024461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00936741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0102377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0138458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00914508[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0135906[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.013568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0204028[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0242179[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0135194[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00827161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0242385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0135774[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0211513[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0335349[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0121707[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0126598[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0118301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.01776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0114791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0109426[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0311321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0172779[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0103677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00865832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0074368[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00855959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0243535[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00897853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0318293[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0157556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0142293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0318972[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0204066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0137937[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0312582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0241219[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00747164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136294[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00830855[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.232144 seconds. Fastest Tactic: 0x322f337abc345152 Time: 0.00719637[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x322f337abc345152[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0933493[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0705536[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00571263 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0705536[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0181243[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0334923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0141787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0127506[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.018867[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0114939 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0127506[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0611111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0148197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.060592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0175184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111071[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0322017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0317246[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0133978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0374187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0102581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.017488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0104453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0319971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0325333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0174672[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0114976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0114212[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.011414[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0141173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0245036[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0180923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0607467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0610204[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0703382 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.0102581[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.010495[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00939081[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0107943[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00965516[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0123603 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00939081[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00702622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0054804[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00536279[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00817251 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00536279[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0113291[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0039498[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00681774[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00456043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00705244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0111129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00539923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00713191[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0110424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00412049[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0042398[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00451171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00399962[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00532825[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00742613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00483107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00497799[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00383853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00422899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00422103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0108662[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0111719[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00685213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0068419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00516447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00386489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00412971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00448938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00479893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0058012[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00605543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00669845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0111289[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0950563 seconds. Fastest Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00383853[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65fbe45b4cb1d8a5[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.106496[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0919733[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00577214 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0919733[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0303874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0169552[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0407242[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0167728[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0303185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0304853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0123903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0168875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0168869[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0447067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0124041[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0168389[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0122933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0574098[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0175506[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0303787[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0170048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0130896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0174752[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.058384[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0588356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0310991[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0124132[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0171237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0341856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0128275[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0170459[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0573369[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0663891 seconds. Fastest Tactic: 0x08af511817b7463e Time: 0.0122933[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x08af511817b7463e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0233593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0170955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0188978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.03384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0135509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.010955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0108704[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0305648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0159878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0440573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.019754[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0159157[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0613867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0111353[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0253897[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0397843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0308451[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0102048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0578222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0237106[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0143364[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0207021[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0236316[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0245608[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0216053[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0277497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.016696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0234702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0577902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0292578[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0168635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00989615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0170123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0558347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0358571[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.021736[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.038912[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0136286[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0206218[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0295271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.0160701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0444613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0145085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0322579[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0136508[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0619413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.013328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.0294258[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0150618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0238629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0578631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0114158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.04422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0575271[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0616587[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0188984[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0168757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0443307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0145711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0169467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0238101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0144333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0236928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0236245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0362037[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.044268[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0235484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0113294[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.04424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0234816[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0362453[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0618667[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0208593[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.021136[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0191425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0308771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0178543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.017232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0574844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0303884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0168779[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.013638[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0103425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0134054[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0441867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0142982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0590613[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0271155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0238322[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0292818[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0591147[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0348469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0237006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0576196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0441613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0113045[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0292729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0235627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0121139[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0293431[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.307797 seconds. Fastest Tactic: 0x1d53511430a5d47e Time: 0.00989615[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1d53511430a5d47e[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.09392[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0711979[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00538743 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0711979[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0191479[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0361995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0149737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0135501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0199002[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0120027 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0135501[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0616622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0152204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0610933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0177971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111897[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0323258[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0317634[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0136649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0373618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0102271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0177061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0104178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0320592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0320349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0177791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0114396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0116579[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0116142[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0139018[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0245783[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0180278[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0613067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.06144[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0611093 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.0102271[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146616[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0120389[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0152366[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126677[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0111587 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0120389[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00640906[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00469558[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00451675[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00862129 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00451675[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.013896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0036509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00837707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0045231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00860198[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0135936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00573325[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00863563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0132275[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00426855[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00435366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00456782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00375407[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00577471[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.008992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00549543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00555965[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00381333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00405616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00404781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0134575[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0134903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00816729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00569998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00390636[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.0039045[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00393073[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00469634[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00678314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00595619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00819538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0134093[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.09051 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.0036509[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]}[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023])[0m
[38;5;13m[V] Compiler backend is used during engine build.[0m
[38;5;104m[X]  (foreignNode) Set user's cuda kernel library[0m
[38;5;104m[X] Subgraph compilation completed in 2.822 seconds.[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.04736[0m
[38;5;104m[X] {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023]) profiling completed in 2.84698 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.04736[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.0084728[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00838773[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00899705[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00859733[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00885867[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146946 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00838773[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00844774[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.0083744[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00898919[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00853387[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00885502[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0147183 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.0083744[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00593086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00783306[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00560978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00666923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00600172[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00677077[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00677931[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00663843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00743988[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00618924[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00672043[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00578465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00685714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00649395[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00605429[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00782462[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00586349[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00559574[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00560142[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00761745[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00653679[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00649662[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00583559[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.0063358[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00667883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00671893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00590952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00653971[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0815238 seconds. Fastest Tactic: 0x5693be686f87be25 Time: 0.00559574[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5693be686f87be25[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00563947[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00591944[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00784174[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00557867[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00629394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00667029[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00599791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00670421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00676629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00666562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00746193[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00616747[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00675605[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00581352[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00605448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00695133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00647405[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00604533[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00567033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0078333[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00579531[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00560516[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00560338[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00767927[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00579807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.00642646[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00651569[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00653231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.005728[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.0064759[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00678528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00740104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00578354[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.00560889[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00632292[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00654934[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00667648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00615195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00531251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0059331[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00692136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.00587472[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.118524 seconds. Fastest Tactic: 0xa6235a0b3508ed71 Time: 0.00531251[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa6235a0b3508ed71[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00537669[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00625659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00613818[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.005824[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00646564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00628109[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00674709[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00511855[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00504454[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00650851[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0074221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00674987[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00661375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.0055068[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.0060641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00609784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00553075[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00536313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00687695[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00560071[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00620603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00570197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00583761[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.0068049[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00566726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00532961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00499614[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00734609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0064519[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00560587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00573053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00560231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00705467[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00577527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00554317[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00611588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00547847[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00593179[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00580671[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00692419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00537143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00562525[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.006624[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00523333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00574713[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00585974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00616204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00747046[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00606313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00599581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00596248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00603333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00515216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00722678[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00571083[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00564711[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00516185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00609513[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00642298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00615797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00633177[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00565062[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.0053889[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00682231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00605753[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00589848[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00577545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00678464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00625798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00590316[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00671851[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00614575[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00613295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00621511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00507588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00572873[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0077765[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00562862[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00558027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00578887[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00752948[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0056144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00542434[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00567846[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00543449[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00690939[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00595013[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00583264[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00552551[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00600972[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00628504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00531386[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00527333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00610657[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00578336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00622519[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00589249[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00632111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00747022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00657799[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00791938[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00548214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00712851[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00700333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00561796[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00752758[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00627891[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00555034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00557387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00532131[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00688283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00620978[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.0054699[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00549281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00623072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00591738[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00549264[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00564942[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00598819[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.0054006[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00582014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00646687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00693094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00526733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00821229[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.365807 seconds. Fastest Tactic: 0x5e4918ccf433630e Time: 0.00499614[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4918ccf433630e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00536042[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00620247[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00616514[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00582161[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00646831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00623388[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00675285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00508687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00502973[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00642851[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0074413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00673877[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00668949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00552813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.006036[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00607884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00557422[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00538564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00686998[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00558471[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00618845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00570667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00582124[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.006768[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00565388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00534061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00493208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00736023[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00645395[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00555556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0057806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00554719[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00707[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00577398[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00560729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00610871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00548214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00597581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00579936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00691004[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00535518[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00561831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00665684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00517744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00571498[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00586293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00613624[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00740522[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00609358[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00596095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00599276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00605448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0051799[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00723745[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00569853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00564972[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00514544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00608504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00644308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00612054[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00634627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00564587[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00539407[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00679936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00608369[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00588033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00575301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00683407[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00626153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00587995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00673003[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00619674[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00606914[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00625817[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00508202[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00576993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00777997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00564676[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00560356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00568552[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00749416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00569058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00543364[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00568317[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00544069[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00689981[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00597505[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00584833[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.0055332[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00599219[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00629897[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00535416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.005281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00707244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00608155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.0057863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00624099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00596267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00629756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00745529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00656188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00780924[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00549438[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.0071823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.006998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00568353[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00749464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00625403[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00554142[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00557013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00532961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00692898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00619003[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00542641[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00549561[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00619536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00593778[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.0055026[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00558169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00599657[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00541574[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00575467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00643795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00694889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00524033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0081907[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.36173 seconds. Fastest Tactic: 0x5e4918ccf433630e Time: 0.00493208[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4918ccf433630e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00728812[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00723472[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00910184[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00848293[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00877726[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0144428 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00723472[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00733959[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00720681[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00912144[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00846347[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00878091[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0145193 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00720681[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00579605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00488471[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00529812[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00440688[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00563147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00408013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00429237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00737461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00439677[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00423021[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00411536[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00542606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00432711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0073018[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.0060705[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00483416[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.0043293[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00528233[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00422886[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00431849[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00738064[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00730736[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00540766[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00432588[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00413801[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00409262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00565966[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00428595[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0805712 seconds. Fastest Tactic: 0x22ebff09f6ab32eb Time: 0.00408013[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x22ebff09f6ab32eb[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00526883[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00578961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00494196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00530404[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00719432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00438737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00563627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00409912[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00431754[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00740823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00438821[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00422186[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00412155[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00537058[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00425492[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00432492[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00725913[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00613799[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00422224[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.004881[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00432752[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00529896[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00432232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00433371[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00591064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.0072295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00393211[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00734701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.00540267[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00724335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00428444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.004416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00539647[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.00531877[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00436945[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00393713[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00410237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00409288[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00563876[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0042854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00485627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.00425181[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.120756 seconds. Fastest Tactic: 0x788dd0382d5ebd44 Time: 0.00393211[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x788dd0382d5ebd44[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00567575[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00449251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00439537[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00470625[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.0079807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.0042217[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00822998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00537824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00541041[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00792856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00443607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00437638[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00821724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00463259[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00721679[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00443958[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00636136[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00427274[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00464948[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00506457[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00678037[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00454486[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00481234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00425357[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00468281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00557974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00527633[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.0055648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00763539[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00474096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00475989[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00625857[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.0052575[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00452166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00390313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00420613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00442344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00738458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00435615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00482209[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00455899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.0046877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00597219[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00476663[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00641867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00752237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00465881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00752024[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00426221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00668459[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00437084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00540456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00434466[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00442161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00407401[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00483447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00406425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00442021[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00504358[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00793752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00628504[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00505277[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00826146[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00450551[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00458725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0064361[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00448299[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0043107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00405102[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0044261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00413511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.004968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00460128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00385214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00474757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00584221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.0063195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00631427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00632534[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0046677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00642831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00622697[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00630682[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00600476[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00830959[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00458374[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00458594[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00447417[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0047058[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00482529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00580046[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00562045[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00486678[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00748302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00427829[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00439916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00415565[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00421827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00453593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00432656[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00461384[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00498659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00463644[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00453376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00654201[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00436239[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00764752[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00510966[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00467585[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00454717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0050041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00750838[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00460376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00657067[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.0057589[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00739687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00493929[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00610638[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00435685[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00732846[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00492957[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.0039164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00468237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00455812[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00389867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00540439[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.35854 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00385214[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00567105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00448555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00439677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00472173[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00794667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00420547[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00830543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00537058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00541179[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.0079107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00440744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00437901[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00803784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00470836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00722769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00603733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00637927[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00427247[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00464444[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00510384[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00679061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00453074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00483123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00420693[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00469481[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00558187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00531048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00558329[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00765818[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00467496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00481143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00631567[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00530099[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00453622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00388044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.0041888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00445326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00742376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00435491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0048521[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00456942[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00472939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00590148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00479406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00641805[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00751265[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0047043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00753469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00425829[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00670101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00441221[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00540559[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00436654[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00442007[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00408624[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00482849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00407141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00446478[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0050482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0079169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00630501[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00507798[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00826641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00455221[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00458871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00642236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00447573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00435615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.0040514[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00441347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00412708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00493678[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00460215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00382061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0047079[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00584276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00630923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00626133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00631728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00465911[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00643344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00619319[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00631587[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00593965[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00826407[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00458652[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00457921[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00447388[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00474411[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00481478[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00582124[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00572204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00484267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00747283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00431631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00442105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.0041542[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.004256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00452829[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00433053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00461969[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00493992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00464192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00452094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00650769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00439158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0076657[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00514757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00469526[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00453737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00497974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00752901[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00460318[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00658803[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00578722[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00739849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00495655[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.0061279[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00435879[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00734145[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00492831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00392308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00469889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00455294[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00387597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00540645[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.37432 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00382061[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27[0m
[38;5;104m[X] =============== Computing costs for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00974994[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00868869[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0100038[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00900519[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0126568 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00868869[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00591626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00451272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00438524[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00899273 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00438524[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0108711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00340408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00685845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00390933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00702178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.01064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00493035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00711943[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0101989[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00378104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00388738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00399187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00340365[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00492847[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00726864[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00465185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0047348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00346502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00376689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00361933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0105157[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0105917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00669824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00680511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0048035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00348156[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00354089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00359501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.004112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00563182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00507103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00667307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0104023[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0975276 seconds. Fastest Tactic: 0x5bd8221bd57baf93 Time: 0.00340365[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5bd8221bd57baf93[0m
[38;5;104m[X] =============== Computing costs for PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0025034[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00236823[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00234787[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00255397[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00234249[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00225074[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00298848[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00261558[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00249059[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00236944[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00247751[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0314838 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00225074[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00250348[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00239428[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00233816[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00257141[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00234458[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00226436[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00300877[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00259944[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00249966[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00236034[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00250971[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0295708 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00226436[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00249361[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00239398[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00233742[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00252182[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00235801[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00229757[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00297972[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00259175[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00249481[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00238094[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00249122[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0292712 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00229757[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00234757[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00248367[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00236327[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00273486[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00277175[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00263551[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00356142[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00342313[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00344303[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00314687[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00247451[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.0024956[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00236921[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00276628[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.0024866[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.00234734[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00345699[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.00286778[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.00278427[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.00277439[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00249003[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00248644[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00264147[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00317958[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.0023714[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00248525[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00248915[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0787148 seconds. Fastest Tactic: 0x000000000000000f Time: 0.00234734[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000000f[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00237685[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00248723[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00279733[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00355688[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00235568[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0143543 seconds. Fastest Tactic: 0x000000000000001f Time: 0.00235568[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001f[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.0035798[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00354538[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.0037216[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.00871257 seconds. Fastest Tactic: 0x000000000000001d Time: 0.00354538[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001d[0m
[38;5;104m[X] =============== Computing costs for /model/encoder/Resize[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,400,20,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00911423[0m
[38;5;104m[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.00226338 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00911423[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00251116[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00252255[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00274912[0m
[38;5;104m[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.00859536 seconds. Fastest Tactic: 0x0000000000000003 Time: 0.00251116[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000003[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146059[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0119387[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0150868[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0125843[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0105116 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0119387[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00675178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00508962[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0050657[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00833062 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.0050657[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0138607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00470655[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00832312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00464281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00849813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0133935[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00585469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00862167[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0111552[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00452886[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00460581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00461194[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00479939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00578979[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00907733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00558436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0056631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00443214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.004684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00437873[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0132197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0114027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00820683[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00820969[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00568461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00451762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00432396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0049418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00496643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00674688[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00600457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00807949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0115899[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0934021 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00432396[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0103105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00446549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0102507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0074688[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00446535[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00512372[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00712011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0047079[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00433745[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0104413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00434341[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00598153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00428677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00509915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00528417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00456216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00521783[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00441684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00418066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0103593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.010479[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00703466[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00549298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00713101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00696089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.0068813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00528667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00457439[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00432793[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00498245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0110637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.0042104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00550557[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0921701 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00418066[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.010933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00564462[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00701333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00555769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00531149[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0178959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00780998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.006964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0106273[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00534959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.01057[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0178403[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0072538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0108649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0068493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00592543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00543019[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00531691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0180907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.005245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0178751[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00721498[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0571593 seconds. Fastest Tactic: 0x6176c23707257237 Time: 0.005245[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6176c23707257237[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6176c23707257237, 0.005245 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0275356[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.021596[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00491288 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.021596[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0082188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0135561[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.006592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00632071[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0084408[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0126468 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00632071[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0209407[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00630601[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0188575[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00764679[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00559076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0122213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0117811[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00626983[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0134451[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00541763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00745031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00567069[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0119592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0120819[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00749511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00570088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00546151[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00573795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00634285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00939852[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00775224[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0207762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0190584[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.05855 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.00541763[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00301079[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00277042[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.0027506[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00288984[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00275861[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00279405[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00343445[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00305251[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.0028532[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00284393[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00306544[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0352368 seconds. Fastest Tactic: 0x0000000000000002 Time: 0.0027506[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000002[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00363548[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00418826[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00557813[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.0055075[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00662484[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.00976518[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00869662[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.0101867[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.0161509[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0194625[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00346458[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00332525[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00425694[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00594658[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00297043[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00315381[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00334613[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00412945[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00558258[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00353359[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00306848[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0547096 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00297043[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00604114[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00383399[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00601829[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00511402[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0040066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00389482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00488703[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00405976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00391169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00629192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00390499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00482301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00362174[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00388589[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00408936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00393725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00403238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00408481[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00348856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00620464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00620741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00470475[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00431029[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0048606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.004707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00458287[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00407935[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00375946[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00380715[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00375047[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0064798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00385692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00428567[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.102707 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00348856[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3[0m
[38;5;104m[X] =============== Computing costs for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00755887[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00457074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0075823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00596381[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00459485[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00441263[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00577067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00467437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00435782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00774788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00435934[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00530726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00407961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00432684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00467615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00444365[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00456317[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00402069[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00769285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00781172[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00559698[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00468133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00572258[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00551659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00544258[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00449095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00431767[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.0043014[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00436848[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00818576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00463615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00459777[0m
[38;5;104m[X] model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0957934 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00402069[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3[0m
[38;5;104m[X] =============== Computing costs for /model/encoder/Resize_1[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0294489[0m
[38;5;104m[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00195424 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0294489[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00359192[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00327718[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00412787[0m
[38;5;104m[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00864998 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00327718[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0151947[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0122156[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0159345[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0127542[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.011333 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0122156[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00855412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00820605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0103457[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0073979 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00820605[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0140396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0100866[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00974293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00929126[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00992282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.013896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00894175[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0100891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.012839[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.0083488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00843973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00943733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0102248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00818628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0108477[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00864328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00863563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00952089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0097088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00895972[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0134605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0129904[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00995608[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00992031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00810235[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00955855[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00878962[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0101867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00838453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00985192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00834427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00974903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0131705[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0915035 seconds. Fastest Tactic: 0x7720f198395e7d3d Time: 0.00810235[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7720f198395e7d3d[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,2560,32) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0106917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.0101615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0103396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00758424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00840213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00582566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00718525[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0102274[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00919812[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0105537[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00914825[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0074816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00662086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00617995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00664554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.010305[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00620563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00935289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00657401[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.010533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0107689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00713328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.0068136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.007328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00719909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00704511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00591645[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00692767[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00893305[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00607709[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0112135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00907906[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00620603[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0863109 seconds. Fastest Tactic: 0x458f02d2b10db57c Time: 0.00582566[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x458f02d2b10db57c[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0109767[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0102562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00723404[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0134545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00819616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0179919[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00819096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.008048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0107537[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0127095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0107153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0178931[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0073658[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0109344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00779882[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00852[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0131643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0125092[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0181923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0100571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0178302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00753398[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0542776 seconds. Fastest Tactic: 0xfdf7509af98902e0 Time: 0.00723404[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfdf7509af98902e0[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0xfdf7509af98902e0, 0.00723404 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0219013 ms[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x23b890da05937b9e, 0.00894709 ms[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0x2d8ab2aa0639fda9, 0.00950311 ms[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1), Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00525783[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00416987[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00413708[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00403354[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.0038337[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00370975[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00395005[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.0038937[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00388986[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00395043[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00523617[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0289369 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00370975[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00818966[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00662233[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00818992[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00735258[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00827499[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.0110283[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00914623[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.012048[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.0162397[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0200885[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00707533[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00653741[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00660622[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00686933[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00595924[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00603219[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00601334[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00662651[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00881347[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00692092[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00599086[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0459745 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00595924[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00621689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00708755[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00619832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00554597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00784496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00597276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00551292[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00759103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00683603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0063521[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.0073767[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00793451[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00599086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00618173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00651631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00721112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00632976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00761891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00581315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00636337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00634586[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00524633[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00701578[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00538133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.0052135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00511596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.0061764[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00646975[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00698466[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.0060301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00684147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00667925[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00638149[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0902569 seconds. Fastest Tactic: 0x65a38dbc9e991257 Time: 0.00511596[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65a38dbc9e991257[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0123017[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.012075[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.013033[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0127127[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0103262 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.012075[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.0113422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0100389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0116204[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00697595 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.0100389[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0145136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0101789[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0123375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0100223[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0125831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0140929[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0118867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0127743[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0135394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.010429[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.0104417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0101808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0103234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.01158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0127992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0112889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.011584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.010379[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0108614[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00983498[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0139324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0137566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0120114[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0123417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0113547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0104697[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00964327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.010611[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0112192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0139276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0121147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.012125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0138509[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0828242 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00964327[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0177729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0104165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0129087[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00843093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0313387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.011865[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.010448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0114891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0173131[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0314676[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0112668[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0104457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00859843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0128245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0114556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0318798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0102397[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0313542[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0107453[0m
[38;5;104m[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0525158 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.00843093[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0177718[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0104257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104777[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.012919[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.0084512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0312979[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0119029[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0104243[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.01748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.011472[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0173291[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0314104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0113042[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0104139[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.0085935[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0128595[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0114748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0319418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0102426[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0313251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.010724[0m
[38;5;104m[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0539552 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.0084512[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0xff6944b17d5b2e32, 0.0119387 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x5e4f6d7c83746fd6, 0.0050657 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x9ec201b34455146e, 0.00432396 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) [Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x2eba0b6a8ec55fa3, 0.00418066 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6176c23707257237, 0.005245 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6176c23707257237, 0.005245 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.021596 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x85c1a5f7f239cf84, 0.00632071 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x45f7566cdb2b10fb, 0.00541763 ms[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000002, 0.0027506 ms[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000018, 0.00297043 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x2eba0b6a8ec55fa3, 0.00348856 ms[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00982306[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00872807[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0101372[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00905023[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107631 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00872807[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00570938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00477623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00492706[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0083825 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00477623[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0108865[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00478415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00691657[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00534011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00711648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.010738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00557067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00714326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0103809[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00497678[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00508153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00549491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00494133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00552848[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00734191[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.005271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00539011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00518925[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00536093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00534891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0104042[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0104907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00687412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00684713[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00533875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00507652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00518909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00489755[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00530337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00634123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00565026[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00673856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.010532[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0909899 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00478415[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(6400,1:16,320,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178156[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0103357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00627635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00740916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0312747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0116465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0103282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00688043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0313193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0109505[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0175938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0102293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00859651[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00579182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00663634[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0318322[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00711716[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0312747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0106083[0m
[38;5;104m[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0544664 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00579182[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.017874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00762497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0103085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0062641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00738064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.031265[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0116903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0103493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00688392[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0172555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0313076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0110572[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0175349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0102436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00864629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00577306[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00664471[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0318041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00707822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0312378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0106357[0m
[38;5;104m[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0544973 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00577306[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146249[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0119863[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.015046[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.012589[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107916 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0119863[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00660141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00495231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00467733[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00884718 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00467733[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0137946[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00347052[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0082534[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00458214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00849733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0136405[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00576883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00859378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0111424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00430496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00441193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00454861[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00358105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00578501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00899481[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00553023[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00560018[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00371105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0040113[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00395507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0132164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0114066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00823415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00819564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00565084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00378631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00387605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00371023[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00471977[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00671296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00590091[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00807137[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0115303[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0953186 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00347052[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.010513[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00348256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0102106[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00740684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00428485[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00520582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00716028[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00384356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00376605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.010335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00403791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00591645[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.0041521[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00494039[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.0052375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00359226[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00514133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00398413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00406387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0102662[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0107806[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00695244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00537652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00704489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00692397[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00678698[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00533824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00443453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00392784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00481798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0110225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00372931[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00547322[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.096338 seconds. Fastest Tactic: 0xc6cdb1e47323bb01 Time: 0.00348256[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc6cdb1e47323bb01[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0109258[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00560231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00694889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00468785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.0052795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0179032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00783628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00700177[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.010619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00496282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0104267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0178408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00713078[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0107541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00686302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.0059581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0044581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00483695[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0181945[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00532978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0181934[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00725403[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0588256 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.0044581[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1)] got cached result: CaskConvolution, tactic 0xc985777c89c6b3a4, 0.0044581 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0270474[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.021578[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00521351 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.021578[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00817197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0134583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00658845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00631125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0084416[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0132387 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00631125[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0209587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00616475[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0187994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00754797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00437001[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0121059[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0116675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00614206[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0134784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00478491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00745955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00498467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.011992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0120145[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00747093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00550365[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00533977[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00465778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00618193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00944741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00775318[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0207824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0190519[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0665904 seconds. Fastest Tactic: 0xd14bd6d95fefd45e Time: 0.00437001[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd14bd6d95fefd45e[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,400,20,1), Float(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00241458[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.0025003[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00246612[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00279022[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00245914[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00238237[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00324909[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00288083[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00263106[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00249313[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00235478[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0320691 seconds. Fastest Tactic: 0x000000000000001c Time: 0.00235478[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1600,400:32,20,1), Float(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00328785[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00396047[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00547707[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00546168[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00670976[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.0093914[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00856643[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.0105637[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.0146838[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0191283[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00274885[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00317062[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00413103[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00591401[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00245843[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00258538[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00316357[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00413708[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00512145[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00290885[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00250229[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0528553 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00245843[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(6400,1:16,320,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00605848[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00304175[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00604514[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00494965[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00357901[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00388416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.0048163[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00334389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00318151[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00620148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00346425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00451978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00331628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00376701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.0040141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00318415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00397638[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0033989[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00326808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00609009[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00618252[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00459748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00415262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00480579[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.0046837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00454328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00400762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00347878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00332314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00364974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0064521[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00319208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00411825[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.100836 seconds. Fastest Tactic: 0xc6cdb1e47323bb01 Time: 0.00304175[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc6cdb1e47323bb01[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00978956[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00868868[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0100436[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00902544[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107421 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00868868[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00561227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.004182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00405989[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00869717 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00405989[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0108163[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00334123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00682971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00389681[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0071133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0106397[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0048456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00709991[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0101744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00377648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00386881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00399162[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00337519[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00490274[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00725449[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00461691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0047028[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00344237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00375323[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00361956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0103832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.01043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00668117[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00670997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00482865[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00351035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00348878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00359788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00414538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00565695[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00501747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00665391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0103476[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0906181 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00334123[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]}[0m
[38;5;104m[X] *************** Autotuning format combination: Int64(2,1), Float(1638400,6400,80,1), Float(409600,1600,40,1), Float(102400,400,20,1) -> Float(57600,192,24,2,1), Float(57600,192,24,2,1), Float(57600,192,24,2,1), Float(28800,96,12,1), Float(28800,96,12,1), Float(28800,96,12,1), Float(28800,96,1), Float(28800,96,1), Float(28800,96,1), Float(57600,192,1), Float(57600,192,1), Float(57600,192,1), Int64(300,1), Float(1200,4,1), Float(300,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023])[0m
[38;5;104m[X]  (foreignNode) Set user's cuda kernel library[0m
[38;5;104m[X] Subgraph compilation completed in 7.279 seconds.[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.434859[0m
[38;5;104m[X] {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023]) profiling completed in 7.40593 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.434859[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00483308[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551851[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00487822[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00841031 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00483308[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00929629[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005608[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00400089[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00725948 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00400089[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0287449[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0239985[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0287636[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00522715 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0239985[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.216352[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0188041[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.216235[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00608861 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0188041[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.010132[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00592281[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0101032[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00664543 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00592281[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0293751[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0251893[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.029368[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00553333 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0251893[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.215931[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0226048[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.215749[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00636702 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0226048[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00849867[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00990933[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00849147[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00721978 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00849147[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0299938[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0120011[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0299298[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00624632 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0120011[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.216117[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.030657[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.015904[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0060664 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.015904[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00885642[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0501166[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00884856[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00693284 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00884856[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.010732[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0142551[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0107183[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00722801 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0107183[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.217259[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0240602[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.217413[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00681223 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0240602[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0116962[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0502766[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0117451[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00725877 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0116962[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129846[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0143133[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00520484[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00858233 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00520484[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0300044[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0120819[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0300524[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00691779 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0120819[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0260078[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0117013[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0260636[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00667422 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0117013[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0640569[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0122053[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0640213[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00670965 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0122053[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00967497[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0122491[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00471226[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00867853 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00471226[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0199027[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0113582[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0198783[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00678845 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0113582[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0117013 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0122053 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00471226 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0113582 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(1638400,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0498789[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0117767[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0500175[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00668496 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0117767[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.138802[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.012112[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.138802[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00703077 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.012112[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,102400:32,320,1) -> Int8(1638400,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0153581[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0157435[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00649292[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00794334 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00649292[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0105667[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0116443[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0105517[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00751201 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0105517[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0113504[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0108126[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0113198[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00723711 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0108126[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0287636[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0107819[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00408156[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00805935 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00408156[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0145414[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00958293[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0145104[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00681697 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00958293[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00685518[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00852853[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00363687[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00898526 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00363687[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0113205[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00664283[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0113461[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.0071124 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00664283[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.010526[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00625758[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00480289[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00809605 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00480289[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.012011[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0069416[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0120019[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00628878 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0069416[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0305387[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00700244[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0305028[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00591235 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00700244[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0112892[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00769891[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0113358[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0066201 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00769891[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0289502[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00804876[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00407089[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00719043 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00407089[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0148132[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00772412[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0148132[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00637025 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00772412[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0373416[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00782115[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0374234[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00590029 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00782115[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00683929[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00812114[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00359547[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00808182 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00359547[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.011382[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00796165[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0113685[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00650605 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00796165[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00772412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00782115 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00359547 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00480289 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00700244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00407089 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00772412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00782115 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00359547 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.04566[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00764364[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0456213[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00644037 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00764364[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0244267[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00791789[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0243703[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00622865 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00791789[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.04562[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00854838[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.04556[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00616753 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00854838[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0243581[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00882948[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0222279[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00616274 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00882948[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00522467[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00850213[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00526233[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00769384 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00522467[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00841333[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00766861[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00376582[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00760512 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00376582[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00931259[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00697556[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00930904[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00659625 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00697556[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0279627[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00700866[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0279591[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00592863 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00700866[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.022309[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00728417[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0222[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00605879 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00728417[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0224128[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00729484[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0223794[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00607503 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00729484[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00932652[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00772751[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00931526[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00673852 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00772751[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0409102[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00767515[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0408841[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00616409 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00767515[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00480289 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0069416 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00700244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00769891 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00407089 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00772412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00782115 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00359547 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00796165 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00772412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00782115 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00359547 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00764364 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00791789 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00522467 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00376582 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00697556 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00700866 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00728417 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00729484 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00772751 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00767515 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00480289 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00700244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00407089 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00772412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00782115 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00359547 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00480289 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0069416 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00700244 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00769891 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00407089 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00772412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00782115 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00359547 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00796165 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0128008[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00693116[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0127818[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00650961 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00693116[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0225124[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00706978[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0225756[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00600955 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00706978[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00465467[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00716573[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00288101[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00799304 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00288101[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:4,80,1) -> Int8(12800,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00948[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00599162[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0025968[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0073133 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0025968[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,6400:32,80,1) -> Int8(102400,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00354212[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557067[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00260325[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00830411 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00260325[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0219993[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00532402[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0220213[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0062508 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00532402[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0122914[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00533587[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0122846[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00646119 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00533587[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.022048[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00622953[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.022036[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00633263 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00622953[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.01228[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00761357[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.012299[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00655319 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00761357[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.003816[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00723154[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00371295[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00845972 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00371295[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00539854[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00658991[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00291534[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00767584 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00291534[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00979809[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549491[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00971946[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0065536 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00549491[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0152945[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00591626[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0152975[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00619051 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00591626[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0125796[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00657903[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0125614[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00633972 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00657903[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0126574[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00685431[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0126609[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00641296 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00685431[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00573795[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00648554[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00576957[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00739644 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00573795[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0220987[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0065393[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0221447[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00613827 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0065393[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00620129[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00688871[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00330762[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00782471 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00330762[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00930992[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00630964[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00924656[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00668532 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00630964[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0159177[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00655101[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0157057[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00618848 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00655101[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00646031[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00677547[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00641107[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00705054 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00641107[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0155093[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00709696[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00315673[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00714029 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00315673[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00693116 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00706978 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288101 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0063352[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00631024[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00634425[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00696922 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00631024[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00693116 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00706978 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288101 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00532402 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00533587 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00371295 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00291534 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00549491 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00591626 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00657903 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00685431 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00573795 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0065393 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00330762 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00655101 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00315673 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00693116 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00706978 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288101 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00330762 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00630964 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00655101 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00641107 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00315673 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00693116 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00706978 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288101 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00631024 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00800965[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00569456[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00809626[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00684367 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00569456[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0130905[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0058706[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0130462[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00627784 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0058706[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00355461[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00571553[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00261592[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00840615 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00261592[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00618074[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00569401[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00251124[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00746177 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00251124[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00311606[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00575283[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00261308[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00827807 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00261308[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0123907[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561085[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0123745[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0063182 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561085[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00742969[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552131[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00746643[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00680101 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552131[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0124255[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558045[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0123994[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00630531 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558045[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00741025[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551065[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00741078[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00685256 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551065[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00304078[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547375[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00302847[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0085196 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00302847[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0039429[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055584[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00255731[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00801604 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00255731[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00696911[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561707[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.006988[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00680727 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561707[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00912951[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055584[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00911683[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00655661 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055584[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00751004[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554789[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00754252[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00681305 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554789[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00762351[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545927[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00766085[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00681951 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00545927[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00413748[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562062[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00415091[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00787092 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00413748[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0126629[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547795[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0126475[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0063777 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00547795[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00441796[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559058[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00290477[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00777072 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00290477[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00734075[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562809[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00734771[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00698348 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562809[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00946874[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00579458[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00945126[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00656247 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00579458[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00469514[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00574915[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0047022[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00751467 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00469514[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00932267[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00572891[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00324132[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00753701 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00324132[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00569456 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0058706 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261592 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00449322[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00564462[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00444729[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00790939 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00444729[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00569456 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0058706 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261592 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(409600,1600,40,1) -> Float(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561085 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552131 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00302847 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00255731 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561707 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0055584 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554789 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00545927 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00413748 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00547795 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00290477 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00579458 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00324132 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00569456 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0058706 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261592 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00290477 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562809 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00579458 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00469514 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00324132 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00569456 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0058706 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261592 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x0000000000000000, 0.00444729 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00581002[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00582492[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00579752[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00723421 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00579752[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00819148[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00564942[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00818992[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00675248 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00564942[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00313351[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00568353[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00242914[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00820814 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00242914[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00455034[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00575356[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00263803[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0080486 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00263803[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00289892[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00576589[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00229611[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00813637 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00229611[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0072749[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554877[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00728464[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00689843 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554877[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00504804[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541643[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0050326[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00755257 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0050326[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00722859[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551082[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00732174[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00689288 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551082[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00505778[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541247[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00507345[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00742791 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00505778[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00262694[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00532521[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00264164[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0085152 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00262694[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00314268[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554702[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00238179[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0082621 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00238179[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00517809[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559787[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00516677[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00730556 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00516677[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00614051[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556622[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00609048[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0070533 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00556622[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00502639[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554107[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00504104[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00740101 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00502639[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0051019[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559591[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00507556[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00738542 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00507556[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00332969[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055381[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0033279[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00836535 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0033279[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00791591[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558951[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00793352[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00674429 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558951[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00333402[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00569853[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00262075[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00855082 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00262075[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00564587[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562525[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0056272[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00737431 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562525[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00615719[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00566671[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00616456[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00711772 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00566671[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00376246[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567304[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00374975[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0079723 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00374975[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00615486[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00611084[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00299876[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00754387 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00299876[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00579752 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564942 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00242914 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00348511[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565406[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00349189[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00824242 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00348511[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00579752 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564942 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00242914 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(204800,400,20,1) -> Float(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554877 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(6400,400:32,20,1) -> Float(204800,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0050326 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00238179 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00556622 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00507556 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00558951 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299876 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00242914 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00372468[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00537414[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00374963[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00813519 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00372468[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00330762 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00630964 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00655101 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00641107 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00315673 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00693116 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00706978 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288101 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00631024 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0226894[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00631527[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0226382[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00604327 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00631527[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0232178[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00569546[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0232078[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00619295 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00569546[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.04208[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00585469[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.042028[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00583583 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00585469[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0128779[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00619259[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0128874[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00628252 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00619259[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00626469[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00611976[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00332599[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00784966 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00332599[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102413[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00613663[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.010241[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00650992 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00613663[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0221347[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00638128[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0223815[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00619018 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00638128[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0227804[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00705378[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0227243[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00607915 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00705378[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102652[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00707889[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102723[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00661858 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00707889[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0418507[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00718116[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.041944[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00580926 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00718116[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129157[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00741428[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0129329[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00628648 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00741428[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00633761[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00704733[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00334891[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00796975 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00334891[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102684[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00658322[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102607[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00658139 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00658322[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0273698[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00690743[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00384098[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0070853 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00384098[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00290477 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562809 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00579458 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00469514 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00324132 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00569456 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0058706 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261592 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x0000000000000000, 0.00444729 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00877128[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00601486[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00873217[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00681477 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00601486[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00893137[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00632976[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00893726[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00664457 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00632976[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0139813[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00643487[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0138645[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00623341 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00643487[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00571028[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00656272[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00573361[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00722367 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00571028[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00352977[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00600857[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00261117[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0083556 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00261117[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00443958[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00577692[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00445113[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00768253 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00443958[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0077946[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554964[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00780205[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00689948 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554964[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00809829[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562418[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00803759[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00678242 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562418[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00441095[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567828[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00443846[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00766593 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00441095[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.013111[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00571751[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0131569[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00632362 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00571751[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00567883[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567232[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00566274[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00717963 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00566274[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00356755[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005776[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00258563[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00826664 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00258563[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00441207[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00564107[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00441347[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00774071 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00441207[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0100339[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00568841[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00274028[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00749475 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00274028[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279725[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560249[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00225463[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00880256 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00225463[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00434161[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005624[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00432451[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00768779 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00432451[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00263803 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00229611 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00348878[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557031[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00238956[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00808825 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00238956[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00263811[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554405[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0052295[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0078187 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00263811[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00355597[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554404[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00237163[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00804632 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00237163[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00488719[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558827[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00486323[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00753411 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00486323[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00439691[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563396[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00527617[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00747594 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00439691[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00372468 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00273425[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551834[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00270675[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00836583 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00270675[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00371971[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00542452[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0037286[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00808487 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00371971[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00275993[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549403[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00277642[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00884411 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00275993[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0112263[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00590035[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0112324[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00644576 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00590035[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00349478[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557547[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00242497[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00807017 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00242497[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00262694[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558827[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00524033[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00757507 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00262694[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00356051[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559716[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00240361[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00805311 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00240361[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00485843[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563858[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.004896[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00752754 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00485843[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00442779[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561209[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0052485[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00747741 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00442779[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00367273[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559467[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00240069[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00804634 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00240069[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00365647[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055005[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00363247[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00822673 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00363247[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00263173[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559787[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00265871[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.008111 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00263173[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0058487[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560178[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00583779[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00716935 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00560178[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0110166[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00587004[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0110073[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00650703 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00587004[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00235808[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557085[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00548004[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00748075 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00235808[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.003591[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551816[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00357458[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00818148 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00357458[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00361829[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560569[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00360637[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00814291 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00360637[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00489615[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00550418[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00490525[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00751718 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00489615[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00439944[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0165892[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00444224[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00710338 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00439944[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00368035[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565605[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0023451[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00802606 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0023451[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00267913[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055437[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00270073[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0083723 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00267913[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00368282[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557405[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00368832[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00811 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00368282[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00586854[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552376[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00587135[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0071764 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552376[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0112764[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00591832[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0112775[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00681859 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00591832[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00373416[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551327[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0037331[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00803738 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0037331[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00268459[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554877[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00265134[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0083398 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00265134[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00373476[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545342[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00372551[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00807915 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00372551[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279236[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0056176[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00276126[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00880482 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00276126[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0112039[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00593029[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0112384[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00643364 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00593029[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00283625[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0158565[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.005504[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00756721 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00283625[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00359421[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0170085[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00361864[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00748659 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00359421[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00280901[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0160361[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00280125[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00829325 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00280125[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0036105[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0170171[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00361887[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00754877 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0036105[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00485534[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0170149[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00487791[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00689242 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00485534[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00237208[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547445[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00236928[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00811771 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00236928[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(102400,400,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00432451 ms[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.003656[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0056734[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00366945[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0079801 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.003656[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0053628[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562773[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00536804[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00728396 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0053628[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00234555[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00271346 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00234555[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0043405[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00252492 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0043405[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00372054[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561991[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00372219[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00795605 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00372054[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00540353[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00550767[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00539802[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00728563 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00539802[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00375707[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556107[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00376641[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00785638 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00375707[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00527183[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005543[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0052575[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00730877 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0052575[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279938[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00323694 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00279938[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00433884[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00251335 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00433884[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0045538[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562613[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00453261[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00765005 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00453261[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00334912[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00573451[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00335221[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00831287 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00334912[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00278089[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559004[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00277078[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00905942 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00277078[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00440519[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560445[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00293025[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00828301 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00293025[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00734191[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555751[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00736023[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00693714 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00555751[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00948474[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00572348[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.009456[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00654148 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00572348[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00572927[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055423[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00571318[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00723869 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0055423[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00354712[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00570522[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00262242[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00817695 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00262242[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00442217[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561689[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00442947[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00771258 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00442217[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00970148[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00566581[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00274562[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00832291 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00274562[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00626805[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556818[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00334123[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00773883 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00334123[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0160554[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565116[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0159218[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00615119 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00565116[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0153658[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558845[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00400165[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00705944 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00400165[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0138969[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00564356[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0138722[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00631227 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00564356[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0236501[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00584[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0237362[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00640641 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00584[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00460814[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00600552[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00288634[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00807023 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00288634[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00565116 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00400165 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261308 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,1600,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00735327[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559467[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00732452[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00682225 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00559467[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0050431[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545841[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00501795[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0074495 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00501795[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559467 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00501795 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00621847[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00571932[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00620879[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00698035 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00571932[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00570269[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561351[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00571571[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006]) profiling completed in 0.00723037 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00561351[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00441207[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560018[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00441923[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006]) profiling completed in 0.00771778 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00441207[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00579458 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00777005[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554614[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0077822[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00684331 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554614[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0058706 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00572547[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00579881[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00572222[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00712478 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00572222[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00359501[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553303[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00359306[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00819263 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00359306[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00974049[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565894[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00434632[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00717333 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00434632[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0205911[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00628385[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0206055[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00615792 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00628385[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0284427[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00599524[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0284978[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00601237 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00599524[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129112[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00633479[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0129325[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.0062668 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00633479[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00630964[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00642134[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00338608[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00774912 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00338608[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102691[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00609901[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102452[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00654672 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00609901[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.027383[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0127447[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00465926[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00692646 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00465926[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0168011[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0161051[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00623447[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00722992 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00623447[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0536579[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0202329[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.053792[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00690159 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0202329[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0483093[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00912346[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0111406[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00659973 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00912346[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0437[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0162656[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0437493[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00627019 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0162656[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0805013[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.02255[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0805013[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00761355 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.02255[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00974141[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.022076[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00484498[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00919467 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00484498[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0202329 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00912346 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.02255 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288101 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00532402 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00533587 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00532402 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00533587 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00655101 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129284[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0120103[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0128661[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00670497 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0120103[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00630199[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0102216[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00340256[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00810045 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00340256[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102623[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00805638[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102749[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00667952 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00805638[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00973989[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00768848[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00435823[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00763434 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00435823[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0285004[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00702889[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0284916[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00596097 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00702889[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0264361[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00722587[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00447061[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00698362 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00447061[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0227641[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00714712[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.022789[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0060662 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00714712[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0420227[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00737646[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0418693[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00587186 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00737646[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00630461[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00737809[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.003344[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00798485 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.003344[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00702889 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00447061 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00737646 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00571028 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261117 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00443958 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00566274 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00258563 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00441207 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00274028 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279565[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552971[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00279022[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00855489 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00279022[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00441235[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547445[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00291673[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.0081638 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00291673[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00733009[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054326[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00735096[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00693196 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0054326[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00955032[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544086[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00957044[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00646846 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544086[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00772388[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00537755[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00776161[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00684076 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00537755[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0079713[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054755[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00804165[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00671762 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0054755[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00441277[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553583[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00440351[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00766665 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00440351[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0131467[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557334[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0131372[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00629156 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557334[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00570613[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548004[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00572836[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00739895 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00548004[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00354459[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552201[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00261708[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00851772 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00261708[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00441263[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554282[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00441642[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00766415 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00441263[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0100035[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558649[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00275588[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00763331 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00275588[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00334123 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00565116 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00400165 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564356 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00288634 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00565116 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00400165 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00584 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261308 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559467 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00501795 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00559467 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00501795 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571932 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) [Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00561351 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00355155[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054692[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00262694[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00841468 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00262694[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00441207 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00290477 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00579458 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00324132 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00569456 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0058706 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00261592 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00579458 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00324132 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0058706 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00337476[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548686[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00334112[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00824736 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00334112[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00288478[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545635[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00232341[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00838481 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00232341[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00318659[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551711[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00319299[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00823768 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00318659[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00338683[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00550015[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00336459[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00825071 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00336459[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00288837[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545118[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00231948[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00822275 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00231948[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00318263[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055061[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00317775[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00832207 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00317775[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00471767[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00550155[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00248399[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00764628 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00248399[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00238194[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005259[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00237178[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00793819 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00237178[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00277805[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00531776[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00224329[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00885675 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00224329[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00378032[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00532284[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00375335[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00789003 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00375335[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00437749[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005282[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00437098[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00761203 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00437098[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00362319[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00531505[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00363826[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0079952 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00362319[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00380788[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0052825[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00376462[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00797214 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00376462[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00288478[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541626[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00290161[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00881869 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00288478[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00534434[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00527967[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00533283[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00726346 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00527967[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00237307[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00261595 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00237307[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279627[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00318604 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00279627[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00377612[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00264556 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00377612[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00436266[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00250635 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00436266[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00371757[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00539407[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00373559[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0079099 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00371757[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00378691[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055068[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00379115[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00795878 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00378691[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00294643[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00540129[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00293137[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00878206 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00293137[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00532944[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0053963[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00534637[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00725932 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00532944[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00379212[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00530641[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00379661[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00780045 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00379212[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00374748[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00524317[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00376893[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0079526 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00374748[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00295033[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544619[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00289986[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00878845 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00289986[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.005286[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005216[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00531539[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00727735 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.005216[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00279493[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00303309 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00279493[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00282784[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00300097 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00282784[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00380037[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.002658 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00380037[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00442288[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00249662 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00442288[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00262075 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566671 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299876 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00579752 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564942 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00242914 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566671 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299876 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00564942 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1600,400:32,20,1) -> Int8(12800,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00271896[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543398[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00219182[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00848162 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00219182[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,400,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00361566[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00533858[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00359765[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00812521 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00359765[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1600,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00307132[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00521417[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00305348[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.008607 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00305348[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(51200,400,20,1) -> Float(1600,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00359765 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(1600,400:32,20,1) -> Float(51200,400,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00305348 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00373452[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563147[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00371911[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00799469 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00371911[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00229611 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0220727[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0068197[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0220767[0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0059838 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0068197[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.1/conv/Conv_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552131 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/Conv_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00372468 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] Formats and tactics selection completed in 24.2602 seconds.[0m
[38;5;104m[X] After reformat layers: 85 layers[0m
[38;5;104m[X] Total number of blocks in pre-optimized block assignment: 81[0m
[38;5;13m[V] Detected 2 inputs and 15 output network tensors.[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Host Persistent: 4880 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/MaxPool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Host Persistent: 4944 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 13107200 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Host Persistent: 308 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 54067200 bytes[0m
[38;5;104m[X] Skipped printing memory information for 17 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.[0m
[38;5;13m[V] Total Host Persistent Memory: 307312 bytes[0m
[38;5;13m[V] Total Device Persistent Memory: 0 bytes[0m
[38;5;13m[V] Max Scratch Memory: 54067200 bytes[0m
[38;5;13m[V] [BlockAssignment] Started assigning block shifts. This will take 82 steps to complete.[0m
[38;5;104m[X] STILL ALIVE: Started step 76 of 82[0m
[38;5;13m[V] [BlockAssignment] Algorithm ShiftNTopDown took 1.26625ms to assign 6 blocks to 82 nodes requiring 63129600 bytes.[0m
[38;5;104m[X] Total number of blocks in optimized block assignment: 6[0m
[38;5;13m[V] Total Activation Memory: 63129600 bytes[0m
[38;5;13m[V] Total Weights Memory: 23935536 bytes[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Set kernel index: 0[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Set kernel index: 1[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Set kernel index: 2[0m
[38;5;104m[X] Finalize: /model/backbone/MaxPool Set kernel index: 3[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Set kernel index: 4[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Set kernel index: 5[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Set kernel index: 6[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Set kernel index: 4[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Set kernel index: 7[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Set kernel index: 8[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Set kernel index: 9[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Set kernel index: 10[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Set kernel index: 11[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Set kernel index: 2[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Set kernel index: 12[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Set kernel index: 8[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Set kernel index: 13[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Set kernel index: 10[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Set kernel index: 11[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Set kernel index: 13[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Set kernel index: 10[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Set kernel index: 8[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Set kernel index: 14[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Set kernel index: 15[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Set kernel index: 16[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Set kernel index: 17[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Set kernel index: 15[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Set kernel index: 18[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Set kernel index: 19[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Set kernel index: 20[0m
[38;5;104m[X] Finalize: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Set kernel index: 18[0m
[38;5;104m[X] Finalize: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Set kernel index: 21[0m
[38;5;104m[X] Finalize: /model/encoder/Resize Set kernel index: 22[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Set kernel index: 6[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 15[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Set kernel index: 25[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: /model/encoder/Resize_1 Set kernel index: 26[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Set kernel index: 27[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Set kernel index: 28[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 29[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 29[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 10[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Set kernel index: 25[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Set kernel index: 30[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Set kernel index: 31[0m
[38;5;104m[X] Finalize: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Set kernel index: 32[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Set kernel index: 6[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 15[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Set kernel index: 25[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Set kernel index: 31[0m
[38;5;104m[X] Finalize: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Set kernel index: 33[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Set kernel index: 11[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Set kernel index: 34[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 33[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 33[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 35[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Set kernel index: 25[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Set kernel index: 34[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Set kernel index: 18[0m
[38;5;104m[X] Total number of generated kernels selected for the engine: 36[0m
[38;5;104m[X] Kernel: 0 CASK_STATIC[0m
[38;5;104m[X] Kernel: 1 CASK_STATIC[0m
[38;5;104m[X] Kernel: 2 CASK_STATIC[0m
[38;5;104m[X] Kernel: 3 CASK_STATIC[0m
[38;5;104m[X] Kernel: 4 CASK_STATIC[0m
[38;5;104m[X] Kernel: 5 CASK_STATIC[0m
[38;5;104m[X] Kernel: 6 CASK_STATIC[0m
[38;5;104m[X] Kernel: 7 CASK_STATIC[0m
[38;5;104m[X] Kernel: 8 CASK_STATIC[0m
[38;5;104m[X] Kernel: 9 CASK_STATIC[0m
[38;5;104m[X] Kernel: 10 CASK_STATIC[0m
[38;5;104m[X] Kernel: 11 CASK_STATIC[0m
[38;5;104m[X] Kernel: 12 CASK_STATIC[0m
[38;5;104m[X] Kernel: 13 CASK_STATIC[0m
[38;5;104m[X] Kernel: 14 CASK_STATIC[0m
[38;5;104m[X] Kernel: 15 CASK_STATIC[0m
[38;5;104m[X] Kernel: 16 CASK_STATIC[0m
[38;5;104m[X] Kernel: 17 CASK_STATIC[0m
[38;5;104m[X] Kernel: 18 CASK_STATIC[0m
[38;5;104m[X] Kernel: 19 CASK_STATIC[0m
[38;5;104m[X] Kernel: 20 CASK_STATIC[0m
[38;5;104m[X] Kernel: 21 TRT_SERIALIZABLE:generatedNativePointwise[0m
[38;5;104m[X] Kernel: 22 TRT_SERIALIZABLE:ResizeVectorizedNearestKernel[0m
[38;5;104m[X] Kernel: 23 CASK_STATIC[0m
[38;5;104m[X] Kernel: 24 CASK_STATIC[0m
[38;5;104m[X] Kernel: 25 TRT_SERIALIZABLE:generatedNativePointwise[0m
[38;5;104m[X] Kernel: 26 TRT_SERIALIZABLE:ResizeVectorizedC4x4NearestKernel[0m
[38;5;104m[X] Kernel: 27 CASK_STATIC[0m
[38;5;104m[X] Kernel: 28 CASK_STATIC[0m
[38;5;104m[X] Kernel: 29 CASK_STATIC[0m
[38;5;104m[X] Kernel: 30 CASK_STATIC[0m
[38;5;104m[X] Kernel: 31 CASK_STATIC[0m
[38;5;104m[X] Kernel: 32 CASK_STATIC[0m
[38;5;104m[X] Kernel: 33 CASK_STATIC[0m
[38;5;104m[X] Kernel: 34 CASK_STATIC[0m
[38;5;104m[X] Kernel: 35 CASK_STATIC[0m
[38;5;13m[V] Compiler backend is used during engine execution.[0m
[38;5;104m[X] Disabling unused tactic source: JIT_CONVOLUTIONS[0m
[38;5;13m[V] Engine generation completed in 24.8382 seconds.[0m
[38;5;104m[X] Layers:
    Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: images, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 864}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8, TacticValue: 0x5cc792a989a1d1a6, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_1/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 9216}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3, TacticValue: 0x13463e9bf9ae0d73, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_2/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 18432}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x214f03e23f252333, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_3/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear]
    Name: /model/backbone/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/MaxPool]
    Name: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xb936321f82fd390c, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 4096}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x0e07dc8353bf7e9f, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 73728}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x705baf38e41eee0b, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 8192}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x214f03e23f252333, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xad886d4d69834922, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 294912}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 1179648}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32, TacticValue: 0x322f337abc345152, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x65fbe45b4cb1d8a5, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x1d53511430a5d47e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/act/Relu]
    Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye9025_0_myl37_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddResTra_myl37_1, LayerType: kgen, Inputs: [ { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_MulAddResTra_0x862813689358e08ec79eab32f31fafdf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/encoder/Reshape][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1]
    Name: __mye8942_myl37_2, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_TraAdd_myl37_3, LayerType: kgen, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Constant_output_0_constantFloat, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_TraAdd_0x5a9388c92c5b2a167638420a28fa3cf0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Transpose][ONNX Layer: /model/encoder/encoder.0/layers.0/Add]
    Name: __mye8944_myl37_4, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_2_myl37_5, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8387_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8277/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8278/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8626_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2]
    Name: __mye8946_myl37_6, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_1+/model/encoder/encoder_0/layers_0/self_attn/MatMul_myl37_7, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8849dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye8319/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8320/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8774_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye8684, Dimensions: [2,400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add]
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_3_myl37_8, LayerType: gemm, Inputs: [ { Name: __mye8684, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8684, Dimensions: [8,32,400], Format/Datatype: Float }, { Name: __mye8642, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye8333/model/encoder/encoder_0/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x128x16_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl37_9, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x486901888507314d28178a529899ff30, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax]
    Name: __mye8948_myl37_10, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_myl37_11, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8343/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8344/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl37_12, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0x053154cc4b930530fcf23b0caf04c63a, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3]
    Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_myl37_13, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8854dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8357/model/encoder/encoder_0/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8358/model/encoder/encoder_0/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_116_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl37_14, LayerType: kgen, Inputs: [ { Name: __mye8585_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8575_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye9021_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x81c6f38dc18b20647aef42cb9b16a94b, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/encoder/encoder_0/layers_0/linear1/MatMul_myl37_15, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Int8 }, { Name: __mye8859dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye8646_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye8653zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear1_bias _ ONNXTRT_Broadcast_131_constantFloat, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_gelu_erf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Div][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Erf][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl37_16, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }, { Name: __mye8864dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye8657_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8664zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear2_bias _ ONNXTRT_Broadcast_145_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/Add_2]
    Name: __myl_ResMeaSubMulMea_myl37_17, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMea_0xade3a566ff3432c2f2753f66a7f593a6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization]
    Name: __myl_AddSqrDivMulMulAddResTra_myl37_18, LayerType: kgen, Inputs: [ { Name: __mye8539_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8549_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Reshape_1_output_0, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_AddSqrDivMulMulAddResTra_0x0caebe133d43683f5670c896620d9227, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization][ONNX Layer: /model/encoder/Transpose_1]
    [ONNX Layer: /model/encoder/Reshape_1]
    Name: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x5e4918ccf433630e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x483ad1560c6e5e27, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Reshape_1_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.0/act/Mul]
    Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    Name: /model/encoder/Resize, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000003, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize]
    Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_2]
    Name: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/Resize_1, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize_1]
    Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_3]
    Name: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x7720f198395e7d3d, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x65a38dbc9e991257, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/Resize_1_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_4]
    Name: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6176c23707257237, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/Add]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xd14bd6d95fefd45e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/Add]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye157995_0_myl84_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^signal^1_myl84_1, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^wait^1_myl84_2, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl84_3, LayerType: kgen, Inputs: [ { Name: __mye155438_dconst, Dimensions: [1,1,6400], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xb7911a963641d99b9b7644b75b6b02a0, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulMinMaxRouCasResTra_myl84_4, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: __mye157895_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x53ec280dcdcbc7be42089db5a99e26ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl84_5, LayerType: kgen, Inputs: [ { Name: __mye155461_dconst, Dimensions: [1,1,1600], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xc7826108fa2ff5e34bf8bfa07dbc52f7, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/input_proj.1/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __myl_MulMinMaxRouCasResTra_myl84_6, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: __mye157895_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x8592f20b4eb6c9ee9a9e56f44ec5871e, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __mye157305_myl84_7, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __mye157307_myl84_8, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_myl84_9, LayerType: kgen, Inputs: [ { Name: __mye157895_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __mye155484_dconst, Dimensions: [1,1,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __mye153891_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_0x14d97ab92d57b85a1bd3815e99f6e152, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Concat_3][ONNX Layer: /model/decoder/Reshape_2]
    [ONNX Layer: /model/decoder/Transpose_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear]
    Name: __mye157309_myl84_10, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157311_myl84_11, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_1/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_0/cross_attn/value_proj/MatMul_myl84_12, LayerType: gemm, Inputs: [ { Name: __mye153891_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156133dconst, Dimensions: [3,256,256], Format/Datatype: Int8 }, { Name: __mye153915_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye153936_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye154934_dconst, Dimensions: [3,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153891, Dimensions: [3,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add]
    Name: /model/decoder/enc_output/proj/MatMul_myl84_13, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156138dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153194_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153201zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_proj_bias _ ONNXTRT_Broadcast_275_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/proj/MatMul][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/Add]
    Name: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_14, LayerType: kgen, Inputs: [ { Name: model_decoder_enc_output_norm_weight _ ONNXTRT_Broadcast_279_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_norm_bias _ ONNXTRT_Broadcast_281_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: __mye157905_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0xf1c80ff651c1b506b1815818d6281ad3, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/norm/LayerNormalization][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear]
    Name: __mye157313_myl84_15, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157315_myl84_16, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_score_head/MatMul_myl84_17, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156148dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153232_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153239zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_enc_score_head_bias _ ONNXTRT_Broadcast_289_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/enc_score_head/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/Add]
    Name: __myl_Max_myl84_18, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400,1], Format/Datatype: Float }], TacticName: __myl_Max_0x4330a02939b906fc5f8c1bd769456467, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/ReduceMax]
    Name: __myl_Top_myl84_19, LayerType: kgen, Inputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/TopK_output_0'.1, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x7e62297dffa2e596ee60049838a70f81, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/TopK]
    Name: __mye157317_myl84_20, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_bbox_head/layers_0/MatMul_myl84_21, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157393_xformed___mye156143dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157401_xformed___mye153216_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153212zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157397_xformed___mye153225_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.0/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/Add]
    Name: /model/decoder/enc_bbox_head/layers_1/MatMul_myl84_22, LayerType: gemm, Inputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157405_xformed___mye156153dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157413_xformed___mye153254_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153250zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157409_xformed___mye153263_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.1/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act_1/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/Add]
    Name: __myl_FcAdd_myl84_23, LayerType: fusion, Inputs: [ { Name: model_decoder_anchors_constantFloat, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156163dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153270_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153277zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_enc_bbox_head_layers_2_bias _ ONNXTRT_Broadcast_311_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.2/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/Add][ONNX Layer: /model/decoder/Add]
    Name: __mye157319_myl84_24, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_myl84_25, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __mye157909_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], Outputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,4], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], TacticName: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_0xea994e8a02766a6b87cc77a0ab1bb663, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/Unsqueeze][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Sigmoid][ONNX Layer: /model/decoder/GatherElements]
    Name: __myl_MovCon_myl84_26, LayerType: kgen, Inputs: [ { Name: __mye156470, Dimensions: [1,300,12], Format/Datatype: Int8 }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,4], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MovCon_0x9482c2d60923b5d68d1030431d0b6d2e, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_0/MatMul_myl84_27, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156484_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153292_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153288zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153301_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1/MatMul_myl84_28, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156173dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153308_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153315zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye149975_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/Add]
    Name: __myl_RepGatResAdd_myl84_29, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], Outputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_RepGatResAdd_0x3585782c9d9cf8f0d2b18744e46affde, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/GatherElements_1][ONNX Layer: /model/decoder/decoder/layers.0/Add]
    Name: __mye157321_myl84_30, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157323_myl84_31, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_2_myl84_32, LayerType: gemm, Inputs: [ { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156158dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149287/model/decoder/decoder/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149288/model/decoder/decoder/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye153089_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_2]
    Name: __mye157325_myl84_33, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_1+/model/decoder/decoder/layers_0/self_attn/MatMul_myl84_34, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156178dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149341/model/decoder/decoder/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149342/model/decoder/decoder/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155184_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153876, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add]
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_3_myl84_35, LayerType: gemm, Inputs: [ { Name: __mye153876, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153876, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153149, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149376/model/decoder/decoder/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_36, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Softmax]
    Name: __mye157327_myl84_37, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_myl84_38, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149386/model/decoder/decoder/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149387/model/decoder/decoder/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_39, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_0/self_attn/Gemm_myl84_40, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye149991_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149400/model/decoder/decoder/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149401/model/decoder/decoder/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_351_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_41, LayerType: kgen, Inputs: [ { Name: __mye152985_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152975_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157913_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x91b2c7046943674462a660380f1917c4, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/Add_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157329_myl84_42, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157331_myl84_43, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/cross_attn/attention_weights/MatMul_myl84_44, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156183dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153319_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153326zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_384_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add]
    Name: __mye157333_myl84_45, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __mye157335_myl84_46, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_Res_myl84_47, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_Res_0xda8bf33a974d44399d3e333167403bb6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_2]
    Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/MatMul_myl84_48, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156188dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153330_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153337zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_375_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add]
    Name: __myl_Res_myl84_49, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }], TacticName: __myl_Res_0x88f413fc7012b12a1acd61bb86a9989a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_1]
    Name: __myl_MaxSubExpSum_myl84_50, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_46, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_45, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax]
    Name: __mye157337_myl84_51, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl84_52, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18222, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18237, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18252, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18267, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18446, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18461, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18476, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18491, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18670, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18685, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18700, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18715, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150465_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150475_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150485_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150579, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150575, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye149996_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_49, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_48, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_47, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0xf96d9f98493dda1d394fe8f1b3a4a64a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8]
    Name: __mye157339_myl84_53, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_54, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_46, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157927_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_48, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_47, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_49, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_45, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150954_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_55, LayerType: kgen, Inputs: [ { Name: __mye150954_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_51, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_56, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_51, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155354_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153341_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153348zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_579_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_52, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_57, LayerType: kgen, Inputs: [ { Name: __mye152940_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152930_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157931_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_52, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_54, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_0/linear1/MatMul_myl84_58, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157417_xformed___mye156193dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153363_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153359zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153372_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_55, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl84_59, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_54, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_55, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156198dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153379_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153386zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_linear2_bias _ ONNXTRT_Broadcast_601_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_56, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_60, LayerType: kgen, Inputs: [ { Name: __mye152904_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152890_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157935_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_56, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157341_myl84_61, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157343_myl84_62, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_2_myl84_63, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156208dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149485/model/decoder/decoder/layers_1/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149486/model/decoder/decoder/layers_1/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152875_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_2]
    Name: __mye157345_myl84_64, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/MatMul_myl84_65, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157421_xformed___mye156203dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153401_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153397zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153410_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_60, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_1/MatMul_myl84_66, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_60, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157425_xformed___mye156213dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153428_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153424zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153437_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_61, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/MatMul_myl84_67, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_61, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156218dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153444_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153451zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_0_layers_2_bias _ ONNXTRT_Broadcast_631_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add]
    Name: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_myl84_68, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_62, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156491, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye157909_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_64, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_63, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_0xefac8e563c6580f9cd110df4750663ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log][ONNX Layer: /model/decoder/decoder/Sigmoid_1][ONNX Layer: /model/decoder/decoder/Add][ONNX Layer: /model/decoder/decoder/Div][ONNX Layer: /model/decoder/decoder/Sub][ONNX Layer: /model/decoder/decoder/Clip]
    Name: /model/decoder/decoder/query_pos_head/layers_0_1/MatMul_myl84_69, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_63, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156507_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153466_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153462zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153475_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_65, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_1/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_1/MatMul_myl84_70, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_65, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156228dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153482_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153489zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150074_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add]
    Name: __myl_Add_myl84_71, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_1+/model/decoder/decoder/layers_1/self_attn/MatMul_myl84_72, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156233dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149550/model/decoder/decoder/layers_1/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149551/model/decoder/decoder/layers_1/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155194_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153853, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_3_myl84_73, LayerType: gemm, Inputs: [ { Name: __mye153853, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153853, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153153, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149585/model/decoder/decoder/layers_1/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_69, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_74, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_69, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_69, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_70, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Softmax]
    Name: __mye157347_myl84_75, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_myl84_76, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_70, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149595/model/decoder/decoder/layers_1/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149596/model/decoder/decoder/layers_1/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_71, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_77, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_71, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_1/self_attn/Gemm_myl84_78, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150090_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149609/model/decoder/decoder/layers_1/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149610/model/decoder/decoder/layers_1/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_self_attn_out_proj_bias _ ONNXTRT_Broadcast_676_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_79, LayerType: kgen, Inputs: [ { Name: __mye152825_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152815_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157942_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_75, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157349_myl84_80, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157351_myl84_81, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_1/cross_attn/attention_weights/MatMul_myl84_82, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156238dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153493_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153500zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_707_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add]
    Name: __mye157353_myl84_83, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __mye157355_myl84_84, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_Res_myl84_85, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_Res_0xda8bf33a974d44399d3e333167403bb6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_2]
    Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/MatMul_myl84_86, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156243dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153504_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153511zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_698_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add]
    Name: __myl_Res_myl84_87, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }], TacticName: __myl_Res_0x88f413fc7012b12a1acd61bb86a9989a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_1]
    Name: __myl_MaxSubExpSum_myl84_88, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_81, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_80, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax]
    Name: __mye157357_myl84_89, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl84_90, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_64, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18921, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18936, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18951, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18966, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19145, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19160, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19175, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19190, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19369, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19384, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19399, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19414, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150495_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150505_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150515_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150607, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150603, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye150095_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_84, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_83, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_82, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0x11b440b78a8fa74f33eb0bb614ceee80, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8]
    Name: __mye157359_myl84_91, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_92, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_81, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157955_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_83, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_82, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_84, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_80, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150960_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_93, LayerType: kgen, Inputs: [ { Name: __mye150960_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_86, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_94, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_75, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_86, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155300_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153515_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153522zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_904_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_87, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_95, LayerType: kgen, Inputs: [ { Name: __mye152780_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152770_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157959_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_87, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_89, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_1/linear1/MatMul_myl84_96, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157429_xformed___mye156248dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153537_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153533zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153546_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_90, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.1/linear1/Add]
    Name: __myl_FcAdd_myl84_97, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_89, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_90, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156253dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153553_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153560zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_linear2_bias _ ONNXTRT_Broadcast_926_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_91, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_98, LayerType: kgen, Inputs: [ { Name: __mye152744_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152730_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157963_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_91, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157361_myl84_99, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157363_myl84_100, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_2_myl84_101, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156263dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149694/model/decoder/decoder/layers_2/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149695/model/decoder/decoder/layers_2/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152715_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_2]
    Name: __mye157365_myl84_102, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/MatMul_myl84_103, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157433_xformed___mye156258dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153575_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153571zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153584_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_95, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_1/MatMul_myl84_104, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_95, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157437_xformed___mye156268dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153602_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153598zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153611_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_96, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/MatMul_myl84_105, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_96, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156273dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153618_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153625zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_1_layers_2_bias _ ONNXTRT_Broadcast_956_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_97, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add]
    Name: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_myl84_106, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_64, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_97, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156514, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye157909_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_99, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_98, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_0xa06819df43d11e9f71ec4d6314dfc9b2, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log_1][ONNX Layer: /model/decoder/decoder/Add_1][ONNX Layer: /model/decoder/decoder/Sigmoid_2][ONNX Layer: /model/decoder/decoder/Div_1][ONNX Layer: /model/decoder/decoder/Sub_1][ONNX Layer: /model/decoder/decoder/Clip_3]
    Name: /model/decoder/decoder/query_pos_head/layers_0_2/MatMul_myl84_107, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_98, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156530_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153640_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153636zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153649_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_100, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_2/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_2/MatMul_myl84_108, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_100, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156283dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153656_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153663zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150169_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add]
    Name: __myl_Add_myl84_109, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_1+/model/decoder/decoder/layers_2/self_attn/MatMul_myl84_110, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156288dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149759/model/decoder/decoder/layers_2/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149760/model/decoder/decoder/layers_2/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155204_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153830, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_3_myl84_111, LayerType: gemm, Inputs: [ { Name: __mye153830, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153830, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153157, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149794/model/decoder/decoder/layers_2/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_104, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_112, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_104, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_104, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_105, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Softmax]
    Name: __mye157367_myl84_113, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_myl84_114, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_105, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149804/model/decoder/decoder/layers_2/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149805/model/decoder/decoder/layers_2/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_106, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_115, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_106, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_2/self_attn/Gemm_myl84_116, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150185_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149818/model/decoder/decoder/layers_2/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149819/model/decoder/decoder/layers_2/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_self_attn_out_proj_bias _ ONNXTRT_Broadcast_1001_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_117, LayerType: kgen, Inputs: [ { Name: __mye152665_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152655_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157970_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_110, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __mye157369_myl84_118, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157371_myl84_119, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/cross_attn/attention_weights/MatMul_myl84_120, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156293dconst, Dimensions: [1,256,96], Format/Datatype: Int8 }, { Name: __mye153667_dconst, Dimensions: [1,96], Format/Datatype: Float }, { Name: __mye153674zero_beta, Dimensions: [1,96], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_attention_weights_bias _ ONNXTRT_Broadcast_1032_constantFloat, Dimensions: [1,1,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add]
    Name: __mye157373_myl84_121, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __mye157375_myl84_122, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_Res_myl84_123, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_Res_0xda8bf33a974d44399d3e333167403bb6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_2]
    Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/MatMul_myl84_124, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156298dconst, Dimensions: [1,256,192], Format/Datatype: Int8 }, { Name: __mye153678_dconst, Dimensions: [1,192], Format/Datatype: Float }, { Name: __mye153685zero_beta, Dimensions: [1,192], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_sampling_offsets_bias _ ONNXTRT_Broadcast_1023_constantFloat, Dimensions: [1,1,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add]
    Name: __myl_Res_myl84_125, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }], TacticName: __myl_Res_0x88f413fc7012b12a1acd61bb86a9989a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_1]
    Name: __myl_MaxSubExpSum_myl84_126, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_116, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_115, Dimensions: [1,300,8,12], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSum_0x347c06f19d5104086c13b59c8ee7e1d6, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax]
    Name: __mye157377_myl84_127, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_myl84_128, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_99, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye153891, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19620, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19635, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19650, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19665, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19844, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19859, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19874, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19889, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20068, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20083, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20098, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20113, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150525_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150535_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150545_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye157917_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150635, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150631, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0, Dimensions: [1,300,8,12,2], Format/Datatype: Float }, { Name: __mye150190_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_119, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_118, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_117, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubSubEtc_0x11b440b78a8fa74f33eb0bb614ceee80, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8]
    Name: __mye157379_myl84_129, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_130, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_116, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye157983_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_118, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_117, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_119, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_115, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150966_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_131, LayerType: kgen, Inputs: [ { Name: __mye150966_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_121, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_132, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_110, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_121, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155246_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153689_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153696zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_1229_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_122, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_133, LayerType: kgen, Inputs: [ { Name: __mye152620_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152610_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157987_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_122, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_124, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_2/linear1/MatMul_myl84_134, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157441_xformed___mye156303dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153711_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153707zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153720_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_125, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.2/linear1/Add]
    Name: __myl_FcAdd_myl84_135, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_124, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_125, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156308dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153727_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153734zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_linear2_bias _ ONNXTRT_Broadcast_1251_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_126, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_136, LayerType: kgen, Inputs: [ { Name: __mye152584_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157991_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152578_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_126, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x3f53c92c8e85fb99f9934c06da28da1c, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157381_myl84_137, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157383_myl84_138, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_score_head_2/MatMul_myl84_139, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156313dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153738_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153745zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_dec_score_head_2_bias _ ONNXTRT_Broadcast_1299_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,300,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/dec_score_head.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/Add]
    Name: __myl_GatResNegExpAddDivRes_myl84_140, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,1,300,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_129, Dimensions: [1,24000], Format/Datatype: Float }], TacticName: __myl_GatResNegExpAddDivRes_0x1d563258c32f843400fb4233ccab3fa6, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/Sigmoid][ONNX Layer: /postprocessor/Flatten][ONNX Layer: /model/decoder/Gather_8]
    Name: __myl_Top_myl84_141, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_129, Dimensions: [1,24000], Format/Datatype: Float }], Outputs: [ { Name: scores, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_131, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x1c85ccd1fad109f046189f0d3e8dff44, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/TopK]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/MatMul_myl84_142, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157445_xformed___mye156318dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153760_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153756zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153769_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_132, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_1/MatMul_myl84_143, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_132, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157449_xformed___mye156323dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153787_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153783zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153796_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_133, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/MatMul_myl84_144, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_133, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156328dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153803_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153810zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_2_layers_2_bias _ ONNXTRT_Broadcast_1281_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_134, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add]
    Name: __mye157385_myl84_145, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157387_myl84_146, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_myl84_147, LayerType: kgen, Inputs: [ { Name: orig_target_sizes, Dimensions: [1,2], Format/Datatype: Int64 }, { Name: __myln_k_arg__bb1_131, Dimensions: [1,300], Format/Datatype: Int32 }, { Name: __mye150647, Dimensions: [1,1], Format/Datatype: Int64 }, { Name: __mye150651, Dimensions: [1,1], Format/Datatype: Float }, { Name: __mye150655, Dimensions: [1,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_134, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_99, Dimensions: [1,300,4], Format/Datatype: Float }], Outputs: [ { Name: labels, Dimensions: [1,300], Format/Datatype: Int64 }, { Name: boxes, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_0x046287ea34a14bdbbd780dcf069cdb4a, StreamId: 1, Metadata: [ONNX Layer: Cast_3039][ONNX Layer: /model/decoder/decoder/Clip_6][ONNX Layer: /model/decoder/decoder/Sub_2][ONNX Layer: /model/decoder/decoder/Div_2][ONNX Layer: /model/decoder/decoder/Sigmoid_3][ONNX Layer: /model/decoder/Gather_9][ONNX Layer: /model/decoder/decoder/Unsqueeze_3][ONNX Layer: /model/decoder/decoder/Add_2][ONNX Layer: /model/decoder/decoder/Log_2][ONNX Layer: /postprocessor/Split][ONNX Layer: /postprocessor/Squeeze_1][ONNX Layer: /postprocessor/Squeeze_2][ONNX Layer: /postprocessor/Mul][ONNX Layer: /postprocessor/Add][ONNX Layer: /postprocessor/Unsqueeze_2][ONNX Layer: /postprocessor/Sub][ONNX Layer: /postprocessor/Unsqueeze][ONNX Layer: /postprocessor/Concat][ONNX Layer: /postprocessor/Mul_2][ONNX Layer: /postprocessor/GatherElements][ONNX Layer: /postprocessor/Unsqueeze_5][ONNX Layer: /postprocessor/Unsqueeze_3][ONNX Layer: /postprocessor/Add_1][ONNX Layer: /postprocessor/Squeeze][ONNX Layer: /postprocessor/Unsqueeze_1][ONNX Layer: /postprocessor/Sub_1][ONNX Layer: /postprocessor/Mul_1][ONNX Layer: /postprocessor/Squeeze_3][ONNX Layer: /postprocessor/Mul_3][ONNX Layer: /postprocessor/Sub_2][ONNX Layer: /postprocessor/Div][ONNX Layer: /postprocessor/Unsqueeze_4][ONNX Layer: /postprocessor/Tile]
    Name: exit^bb^signal^1_myl84_148, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: exit^bb^wait^1_myl84_149, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    
    Bindings:
    images
    orig_target_sizes
    /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0
    /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0
    /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0
    /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0
    /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0
    /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0
    /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
    /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
    /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
    /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
    /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
    /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
    labels
    boxes
    scores[0m
[38;5;13m[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 83 MiB[0m
[38;5;104m[X] Adding 1 engine(s) to plan file.[0m
[38;5;104m[X] Adding 1 engine weights(s) to plan file.[0m
[38;5;10m[I] Finished engine building in 24.997 seconds[0m
[38;5;13m[V] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Clip_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CoordConvAC version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResizeDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResize version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DetectionLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::FlattenConcat_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GenerateDetection_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchor_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 3[0m
[38;5;104m[X] Plugin creator already registered - ::LReLU_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Normalize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PillarScatterPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PriorBox_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Proposal version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Region_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ResizeNearest_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::RPROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 2[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterND version 1[0m
[38;5;104m[X] Plugin creator already registered - ::SpecialSlice_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Split version 1[0m
[38;5;104m[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1[0m
[38;5;13m[V] Loaded engine size: 28 MiB[0m
[38;5;104m[X] Deserialization required 6622 microseconds.[0m
[38;5;104m[X] Adding 1 engine(s) to plan file.[0m
[38;5;104m[X] Adding 1 engine weights(s) to plan file.[0m
[I] Saving engine to default_mtq_int8_q_qint8break_fusion-output_modified.engine
[38;5;13m[V] [MS] Running engine with multi stream info[0m
[38;5;13m[V] [MS] Number of aux streams is 1[0m
[38;5;13m[V] [MS] Number of total worker streams is 2[0m
[38;5;13m[V] [MS] The main stream provided by execute/enqueue calls is the first worker stream[0m
[38;5;104m[X] Total per-runner device persistent memory is 0[0m
[38;5;104m[X] Total per-runner host persistent memory is 307312[0m
[38;5;104m[X] Allocated device scratch memory of size 63129600[0m
[38;5;104m[X] - Runner scratch: 63129600 bytes[0m
[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +61, now: CPU 0, GPU 83 (MiB)[0m
[38;5;104m[X] CUDA lazy loading is enabled.[0m
[38;5;13m[V] Loading inputs from data loader[0m
[38;5;11m[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. [0m
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] trt-runner-N2-05/19/25-15:43:09    
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[38;5;104m[X] trt-runner-N2-05/19/25-15:43:09     | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}[0m
[38;5;13m[V] trt-runner-N2-05/19/25-15:43:09     | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 to: tensor([101, 113, 101,  ..., 205,   8, 189], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 to: tensor([101, 113, 101,  ..., 233,   6,  64], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 to: tensor([101, 113, 101,  ..., 119, 140,  62], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 to: tensor([101, 113, 101,  ..., 170, 179,  62], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 to: tensor([101, 113, 101,  ..., 123, 142,  63], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 to: tensor([101, 113, 101,  ..., 148,  17, 191], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: labels to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: boxes to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: scores to: tensor([101, 113, 101,  ...,   0,   0,   0], device='cuda:0',
           dtype=torch.uint8)[0m
[I] trt-runner-N2-05/19/25-15:43:09    
    ---- Inference Output(s) ----
    {/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[38;5;104m[X] trt-runner-N2-05/19/25-15:43:09     | Inference Time: 10.489 ms | Received outputs:
        {'/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0': tensor([[[[[ 0.1683,  0.7307],
                   [ 1.4870,  1.9043],
                   [ 0.6844, -0.8817],
                   ...,
                   [ 1.3561,  1.0877],
                   [ 0.9923, -0.4428],
                   [ 2.5361, -0.1088]],
        
                  [[ 2.5242,  1.2648],
                   [ 4.4706, -0.2000],
                   [ 3.3982,  2.0307],
                   ...,
                   [ 1.6612,  3.1648],
                   [ 2.6326,  3.3186],
                   [ 4.2837,  4.3857]],
        
                  [[-0.4385, -0.9614],
                   [ 1.1684,  3.5923],
                   [-1.6433,  4.0119],
                   ...,
                   [-2.1711,  2.4665],
                   [-1.0352,  3.7198],
                   [-2.3037,  2.8799]],
        
                  ...,
        
                  [[-2.1124,  1.4973],
                   [-1.8591, -1.0219],
                   [-1.3538, -2.1622],
                   ...,
                   [-0.5636,  0.9121],
                   [-1.7861, -1.9972],
                   [-0.1911,  0.8260]],
        
                  [[ 0.0481, -2.7189],
                   [-0.4698, -3.5619],
                   [ 0.4552, -3.8701],
                   ...,
                   [-2.5534, -1.7286],
                   [-0.9199, -2.3552],
                   [-1.3323, -1.6664]],
        
                  [[ 1.8577, -0.2875],
                   [ 2.4907, -1.8066],
                   [ 0.9522, -1.9985],
                   ...,
                   [-0.7795, -0.6526],
                   [ 0.4521, -1.4069],
                   [ 2.7385, -5.7918]]],
        
        
                 [[[-0.1652,  0.5795],
                   [ 1.3560,  1.6781],
                   [ 1.4351, -0.4299],
                   ...,
                   [ 0.5377,  0.6620],
                   [ 0.2782,  0.3264],
                   [ 1.9366,  0.6734]],
        
                  [[ 1.3949,  2.0904],
                   [ 4.1423, -0.2215],
                   [ 3.7961,  2.7177],
                   ...,
                   [ 2.9082,  3.5525],
                   [ 2.6163,  2.2934],
                   [ 4.3574,  4.9423]],
        
                  [[ 0.3256, -0.4878],
                   [ 1.9062,  3.9623],
                   [-1.5990,  4.3151],
                   ...,
                   [-0.3099,  2.5687],
                   [ 2.1842,  3.8472],
                   [-0.4817,  3.2072]],
        
                  ...,
        
                  [[-3.3930,  0.5683],
                   [-2.2741, -1.2439],
                   [-2.6267, -2.0435],
                   ...,
                   [-0.3152,  0.5037],
                   [-1.4700, -1.8521],
                   [-1.4812,  1.0529]],
        
                  [[ 0.1953, -3.3105],
                   [-2.2206, -3.6912],
                   [ 1.5572, -3.7939],
                   ...,
                   [ 1.5420, -1.8769],
                   [ 0.4927, -2.2519],
                   [ 0.6766, -1.7042]],
        
                  [[ 3.0955, -0.2605],
                   [ 2.8565, -2.2515],
                   [ 1.3806, -2.3158],
                   ...,
                   [-0.3005, -1.0409],
                   [-0.0714, -2.9530],
                   [ 6.6592, -5.4050]]],
        
        
                 [[[ 0.3200,  0.4864],
                   [ 1.2892,  1.1766],
                   [ 1.7117, -0.3547],
                   ...,
                   [ 0.4656,  0.5821],
                   [ 0.4568, -0.3202],
                   [ 2.1202,  0.0711]],
        
                  [[ 0.0169,  2.0373],
                   [ 4.1832, -0.3152],
                   [ 3.5813,  2.3257],
                   ...,
                   [ 2.9664,  2.8289],
                   [ 2.3571,  2.1656],
                   [ 4.1981,  4.0325]],
        
                  [[-0.0356, -1.3312],
                   [ 2.0237,  3.9342],
                   [-1.8022,  4.1652],
                   ...,
                   [ 0.4999,  2.4222],
                   [ 3.4107,  3.8210],
                   [-0.2438,  2.9666]],
        
                  ...,
        
                  [[-2.9004,  0.3131],
                   [-2.3219, -1.3091],
                   [-2.3707, -1.7392],
                   ...,
                   [ 0.0309,  0.2742],
                   [-1.3637, -1.7350],
                   [-1.5187,  1.4005]],
        
                  [[ 0.5893, -3.1488],
                   [-1.5107, -3.5963],
                   [ 1.8671, -3.8352],
                   ...,
                   [ 2.2957, -1.7909],
                   [ 0.9071, -2.2068],
                   [ 1.4318, -1.5812]],
        
                  [[ 2.9474, -0.3386],
                   [ 2.8507, -2.1804],
                   [ 1.4017, -2.2966],
                   ...,
                   [-2.0582, -0.6158],
                   [-0.1879, -3.3546],
                   [ 6.3501, -6.1239]]],
        
        
                 ...,
        
        
                 [[[ 1.7199,  0.1361],
                   [ 2.1608,  1.0134],
                   [ 2.2516, -1.1025],
                   ...,
                   [ 0.7519, -0.2271],
                   [ 2.2541,  1.9013],
                   [ 2.1754, -0.1595]],
        
                  [[ 0.0430,  2.2006],
                   [ 4.2510,  1.0574],
                   [ 4.1184,  3.1197],
                   ...,
                   [ 3.1561,  1.6948],
                   [ 3.2391,  0.6318],
                   [ 3.6140,  1.7780]],
        
                  [[-1.0625,  0.9438],
                   [ 3.2791,  4.0504],
                   [-0.6831,  4.1745],
                   ...,
                   [ 0.0185,  3.2854],
                   [ 1.7415,  3.8877],
                   [-1.2775,  3.5193]],
        
                  ...,
        
                  [[-3.1233, -0.3985],
                   [-3.6358, -2.2240],
                   [-3.6156, -3.1103],
                   ...,
                   [-1.5569, -1.3362],
                   [-2.0773, -2.3130],
                   [-2.1444, -0.7267]],
        
                  [[ 1.3868, -3.8111],
                   [-2.1059, -3.8813],
                   [ 3.5238, -3.9438],
                   ...,
                   [ 1.9659, -2.5973],
                   [-1.8446, -2.6914],
                   [ 0.7080, -2.9004]],
        
                  [[ 3.7366, -0.6388],
                   [ 3.7297, -2.1301],
                   [ 3.4924, -3.3495],
                   ...,
                   [-1.3910, -2.5221],
                   [ 1.6052, -1.9402],
                   [ 1.6150, -2.1676]]],
        
        
                 [[[ 1.0782,  0.2857],
                   [ 1.7967,  1.1915],
                   [ 1.2660, -0.8723],
                   ...,
                   [ 0.9568,  0.6739],
                   [ 0.1873, -1.1850],
                   [ 2.9515,  0.2339]],
        
                  [[ 3.8624,  1.6025],
                   [ 5.0242, -0.7815],
                   [ 3.7177,  1.9787],
                   ...,
                   [ 3.8138,  2.7316],
                   [ 2.4786,  2.0503],
                   [ 4.0804,  4.0140]],
        
                  [[-0.1826, -1.3878],
                   [ 1.7881,  3.6747],
                   [-1.4403,  4.0452],
                   ...,
                   [ 0.6809,  2.1398],
                   [ 3.1706,  3.7433],
                   [-0.1079,  2.7788]],
        
                  ...,
        
                  [[-2.5092,  0.2358],
                   [-2.0640, -1.3016],
                   [-1.5084, -1.8577],
                   ...,
                   [-0.1002,  0.1936],
                   [-1.6712, -1.9009],
                   [-0.9129,  1.0178]],
        
                  [[ 0.4735, -2.8876],
                   [-0.4343, -3.4962],
                   [ 1.4504, -3.8652],
                   ...,
                   [ 1.7267, -1.8549],
                   [ 1.6335, -2.2312],
                   [ 1.4773, -1.7025]],
        
                  [[ 2.7768, -0.7541],
                   [ 2.9452, -2.2413],
                   [ 2.0397, -2.3093],
                   ...,
                   [-2.0718, -0.1855],
                   [ 0.1713, -2.6059],
                   [ 7.4666, -5.5436]]],
        
        
                 [[[ 0.6289,  0.1797],
                   [ 1.5831,  1.1486],
                   [ 0.9117, -1.2758],
                   ...,
                   [ 0.8872,  0.1577],
                   [ 0.4647, -0.3390],
                   [ 1.8447,  0.5008]],
        
                  [[ 1.9694,  1.1799],
                   [ 3.7432, -0.1305],
                   [ 3.4931,  2.1225],
                   ...,
                   [ 2.8513,  3.5815],
                   [ 2.1052,  2.5564],
                   [ 3.9430,  4.8256]],
        
                  [[-0.0494,  0.2808],
                   [ 1.5149,  3.5961],
                   [-0.5841,  3.9960],
                   ...,
                   [-0.2738,  2.6413],
                   [ 1.5022,  3.5732],
                   [-0.6873,  2.0006]],
        
                  ...,
        
                  [[-2.0237, -0.8743],
                   [-2.2958, -1.8679],
                   [-2.0810, -2.6800],
                   ...,
                   [-0.7205, -0.6499],
                   [-1.0276, -2.0890],
                   [-0.6425, -0.2574]],
        
                  [[ 0.4913, -2.9060],
                   [-1.3162, -3.6689],
                   [ 1.0271, -3.8880],
                   ...,
                   [ 0.0513, -1.8747],
                   [-1.0380, -2.3256],
                   [-0.1188, -1.8810]],
        
                  [[ 2.1920, -1.1925],
                   [ 2.6411, -2.4684],
                   [ 1.7208, -2.6005],
                   ...,
                   [-3.2837,  0.2682],
                   [ 0.4753, -1.3125],
                   [ 5.5169, -2.7646]]]]]), '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0': tensor([[[[[ 0.5720,  1.4100],
                   [ 0.9941, -0.9007],
                   [ 1.7155, -0.6125],
                   ...,
                   [ 0.7306,  0.7690],
                   [ 2.6512,  0.5917],
                   [ 2.2481, -0.2037]],
        
                  [[ 2.6605,  0.8078],
                   [ 2.4402,  2.5195],
                   [ 2.4771,  1.9657],
                   ...,
                   [ 1.9490,  1.1026],
                   [ 2.5002,  1.6636],
                   [ 5.5640,  6.0785]],
        
                  [[-1.6545, -2.0801],
                   [-0.4670,  3.3619],
                   [-1.2574,  3.8609],
                   ...,
                   [ 1.8672,  1.0712],
                   [-0.6271,  3.3606],
                   [ 0.9095,  3.5443]],
        
                  ...,
        
                  [[-4.4335,  1.2195],
                   [-3.6021,  0.3648],
                   [-3.7745, -0.3649],
                   ...,
                   [-1.8700,  0.2956],
                   [-0.5058, -4.2348],
                   [-4.8433, -3.3090]],
        
                  [[ 1.7182,  3.6059],
                   [ 0.2650, -2.5256],
                   [ 0.1443, -3.0491],
                   ...,
                   [-0.3366, -1.2360],
                   [ 0.1070, -3.0476],
                   [ 0.6313, -2.6666]],
        
                  [[ 3.9115, -1.1699],
                   [ 4.2509,  1.0802],
                   [ 3.3586, -0.6181],
                   ...,
                   [ 0.3120, -1.2818],
                   [ 3.0943, -1.8422],
                   [ 2.1949, -3.0456]]],
        
        
                 [[[ 1.9280,  1.4613],
                   [ 1.7571, -1.4850],
                   [ 2.6725, -0.4360],
                   ...,
                   [ 1.0275,  1.5247],
                   [ 2.1110,  1.4404],
                   [ 1.4663, -0.1590]],
        
                  [[ 3.0781,  1.7381],
                   [ 1.5277,  2.8868],
                   [ 2.8623,  2.1426],
                   ...,
                   [ 1.5596,  1.6380],
                   [ 3.1592,  1.7750],
                   [ 7.1296,  4.7676]],
        
                  [[-0.8764, -0.4942],
                   [-1.8063,  3.5496],
                   [-2.5350,  3.8741],
                   ...,
                   [ 0.4442,  1.7182],
                   [-1.4157,  3.4209],
                   [-0.6313,  3.3985]],
        
                  ...,
        
                  [[-4.7020,  1.8321],
                   [-3.9518,  0.5150],
                   [-4.1121, -0.7594],
                   ...,
                   [-0.9721, -0.8655],
                   [ 0.4935, -3.8941],
                   [-2.2654, -3.9801]],
        
                  [[ 0.9370,  2.4313],
                   [-0.7916, -2.8681],
                   [-1.7722, -3.2974],
                   ...,
                   [-0.5013, -1.4621],
                   [-0.4296, -2.9757],
                   [ 0.0525, -1.8480]],
        
                  [[ 3.3894, -1.3137],
                   [ 4.0579,  1.0385],
                   [ 3.6618, -0.1419],
                   ...,
                   [-1.5324, -2.6594],
                   [ 3.0312, -1.4024],
                   [ 2.0449, -3.4692]]],
        
        
                 [[[ 2.5158,  1.0000],
                   [ 2.2122, -0.4411],
                   [ 2.9867,  0.6076],
                   ...,
                   [ 1.7339,  1.4740],
                   [ 2.3040,  1.2409],
                   [ 2.2589,  0.9534]],
        
                  [[ 3.2937,  1.8931],
                   [ 2.3760,  2.2820],
                   [ 3.1543,  2.0334],
                   ...,
                   [ 1.4430,  1.5978],
                   [ 3.0662,  1.6581],
                   [ 7.0063,  6.6770]],
        
                  [[-1.9260, -1.9803],
                   [-0.5655,  3.3613],
                   [-1.9369,  4.1522],
                   ...,
                   [ 1.0058,  1.6892],
                   [-0.5056,  3.4822],
                   [ 0.3058,  3.0066]],
        
                  ...,
        
                  [[-4.1432,  1.0755],
                   [-3.7208,  0.8618],
                   [-3.8333,  0.0788],
                   ...,
                   [-1.2757,  0.2563],
                   [-0.5697, -3.7526],
                   [-2.4254, -3.2423]],
        
                  [[ 1.9787,  4.1240],
                   [-0.1062, -2.0756],
                   [-1.2148, -2.9616],
                   ...,
                   [ 0.4017, -1.2526],
                   [ 0.6247, -3.0245],
                   [ 0.2739, -1.7720]],
        
                  [[ 3.5517, -0.6347],
                   [ 4.0170,  1.0369],
                   [ 3.6474,  0.1523],
                   ...,
                   [-0.3253, -1.1888],
                   [ 3.7176, -0.8249],
                   [ 3.3986, -3.1441]]],
        
        
                 ...,
        
        
                 [[[ 3.1171,  2.3062],
                   [ 2.7631, -2.6021],
                   [ 3.4817, -1.1160],
                   ...,
                   [ 2.0466,  0.3165],
                   [ 2.4948,  1.4978],
                   [ 1.2585, -1.0761]],
        
                  [[ 3.6590,  2.3780],
                   [ 3.5974,  3.0880],
                   [ 3.9919,  3.1003],
                   ...,
                   [ 2.4757,  2.0154],
                   [ 2.7028,  2.0011],
                   [ 4.3097,  0.4014]],
        
                  [[ 1.1843,  1.2343],
                   [-0.6217,  3.9485],
                   [-2.8817,  3.9323],
                   ...,
                   [-0.3273,  3.0071],
                   [ 1.1142,  3.4427],
                   [ 0.2115,  3.6928]],
        
                  ...,
        
                  [[-4.1822,  1.1507],
                   [-3.9612, -0.1165],
                   [-3.9663, -2.2198],
                   ...,
                   [-2.2336, -1.0226],
                   [-2.8392, -2.6185],
                   [-3.0406, -1.3328]],
        
                  [[-0.5877, -1.4341],
                   [-0.4185, -3.7064],
                   [-2.6639, -3.7203],
                   ...,
                   [ 1.0409, -2.1501],
                   [-0.8594, -2.4181],
                   [-1.6837, -2.1270]],
        
                  [[ 4.1895, -1.9499],
                   [ 3.8872,  1.0158],
                   [ 3.8970, -0.5847],
                   ...,
                   [-1.9715, -2.1943],
                   [ 2.4812, -1.7760],
                   [ 1.2576, -1.7643]]],
        
        
                 [[[ 1.0462,  1.2801],
                   [ 1.5841, -0.8387],
                   [ 1.7230, -0.3757],
                   ...,
                   [ 1.5503,  0.2641],
                   [ 2.5836,  0.2952],
                   [ 2.6152,  0.3052]],
        
                  [[ 2.9641,  2.1202],
                   [ 3.5122,  2.0951],
                   [ 2.9106,  2.0338],
                   ...,
                   [ 1.1661,  2.3420],
                   [ 3.9464,  1.9306],
                   [ 7.9581,  6.3314]],
        
                  [[-1.2152, -0.6418],
                   [ 0.8499,  3.1742],
                   [-0.1409,  3.6031],
                   ...,
                   [ 2.6725,  0.8342],
                   [ 0.3129,  3.0222],
                   [ 1.5937,  3.1868]],
        
                  ...,
        
                  [[-4.6563, -0.0544],
                   [-3.7239, -0.7371],
                   [-3.8810, -0.7073],
                   ...,
                   [-0.8860,  0.8677],
                   [-1.3635, -5.2107],
                   [-2.9121, -3.2282]],
        
                  [[ 0.2637,  3.7753],
                   [ 0.6497, -2.4386],
                   [ 0.2213, -3.0363],
                   ...,
                   [-0.1769, -1.5495],
                   [ 1.4964, -2.8188],
                   [ 1.9152, -2.8819]],
        
                  [[ 4.8050, -1.3576],
                   [ 4.4325,  0.3168],
                   [ 3.3521, -0.9813],
                   ...,
                   [ 0.7839, -0.7213],
                   [ 4.7126, -1.7048],
                   [ 3.9225, -2.2835]]],
        
        
                 [[[ 1.8525,  2.0907],
                   [ 0.7756, -1.7403],
                   [ 2.0302, -0.4741],
                   ...,
                   [ 0.8012,  0.6701],
                   [ 2.4306,  1.4116],
                   [ 1.0183,  0.2189]],
        
                  [[ 2.8243,  3.3127],
                   [ 3.6257,  2.9391],
                   [ 2.8641,  2.5209],
                   ...,
                   [ 0.9905,  2.2571],
                   [ 2.6680,  0.7607],
                   [ 7.2671,  1.8105]],
        
                  [[ 0.1955,  0.6618],
                   [ 1.6045,  3.2635],
                   [-0.1936,  3.2824],
                   ...,
                   [ 1.4332,  0.9541],
                   [ 1.8193,  2.9113],
                   [ 1.8402,  3.2090]],
        
                  ...,
        
                  [[-3.4936, -1.1088],
                   [-3.3013, -2.6614],
                   [-3.7349, -2.8044],
                   ...,
                   [-1.4139, -1.3434],
                   [-0.2366, -3.6627],
                   [-3.9093, -4.0145]],
        
                  [[ 0.2690,  0.1769],
                   [-0.9295, -3.3222],
                   [-1.3553, -3.2462],
                   ...,
                   [-1.2510, -1.5781],
                   [-0.4745, -2.3901],
                   [ 0.9872, -2.6945]],
        
                  [[ 2.7268, -1.5494],
                   [ 3.6332,  1.5435],
                   [ 3.3403, -0.1409],
                   ...,
                   [ 0.4615, -1.4339],
                   [ 2.7218, -1.3169],
                   [ 1.6684, -1.7301]]]]]), '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0': tensor([[[[[-2.4460e-01,  5.9259e-02],
                   [-1.1141e-03,  1.4989e+00],
                   [ 9.3589e-01,  1.3096e-01],
                   ...,
                   [ 7.3822e-01,  1.1545e+00],
                   [ 5.9355e-01,  7.9640e-01],
                   [ 1.3332e+00,  1.0065e+00]],
        
                  [[-3.3128e+00,  6.2168e-01],
                   [ 4.5780e+00, -9.5906e-02],
                   [ 4.2308e+00,  2.4372e+00],
                   ...,
                   [ 2.6396e+00,  1.2956e+00],
                   [ 2.3206e+00,  2.0185e+00],
                   [ 3.5549e+00,  3.6931e+00]],
        
                  [[-1.0385e-01, -1.4592e+00],
                   [ 4.7540e-01,  4.6421e+00],
                   [-2.8530e+00,  4.1070e+00],
                   ...,
                   [-5.9980e-01,  1.7821e+00],
                   [ 5.2513e-01,  4.1325e+00],
                   [ 1.6809e-01,  4.7062e+00]],
        
                  ...,
        
                  [[-1.8083e+00,  1.5820e+00],
                   [-1.0673e-01, -1.8363e+00],
                   [-1.5638e+00, -8.5309e-01],
                   ...,
                   [ 1.9661e-01,  8.9650e-01],
                   [ 1.5910e-01, -6.9852e-01],
                   [-7.5167e-01,  9.7835e-01]],
        
                  [[-1.8145e+00,  1.2306e+00],
                   [ 4.9855e-01, -3.5011e+00],
                   [-8.7943e-03, -3.7281e+00],
                   ...,
                   [-1.2735e+00, -8.7183e-01],
                   [ 1.8393e+00, -1.4032e+00],
                   [ 1.3361e+00, -3.7547e+00]],
        
                  [[-9.9999e-01, -3.2500e+00],
                   [ 1.7889e+00, -1.9548e+00],
                   [ 2.2291e+00,  7.2180e-01],
                   ...,
                   [ 3.7514e+00, -1.7926e+00],
                   [ 1.4919e+00, -1.5257e+00],
                   [ 4.6535e+00, -4.5921e+00]]],
        
        
                 [[[-1.0363e+00, -3.7030e-01],
                   [-1.4653e-01,  8.5278e-01],
                   [ 8.9710e-01, -3.4601e-01],
                   ...,
                   [ 1.8813e-01,  6.3310e-01],
                   [ 1.7709e-01,  6.5968e-01],
                   [ 3.5493e-01,  7.3086e-01]],
        
                  [[-1.2033e+00,  6.2885e-01],
                   [ 3.9480e+00, -1.4144e-01],
                   [ 3.9492e+00,  2.1286e+00],
                   ...,
                   [ 9.2297e-01, -2.2781e-01],
                   [ 2.5553e+00,  2.0290e+00],
                   [ 3.3269e+00,  3.2560e+00]],
        
                  [[-1.0450e+00,  5.8974e-01],
                   [-2.6675e-01,  4.2411e+00],
                   [-3.0551e+00,  3.9688e+00],
                   ...,
                   [-5.6716e-01,  5.0445e-01],
                   [ 2.9641e-01,  3.8595e+00],
                   [-3.0443e-01,  4.2137e+00]],
        
                  ...,
        
                  [[-2.7425e+00,  1.4865e+00],
                   [-4.9614e-01, -2.5315e+00],
                   [-2.4631e+00, -1.8376e+00],
                   ...,
                   [ 1.6212e-01, -5.8282e-02],
                   [-1.5044e+00, -9.6412e-01],
                   [-1.5799e+00,  8.4775e-01]],
        
                  [[-6.7745e-01,  9.9032e-01],
                   [-5.9255e-01, -3.6068e+00],
                   [-1.6215e+00, -3.7916e+00],
                   ...,
                   [-1.7266e+00, -1.4176e+00],
                   [ 1.6738e+00, -1.7414e+00],
                   [-2.7741e-01, -3.4234e+00]],
        
                  [[ 5.2612e-01, -3.1993e+00],
                   [ 1.9840e+00, -2.0992e+00],
                   [ 2.9177e+00,  1.4631e+00],
                   ...,
                   [ 2.5318e+00, -2.2789e+00],
                   [ 8.0621e-01, -3.2033e+00],
                   [ 2.5486e+00, -4.1279e+00]]],
        
        
                 [[[-6.9667e-01,  2.2638e-01],
                   [ 5.4468e-01,  1.6002e+00],
                   [ 1.1719e+00,  1.0396e+00],
                   ...,
                   [ 5.2341e-01,  1.0272e+00],
                   [ 6.2335e-01,  1.1889e+00],
                   [ 1.2644e+00,  1.3990e+00]],
        
                  [[-5.3309e-01,  5.7249e-01],
                   [ 3.9635e+00,  1.2741e+00],
                   [ 3.9652e+00,  1.9558e+00],
                   ...,
                   [ 3.0242e+00, -3.0990e-01],
                   [ 2.9645e+00,  3.1089e+00],
                   [ 3.5085e+00,  3.0693e+00]],
        
                  [[-1.5224e-01, -2.5650e+00],
                   [ 1.0114e+00,  4.9784e+00],
                   [-2.3940e+00,  4.1959e+00],
                   ...,
                   [ 1.9131e-01,  1.5759e+00],
                   [ 8.0660e-01,  3.7853e+00],
                   [-5.1231e-02,  4.9137e+00]],
        
                  ...,
        
                  [[-3.0730e+00,  1.4759e+00],
                   [ 1.0543e-01, -1.1583e+00],
                   [-2.5099e+00, -5.8028e-01],
                   ...,
                   [ 8.2855e-01,  1.5681e+00],
                   [-9.8188e-01, -2.1498e-01],
                   [-1.3532e+00,  1.6500e+00]],
        
                  [[-2.1950e+00,  2.1716e+00],
                   [ 5.5100e-01, -3.5577e+00],
                   [-9.5694e-01, -3.7619e+00],
                   ...,
                   [-1.3868e+00, -1.1267e+00],
                   [ 1.4647e+00, -1.3091e+00],
                   [ 1.3839e+00, -3.8337e+00]],
        
                  [[ 1.2671e+00, -2.2827e+00],
                   [ 2.3901e+00, -1.0417e+00],
                   [ 3.2880e+00,  1.1864e+00],
                   ...,
                   [ 2.8722e+00, -2.1391e+00],
                   [ 1.2379e+00, -3.1966e+00],
                   [ 3.1626e+00, -4.4627e+00]]],
        
        
                 ...,
        
        
                 [[[-5.7167e-01, -6.2497e-01],
                   [ 1.0672e+00,  5.8885e-01],
                   [ 2.0102e+00, -1.5116e+00],
                   ...,
                   [ 3.8461e-01,  2.8441e+00],
                   [ 2.0165e+00,  9.3774e-01],
                   [ 3.2381e-01, -5.6183e-01]],
        
                  [[-6.8157e-01,  7.6655e-02],
                   [ 4.0934e+00, -1.2140e-01],
                   [ 3.9803e+00,  3.1164e+00],
                   ...,
                   [ 1.9495e+00,  1.9581e+00],
                   [ 3.0550e+00,  6.7446e-01],
                   [ 2.9742e+00,  2.6829e+00]],
        
                  [[ 1.0621e+00,  5.2548e-01],
                   [ 9.3133e-01,  4.1675e+00],
                   [-2.1602e+00,  4.0463e+00],
                   ...,
                   [ 4.7505e-01,  2.1315e+00],
                   [ 2.1366e+00,  3.9839e+00],
                   [-1.2895e+00,  3.9687e+00]],
        
                  ...,
        
                  [[-3.3039e+00,  1.3057e+00],
                   [-2.4489e+00, -2.8520e+00],
                   [-3.1091e+00, -2.3010e+00],
                   ...,
                   [ 2.0225e+00, -6.7592e-01],
                   [-8.3011e-01, -1.7119e+00],
                   [-2.1564e+00, -1.3018e+00]],
        
                  [[ 6.5215e-01, -3.4082e-01],
                   [-3.3356e-01, -3.9331e+00],
                   [-1.8953e+00, -3.9138e+00],
                   ...,
                   [-2.2640e+00, -2.5536e+00],
                   [ 2.2449e+00, -2.4680e+00],
                   [-3.8316e-03, -2.9635e+00]],
        
                  [[ 1.7071e+00, -2.8927e+00],
                   [ 3.2691e+00, -3.0847e+00],
                   [ 3.7192e+00,  6.4085e-01],
                   ...,
                   [ 1.2062e+00, -1.4557e+00],
                   [ 1.7966e-01, -2.2920e+00],
                   [ 3.5343e+00, -3.3642e+00]]],
        
        
                 [[[ 6.6064e-01, -1.8948e-01],
                   [ 1.6270e+00,  4.5215e-01],
                   [ 1.0713e+00,  5.9602e-01],
                   ...,
                   [ 2.3125e+00,  5.5292e-01],
                   [ 1.4954e+00,  8.3680e-01],
                   [ 2.6472e+00,  5.4101e-01]],
        
                  [[-3.3279e+00,  4.5556e-01],
                   [ 4.6737e+00,  3.3045e-01],
                   [ 4.2664e+00,  1.8192e+00],
                   ...,
                   [ 1.6499e+00, -9.3791e-02],
                   [ 3.1973e+00,  2.0047e+00],
                   [ 4.1917e+00,  3.6893e+00]],
        
                  [[-5.6535e-01, -1.4143e+00],
                   [ 1.5038e+00,  3.8231e+00],
                   [-1.0091e+00,  3.9142e+00],
                   ...,
                   [-9.4697e-01, -5.6655e-01],
                   [ 5.8500e-01,  3.5290e+00],
                   [ 7.1297e-01,  4.2502e+00]],
        
                  ...,
        
                  [[-1.3536e+00,  1.0812e+00],
                   [-9.8232e-02, -2.0423e+00],
                   [-8.0464e-01, -1.4941e+00],
                   ...,
                   [-5.0431e-01, -2.3057e-01],
                   [-5.0593e-01, -1.6464e+00],
                   [-6.0657e-01, -4.4745e-01]],
        
                  [[-1.7834e+00,  2.3293e+00],
                   [ 5.3926e-01, -3.8696e+00],
                   [ 3.5006e-01, -3.6611e+00],
                   ...,
                   [-7.0779e-01, -1.3100e+00],
                   [ 1.9534e+00, -1.9238e+00],
                   [ 1.1940e+00, -3.8249e+00]],
        
                  [[-1.3294e+00, -2.3662e+00],
                   [ 2.3862e+00, -1.9047e+00],
                   [ 2.3069e+00,  4.6286e-01],
                   ...,
                   [ 3.9681e+00, -9.2380e-01],
                   [ 1.9495e+00, -2.1065e+00],
                   [ 3.6077e+00, -4.3836e+00]]],
        
        
                 [[[-3.9864e-02, -1.4339e+00],
                   [ 1.5432e+00,  7.6129e-01],
                   [ 8.8616e-01,  4.3961e-01],
                   ...,
                   [ 1.5180e+00,  8.9999e-01],
                   [ 1.7778e+00,  2.0555e+00],
                   [ 1.6615e+00,  1.0794e+00]],
        
                  [[-1.2760e+00,  1.1474e-01],
                   [ 4.2492e+00,  9.4698e-01],
                   [ 4.2243e+00,  3.2905e+00],
                   ...,
                   [ 1.1346e+00,  1.3847e+00],
                   [ 2.4970e+00,  2.6764e+00],
                   [ 3.6656e+00,  4.5870e+00]],
        
                  [[ 1.1585e+00,  5.6542e-01],
                   [ 2.0118e+00,  3.6277e+00],
                   [ 4.7467e-01,  3.8115e+00],
                   ...,
                   [-5.9069e-02, -4.9513e-02],
                   [ 2.0559e+00,  3.8923e+00],
                   [ 8.1827e-01,  4.4548e+00]],
        
                  ...,
        
                  [[-1.9020e+00, -8.6318e-01],
                   [-1.9278e+00, -2.8496e+00],
                   [-1.5329e+00, -3.3699e+00],
                   ...,
                   [-1.6799e-02, -7.9094e-01],
                   [-5.1981e-01, -1.5389e+00],
                   [-5.5981e-01, -1.2233e+00]],
        
                  [[ 1.5517e+00,  2.7228e-01],
                   [-1.4832e+00, -3.7813e+00],
                   [-1.5284e+00, -3.7587e+00],
                   ...,
                   [-2.1466e+00, -1.4335e+00],
                   [ 1.3413e+00, -2.0455e+00],
                   [-3.2355e-01, -3.4446e+00]],
        
                  [[-1.2452e+00, -2.5014e+00],
                   [ 1.8596e+00, -2.6286e+00],
                   [ 2.6327e+00,  3.2552e+00],
                   ...,
                   [ 2.9058e+00, -5.9040e-01],
                   [-1.7561e-01, -1.8162e+00],
                   [ 4.1330e+00, -4.0583e+00]]]]]), '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0': tensor([[[[ 4.2611,  2.5208,  3.0048,  ..., -2.5158, -3.1061, -3.2853],
                  [ 1.7285,  2.5188,  2.7288,  ..., -5.2524, -3.1134, -2.6027],
                  [-1.1289,  3.1743,  3.1532,  ..., -2.7509, -2.6160, -2.7712],
                  ...,
                  [ 2.9799,  2.8892,  3.1656,  ...,  0.2010, -3.5593,  1.4369],
                  [ 3.3761,  3.3848,  3.6363,  ...,  1.0565, -4.0489, -3.6320],
                  [ 0.8790,  1.2104,  1.3089,  ...,  1.4809, -1.4630,  0.4347]],
        
                 [[ 2.7454,  1.5528,  1.7241,  ..., -0.3583, -2.6358, -1.8451],
                  [-0.2205,  1.9000,  2.1360,  ..., -2.3653, -1.4287, -1.4088],
                  [-2.5069,  1.8000,  2.2174,  ..., -1.8802, -1.1071, -1.0244],
                  ...,
                  [ 2.2663,  1.8159,  2.4761,  ...,  0.2472, -2.9571,  1.1665],
                  [ 1.5704,  1.7918,  2.2531,  ..., -0.0253, -3.1596, -1.4707],
                  [ 0.9477,  0.7388,  0.7064,  ...,  1.1663, -1.0497,  0.5901]],
        
                 [[ 3.5937,  2.1709,  2.6875,  ..., -1.7485, -2.5969, -2.8560],
                  [ 0.5231,  2.2499,  2.4048,  ..., -2.5862, -1.8001, -1.5746],
                  [-1.4632,  2.6256,  2.5827,  ..., -2.9241, -1.5234, -2.0554],
                  ...,
                  [ 2.7665,  2.3039,  3.0443,  ...,  0.5544, -3.6196,  1.0684],
                  [ 2.6162,  2.5753,  3.2822,  ...,  0.2721, -3.9824, -2.1674],
                  [ 0.8778,  1.1710,  0.6830,  ...,  1.3793, -1.4384,  0.6412]],
        
                 ...,
        
                 [[-1.8958, -2.0568, -1.7821,  ...,  2.0477,  2.5764,  1.7905],
                  [-2.0764, -0.2122, -0.0807,  ..., -0.7195, -0.4873,  0.5252],
                  [-2.4318,  0.1430, -0.3656,  ...,  0.2913,  1.2330, -0.1548],
                  ...,
                  [-0.8009, -0.5786, -0.0370,  ...,  1.2198, -0.5582,  1.1529],
                  [-0.2937, -0.1615, -0.4612,  ...,  1.0465,  0.1904, -0.2353],
                  [-0.5993, -0.1997, -0.6117,  ...,  0.1119, -0.4938, -0.4196]],
        
                 [[ 5.3841,  2.5785,  3.3113,  ..., -3.3917, -3.2843, -3.1962],
                  [ 1.4648,  2.3913,  2.6648,  ..., -4.0717, -2.3168, -2.3280],
                  [-0.2862,  3.2660,  2.9892,  ..., -3.3558, -1.5494, -2.4029],
                  ...,
                  [ 2.5353,  2.5312,  3.6756,  ...,  0.6640, -3.3650,  1.3249],
                  [ 3.5193,  3.6053,  3.7378,  ...,  0.4948, -5.6713, -2.9037],
                  [ 0.6202,  1.2732,  1.7173,  ...,  1.9118, -2.0167,  1.0527]],
        
                 [[ 3.4310,  2.3340,  2.3309,  ..., -2.1341, -2.3568, -3.4006],
                  [ 0.8056,  1.6605,  2.8759,  ..., -3.7470, -3.0654, -2.7453],
                  [ 0.7149,  3.2023,  2.2795,  ..., -2.2134, -1.7741, -2.4685],
                  ...,
                  [ 2.0203,  2.5124,  3.2551,  ..., -0.4712, -2.9771,  0.9004],
                  [ 2.7255,  3.5702,  2.5424,  ...,  0.4749, -3.6826, -3.8639],
                  [ 1.0143,  0.8241,  0.6696,  ...,  1.0716, -1.0450,  0.3493]]]]), '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0': tensor([[[[ 2.6295e+00,  2.5487e+00,  1.8629e+00,  ..., -3.5063e+00,
                   -3.3262e+00, -2.8223e+00],
                  [ 1.2518e+00,  2.1524e+00,  2.3482e+00,  ..., -1.9922e+00,
                   -2.1357e+00,  8.8958e-01],
                  [ 6.3101e-01,  3.0760e+00,  3.0132e+00,  ..., -3.9957e+00,
                   -3.9005e+00, -1.4497e+00],
                  ...,
                  [ 2.5367e+00,  1.6969e+00,  5.7400e-01,  ..., -6.6357e-01,
                    1.0716e+00, -3.7494e+00],
                  [ 9.1198e-01,  3.6244e+00,  2.2713e+00,  ..., -3.0245e+00,
                   -4.9523e+00, -2.5281e+00],
                  [ 1.8138e+00,  1.2823e+00,  6.9074e-01,  ...,  6.4143e-01,
                   -3.1266e+00,  2.8874e-01]],
        
                 [[ 1.8983e+00,  1.0944e+00,  1.8833e+00,  ..., -2.5052e+00,
                   -2.8898e+00, -1.6863e+00],
                  [ 1.3399e+00,  1.2512e+00,  2.0406e+00,  ..., -8.1963e-01,
                   -2.0796e+00,  5.0841e-01],
                  [-9.0221e-01,  2.6029e+00,  2.7872e+00,  ..., -2.1093e+00,
                   -3.8410e+00, -1.9628e+00],
                  ...,
                  [ 1.1895e+00,  1.8921e+00,  2.1763e+00,  ..., -7.4481e-01,
                    3.8351e-01, -3.2618e+00],
                  [-5.7939e-01,  2.4326e+00,  2.0041e+00,  ..., -2.9407e+00,
                   -2.6492e+00, -1.4224e+00],
                  [ 6.3685e-01,  1.6309e+00,  1.5456e+00,  ...,  8.8160e-01,
                   -2.6064e+00,  1.6409e+00]],
        
                 [[ 2.1322e+00,  1.2395e+00,  2.4062e+00,  ..., -2.5102e+00,
                   -3.1218e+00, -2.5868e+00],
                  [ 1.3388e+00,  7.7716e-01,  2.1554e+00,  ..., -1.4011e+00,
                   -1.8452e+00,  5.1087e-01],
                  [ 9.3047e-02,  2.9809e+00,  3.1986e+00,  ..., -3.7627e+00,
                   -4.4670e+00, -1.9565e+00],
                  ...,
                  [ 8.6743e-01,  1.7960e+00,  1.8411e+00,  ..., -4.0635e-01,
                    9.4026e-01, -3.0921e+00],
                  [ 6.6665e-02,  3.2681e+00,  2.6429e+00,  ..., -3.4420e+00,
                   -4.3044e+00, -2.3566e+00],
                  [ 2.6057e-01,  1.4352e+00,  1.5869e+00,  ...,  4.1683e-01,
                   -3.4426e+00,  6.2361e-01]],
        
                 ...,
        
                 [[-1.8804e-02, -9.0679e-01,  4.6420e-02,  ...,  5.6177e-01,
                    1.4671e+00, -1.6000e-01],
                  [-2.2358e-01, -1.7347e-01,  7.8926e-02,  ...,  1.1944e+00,
                   -4.2177e-01, -4.8401e-01],
                  [-1.8214e+00, -4.4003e-05, -1.3137e-01,  ...,  8.8955e-01,
                    1.1833e+00, -1.1262e+00],
                  ...,
                  [-8.5592e-01, -3.2823e-01,  2.4042e-01,  ...,  6.8552e-01,
                    5.5699e-02,  3.7845e-02],
                  [-2.1300e+00, -4.3366e-02,  3.8147e-01,  ...,  4.0715e-01,
                   -1.5460e-01,  7.1429e-01],
                  [-1.1560e+00,  3.2071e-01,  1.3844e-01,  ...,  1.0469e+00,
                    1.7627e-01,  7.3206e-01]],
        
                 [[ 3.4297e+00,  3.5927e+00,  1.8174e+00,  ..., -4.6381e+00,
                   -4.2857e+00, -2.5760e+00],
                  [ 9.4917e-01,  2.6556e+00,  2.7177e+00,  ..., -1.8442e+00,
                   -2.3635e+00,  1.7529e+00],
                  [ 7.1535e-02,  3.2639e+00,  2.3605e+00,  ..., -3.6937e+00,
                   -3.5128e+00, -2.9711e+00],
                  ...,
                  [ 2.3318e+00,  2.0294e+00,  9.5141e-01,  ..., -1.6734e+00,
                    2.7099e-01, -3.7182e+00],
                  [-5.4966e-01,  3.9128e+00,  2.2814e+00,  ..., -3.5980e+00,
                   -4.5104e+00, -2.7098e+00],
                  [ 2.5133e+00,  1.9716e+00,  1.2558e+00,  ...,  7.4835e-01,
                   -2.9561e+00,  1.4476e+00]],
        
                 [[ 2.9607e+00,  2.2851e+00,  7.8513e-01,  ..., -3.7864e+00,
                   -2.6337e+00, -2.1483e+00],
                  [ 4.7065e-01,  1.2519e+00,  1.7711e+00,  ..., -6.1025e-01,
                   -1.6458e+00,  5.5597e-01],
                  [-1.1003e+00,  2.8174e+00,  5.4106e-01,  ..., -2.9576e+00,
                   -2.3971e+00, -4.0107e+00],
                  ...,
                  [ 8.7642e-01,  1.1922e+00,  1.1469e+00,  ..., -5.6081e-01,
                    4.3075e-01, -3.4915e+00],
                  [-4.8640e-01,  2.7732e+00,  2.6986e+00,  ..., -3.5611e+00,
                   -3.9170e+00, -1.4755e+00],
                  [ 1.2125e+00,  1.3617e+00,  1.1436e+00,  ...,  1.8816e+00,
                   -2.1333e+00,  3.1291e-01]]]]), '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0': tensor([[[[ 4.0432,  4.0170,  3.3249,  ..., -2.2827, -3.1207, -3.8246],
                  [ 0.9800,  3.9815,  3.2197,  ...,  1.1025, -3.4404, -4.7690],
                  [-0.0531,  2.8222,  3.0953,  ...,  0.8721, -1.4359, -0.5818],
                  ...,
                  [ 2.1721,  2.5899,  3.2993,  ...,  1.3836, -2.5192,  1.5529],
                  [ 1.1639,  3.5522,  2.5923,  ..., -3.7955, -2.3974,  0.7468],
                  [ 0.2589,  2.5643,  1.7778,  ..., -0.0593,  0.9767, -0.4406]],
        
                 [[ 2.7208,  2.2342,  2.4005,  ..., -1.1476, -1.8776, -2.4001],
                  [ 0.1447,  2.1095,  3.2341,  ...,  0.3465, -2.8943, -3.7604],
                  [-0.0855,  2.0417,  2.0281,  ...,  1.0772, -1.7398, -0.6426],
                  ...,
                  [ 1.7239,  1.8852,  1.4068,  ...,  0.6084, -1.9105,  1.1477],
                  [-0.8312,  2.1531,  1.9238,  ..., -2.0526, -2.3218, -0.1008],
                  [ 0.0394,  1.4856,  1.7202,  ...,  0.2331,  0.9001, -1.5392]],
        
                 [[ 3.3694,  3.1472,  2.6346,  ..., -1.1043, -2.7491, -4.0257],
                  [ 0.6859,  2.5569,  2.8033,  ...,  0.7966, -2.5443, -3.9963],
                  [ 0.7327,  3.1052,  2.8407,  ...,  0.7893, -2.3157, -0.2564],
                  ...,
                  [ 2.1075,  1.9509,  2.2160,  ...,  1.0528, -2.4448,  0.6116],
                  [ 0.3089,  3.4314,  2.5030,  ..., -3.4886, -2.2617, -0.5000],
                  [-0.7092,  1.0460,  1.7497,  ...,  0.1319,  0.0481, -1.4391]],
        
                 ...,
        
                 [[ 0.0767,  0.5266, -0.0352,  ...,  1.2743,  0.9852,  0.2187],
                  [-2.9653, -0.4766,  0.8398,  ...,  0.3509,  0.5864,  0.4076],
                  [-2.7855, -0.7134,  0.2877,  ...,  0.4153,  1.9810,  0.5164],
                  ...,
                  [ 0.0261, -0.4490, -0.2143,  ...,  1.0980,  1.2296,  1.2889],
                  [-2.7953, -0.0287,  0.6251,  ...,  0.8660,  0.5468,  0.1689],
                  [-1.1382, -0.4871,  0.6692,  ...,  0.2189,  1.1152, -0.2838]],
        
                 [[ 3.6898,  3.7982,  3.1449,  ..., -2.3583, -3.0083, -4.2316],
                  [ 2.1175,  3.8634,  4.2840,  ...,  0.8298, -4.4467, -5.4375],
                  [ 0.2728,  3.6655,  1.6907,  ...,  0.6970, -2.1825, -1.7727],
                  ...,
                  [ 1.9808,  3.0464,  2.4479,  ...,  0.9279, -2.7479,  0.8326],
                  [ 0.7350,  4.0766,  2.0744,  ..., -3.5957, -2.7662,  0.6987],
                  [ 0.3416,  3.2223,  2.0531,  ..., -1.0705,  0.9742, -0.2733]],
        
                 [[ 3.6837,  2.3163,  1.9711,  ..., -1.7271, -1.5159, -2.5247],
                  [ 1.7117,  3.0187,  4.0444,  ...,  0.5022, -3.6498, -4.1638],
                  [-1.4368,  1.3433,  0.9259,  ...,  1.1591,  0.0460, -0.7501],
                  ...,
                  [ 0.7069,  2.1255,  1.7908,  ...,  0.6464, -2.3182,  1.2112],
                  [-1.7241,  1.5517,  2.6164,  ..., -2.1743, -2.9206,  0.7924],
                  [ 0.6055,  1.9988,  2.0708,  ...,  0.1316,  0.8250, -0.2724]]]]), '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0': tensor([[[ 4.2611,  2.5208,  3.0048,  ...,  1.4809, -1.4630,  0.4347],
                 [ 2.7454,  1.5528,  1.7241,  ...,  1.1663, -1.0497,  0.5901],
                 [ 3.5937,  2.1709,  2.6875,  ...,  1.3793, -1.4384,  0.6412],
                 ...,
                 [-1.8958, -2.0568, -1.7821,  ...,  0.1119, -0.4938, -0.4196],
                 [ 5.3841,  2.5785,  3.3113,  ...,  1.9118, -2.0167,  1.0527],
                 [ 3.4310,  2.3340,  2.3309,  ...,  1.0716, -1.0450,  0.3493]]]), '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0': tensor([[[ 2.6295,  2.5487,  1.8629,  ...,  0.6414, -3.1266,  0.2887],
                 [ 1.8983,  1.0944,  1.8833,  ...,  0.8816, -2.6064,  1.6409],
                 [ 2.1322,  1.2395,  2.4062,  ...,  0.4168, -3.4426,  0.6236],
                 ...,
                 [-0.0188, -0.9068,  0.0464,  ...,  1.0469,  0.1763,  0.7321],
                 [ 3.4297,  3.5927,  1.8174,  ...,  0.7483, -2.9561,  1.4476],
                 [ 2.9607,  2.2851,  0.7851,  ...,  1.8816, -2.1333,  0.3129]]]), '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0': tensor([[[ 4.0432,  4.0170,  3.3249,  ..., -0.0593,  0.9767, -0.4406],
                 [ 2.7208,  2.2342,  2.4005,  ...,  0.2331,  0.9001, -1.5392],
                 [ 3.3694,  3.1472,  2.6346,  ...,  0.1319,  0.0481, -1.4391],
                 ...,
                 [ 0.0767,  0.5266, -0.0352,  ...,  0.2189,  1.1152, -0.2838],
                 [ 3.6898,  3.7982,  3.1449,  ..., -1.0705,  0.9742, -0.2733],
                 [ 3.6837,  2.3163,  1.9711,  ...,  0.1316,  0.8250, -0.2724]]]), '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 0.1683,  0.7307,  1.4870,  ..., -1.4069,  2.7385, -5.7918],
                 [-0.1652,  0.5795,  1.3560,  ..., -2.9530,  6.6592, -5.4050],
                 [ 0.3200,  0.4864,  1.2892,  ..., -3.3546,  6.3501, -6.1239],
                 ...,
                 [ 1.7199,  0.1361,  2.1608,  ..., -1.9402,  1.6150, -2.1676],
                 [ 1.0782,  0.2857,  1.7967,  ..., -2.6059,  7.4666, -5.5436],
                 [ 0.6289,  0.1797,  1.5831,  ..., -1.3125,  5.5169, -2.7646]]]), '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 0.5720,  1.4100,  0.9941,  ..., -1.8422,  2.1949, -3.0456],
                 [ 1.9280,  1.4613,  1.7571,  ..., -1.4024,  2.0449, -3.4692],
                 [ 2.5158,  1.0000,  2.2122,  ..., -0.8249,  3.3986, -3.1441],
                 ...,
                 [ 3.1171,  2.3062,  2.7631,  ..., -1.7760,  1.2576, -1.7643],
                 [ 1.0462,  1.2801,  1.5841,  ..., -1.7048,  3.9225, -2.2835],
                 [ 1.8525,  2.0907,  0.7756,  ..., -1.3169,  1.6684, -1.7301]]]), '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0': tensor([[[-2.4460e-01,  5.9259e-02, -1.1141e-03,  ..., -1.5257e+00,
                   4.6535e+00, -4.5921e+00],
                 [-1.0363e+00, -3.7030e-01, -1.4653e-01,  ..., -3.2033e+00,
                   2.5486e+00, -4.1279e+00],
                 [-6.9667e-01,  2.2638e-01,  5.4468e-01,  ..., -3.1966e+00,
                   3.1626e+00, -4.4627e+00],
                 ...,
                 [-5.7167e-01, -6.2497e-01,  1.0672e+00,  ..., -2.2920e+00,
                   3.5343e+00, -3.3642e+00],
                 [ 6.6064e-01, -1.8948e-01,  1.6270e+00,  ..., -2.1065e+00,
                   3.6077e+00, -4.3836e+00],
                 [-3.9864e-02, -1.4339e+00,  1.5432e+00,  ..., -1.8162e+00,
                   4.1330e+00, -4.0583e+00]]]), 'labels': tensor([[ 0, 67, 67, 26,  8,  0,  8,  8,  8,  0,  8,  0,  0,  8,  0,  8, 67, 24,
                  8, 24, 67,  8,  8,  8,  8,  0,  0, 67,  0, 24,  8,  8, 28,  0,  8, 24,
                  8,  8,  0, 24,  0,  8,  0, 67, 26,  8,  8,  0,  0,  0,  8,  8,  0, 67,
                  8,  8,  0,  0, 67, 24,  0, 26, 28,  8,  0, 67, 24,  8,  0,  0, 24,  8,
                  8,  2, 67, 67,  8,  0,  0,  8, 24,  0, 13,  8,  8, 26, 24, 67,  8, 26,
                 28, 67,  8, 67, 24,  8,  8,  8, 24,  8,  0, 26,  0, 26,  2, 67,  8,  0,
                 26, 24, 26, 76, 56, 24, 13,  8,  0, 26, 28, 26, 67,  0, 79, 28,  8,  8,
                 24,  0,  0, 28, 67,  8,  8, 24,  8,  8, 26, 26, 58, 13, 58, 24, 67, 24,
                  8, 26, 13,  0, 24, 24,  8,  8, 24,  0,  8,  8, 24, 24,  0, 24, 76, 26,
                 26, 26, 26, 28, 13, 24,  0,  0,  8,  0,  8, 26, 13, 26, 26, 67,  8,  8,
                 13,  0,  2,  8,  0,  0,  8, 28, 28, 28, 26, 26,  0,  8,  0,  0, 39, 58,
                 26, 26, 13,  8,  8, 58, 24,  0,  8, 67,  0,  0,  8, 24,  0,  2, 26, 13,
                  8,  8, 26,  8,  8,  8,  2, 24,  0, 26,  0,  8,  8, 28,  1, 28, 13, 26,
                  0, 28, 24,  0, 25, 26, 26, 56,  8,  0, 79,  1,  0, 67,  1,  1, 67, 28,
                 67, 24, 79, 24, 67,  8,  0, 59, 28,  3, 74, 27, 74,  0, 24, 43,  9, 43,
                 10, 73,  3,  8, 79, 24,  0,  0,  2, 24,  0, 56, 13, 28, 73, 79,  0, 24,
                  0, 13, 39, 13, 13, 26, 26,  8, 56, 28, 28, 24]]), 'boxes': tensor([[[262.9834,  66.3367, 549.6310, 475.7531],
                 [360.5361, 136.0820, 373.7987, 169.1412],
                 [360.6453, 135.5560, 384.3634, 199.7513],
                 ...,
                 [169.8333, 238.0871, 224.9150, 279.7958],
                 [154.2778, 201.5859, 230.3990, 279.9140],
                 [188.3705, 226.3198, 223.4867, 258.2858]]]), 'scores': tensor([[0.5844, 0.4758, 0.4500, 0.4185, 0.4173, 0.4012, 0.3364, 0.3303, 0.3272,
                 0.3045, 0.2979, 0.2899, 0.2799, 0.2709, 0.2675, 0.2627, 0.2626, 0.2591,
                 0.2471, 0.2452, 0.2332, 0.2287, 0.2236, 0.2140, 0.2139, 0.2122, 0.2074,
                 0.2015, 0.1970, 0.1943, 0.1862, 0.1773, 0.1768, 0.1759, 0.1750, 0.1707,
                 0.1706, 0.1681, 0.1668, 0.1648, 0.1640, 0.1620, 0.1609, 0.1607, 0.1597,
                 0.1495, 0.1461, 0.1455, 0.1447, 0.1428, 0.1413, 0.1411, 0.1410, 0.1353,
                 0.1353, 0.1334, 0.1321, 0.1316, 0.1316, 0.1303, 0.1270, 0.1243, 0.1229,
                 0.1227, 0.1223, 0.1221, 0.1219, 0.1215, 0.1213, 0.1200, 0.1198, 0.1197,
                 0.1169, 0.1163, 0.1159, 0.1154, 0.1148, 0.1144, 0.1141, 0.1116, 0.1111,
                 0.1108, 0.1106, 0.1097, 0.1082, 0.1076, 0.1074, 0.1069, 0.1063, 0.1062,
                 0.1046, 0.1041, 0.1040, 0.1039, 0.1022, 0.1016, 0.1013, 0.1012, 0.1007,
                 0.1002, 0.0994, 0.0990, 0.0988, 0.0987, 0.0987, 0.0985, 0.0984, 0.0983,
                 0.0983, 0.0980, 0.0972, 0.0970, 0.0964, 0.0952, 0.0941, 0.0930, 0.0926,
                 0.0916, 0.0915, 0.0915, 0.0897, 0.0889, 0.0885, 0.0879, 0.0873, 0.0873,
                 0.0864, 0.0863, 0.0861, 0.0851, 0.0849, 0.0846, 0.0845, 0.0844, 0.0833,
                 0.0829, 0.0827, 0.0824, 0.0817, 0.0812, 0.0810, 0.0807, 0.0806, 0.0786,
                 0.0785, 0.0776, 0.0773, 0.0767, 0.0760, 0.0758, 0.0756, 0.0756, 0.0746,
                 0.0735, 0.0731, 0.0728, 0.0720, 0.0719, 0.0713, 0.0711, 0.0700, 0.0697,
                 0.0695, 0.0695, 0.0691, 0.0687, 0.0683, 0.0679, 0.0677, 0.0675, 0.0664,
                 0.0662, 0.0659, 0.0646, 0.0645, 0.0644, 0.0636, 0.0634, 0.0631, 0.0630,
                 0.0630, 0.0629, 0.0627, 0.0622, 0.0616, 0.0614, 0.0610, 0.0608, 0.0600,
                 0.0593, 0.0591, 0.0590, 0.0590, 0.0584, 0.0584, 0.0582, 0.0581, 0.0581,
                 0.0580, 0.0579, 0.0577, 0.0570, 0.0568, 0.0567, 0.0567, 0.0566, 0.0566,
                 0.0564, 0.0564, 0.0561, 0.0561, 0.0560, 0.0556, 0.0555, 0.0555, 0.0552,
                 0.0548, 0.0546, 0.0545, 0.0545, 0.0541, 0.0538, 0.0538, 0.0527, 0.0526,
                 0.0525, 0.0523, 0.0519, 0.0517, 0.0513, 0.0513, 0.0513, 0.0510, 0.0507,
                 0.0507, 0.0505, 0.0503, 0.0503, 0.0498, 0.0498, 0.0496, 0.0495, 0.0493,
                 0.0492, 0.0491, 0.0490, 0.0489, 0.0488, 0.0486, 0.0485, 0.0484, 0.0483,
                 0.0482, 0.0481, 0.0478, 0.0477, 0.0476, 0.0474, 0.0473, 0.0471, 0.0469,
                 0.0468, 0.0463, 0.0462, 0.0462, 0.0460, 0.0458, 0.0450, 0.0447, 0.0446,
                 0.0443, 0.0443, 0.0443, 0.0440, 0.0438, 0.0438, 0.0437, 0.0436, 0.0436,
                 0.0434, 0.0431, 0.0429, 0.0428, 0.0427, 0.0427, 0.0427, 0.0426, 0.0426,
                 0.0425, 0.0425, 0.0423, 0.0423, 0.0422, 0.0419, 0.0417, 0.0417, 0.0415,
                 0.0412, 0.0410, 0.0409]])}[0m
[38;5;10m[I] trt-runner-N2-05/19/25-15:43:09     | Completed 1 iteration(s) in 10.49 ms | Average inference time: 10.49 ms.[0m
[38;5;14m[I] onnxrt-runner-N2-05/19/25-15:43:09  | Activating and starting inference[0m
[38;5;14m[I] Creating ONNX-Runtime Inference Session with providers: ['CPUExecutionProvider'][0m
[38;5;11m[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. [0m
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] onnxrt-runner-N2-05/19/25-15:43:09 
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[38;5;104m[X] onnxrt-runner-N2-05/19/25-15:43:09  | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}[0m
[38;5;13m[V] onnxrt-runner-N2-05/19/25-15:43:09  | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}[0m
[I] onnxrt-runner-N2-05/19/25-15:43:09 
    ---- Inference Output(s) ----
    {/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 [dtype=float32, shape=(1, 300, 8, 12, 2)],
     /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 [dtype=float32, shape=(1, 300, 8, 12)],
     /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 [dtype=float32, shape=(1, 300, 96)],
     /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 [dtype=float32, shape=(1, 300, 192)],
     labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[38;5;104m[X] onnxrt-runner-N2-05/19/25-15:43:09  | Inference Time: 138.508 ms | Received outputs:
        {'/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0': tensor([[[[[-2.9626e-02,  5.0311e-01],
                   [ 1.3804e+00,  1.7026e+00],
                   [ 8.2607e-01, -8.8824e-01],
                   ...,
                   [ 1.5044e+00,  1.3606e+00],
                   [ 9.4686e-01, -4.4774e-01],
                   [ 2.4600e+00, -2.7122e-01]],
        
                  [[ 3.1604e+00,  1.2959e+00],
                   [ 4.5860e+00, -3.6105e-01],
                   [ 3.5112e+00,  1.8520e+00],
                   ...,
                   [ 1.7517e+00,  2.5488e+00],
                   [ 2.6871e+00,  2.7301e+00],
                   [ 4.2822e+00,  3.8652e+00]],
        
                  [[-4.0771e-01, -9.5015e-01],
                   [ 1.3968e+00,  3.6191e+00],
                   [-1.7504e+00,  3.9618e+00],
                   ...,
                   [-2.4087e+00,  2.4920e+00],
                   [-9.4663e-01,  3.7468e+00],
                   [-2.3662e+00,  2.8445e+00]],
        
                  ...,
        
                  [[-2.3464e+00,  1.3922e+00],
                   [-2.0137e+00, -9.7737e-01],
                   [-1.4886e+00, -2.1549e+00],
                   ...,
                   [-4.4665e-01,  9.3095e-01],
                   [-1.5601e+00, -1.9832e+00],
                   [-1.3788e-01,  8.1932e-01]],
        
                  [[-1.6326e-01, -2.8504e+00],
                   [-5.3179e-01, -3.6033e+00],
                   [ 3.5804e-01, -3.8603e+00],
                   ...,
                   [-2.5892e+00, -1.7269e+00],
                   [-1.1735e+00, -2.3692e+00],
                   [-1.6888e+00, -1.7484e+00]],
        
                  [[ 2.1574e+00, -3.7917e-01],
                   [ 2.6141e+00, -1.6792e+00],
                   [ 1.0647e+00, -2.0192e+00],
                   ...,
                   [-1.3369e+00, -4.7207e-01],
                   [ 3.7600e-01, -1.4008e+00],
                   [ 2.7708e+00, -5.5250e+00]]],
        
        
                 [[[-9.5339e-02,  4.4303e-01],
                   [ 1.3873e+00,  1.4964e+00],
                   [ 1.4088e+00, -5.5196e-01],
                   ...,
                   [ 3.8726e-01,  2.7948e-01],
                   [ 5.3749e-01, -1.5643e-01],
                   [ 2.0041e+00,  5.3412e-01]],
        
                  [[ 1.1943e+00,  1.8083e+00],
                   [ 4.2689e+00, -4.0903e-01],
                   [ 3.8032e+00,  2.3514e+00],
                   ...,
                   [ 2.8568e+00,  3.0104e+00],
                   [ 2.4857e+00,  2.0917e+00],
                   [ 4.3961e+00,  4.5826e+00]],
        
                  [[ 2.3091e-01, -7.1097e-01],
                   [ 1.9139e+00,  3.9665e+00],
                   [-1.7010e+00,  4.3265e+00],
                   ...,
                   [-2.9632e-01,  2.5024e+00],
                   [ 2.1429e+00,  3.8519e+00],
                   [-4.6944e-01,  3.0240e+00]],
        
                  ...,
        
                  [[-3.4047e+00,  5.4983e-01],
                   [-2.2266e+00, -1.3346e+00],
                   [-2.5695e+00, -2.0196e+00],
                   ...,
                   [-4.3132e-01,  3.6328e-01],
                   [-1.5082e+00, -1.8268e+00],
                   [-1.5066e+00,  1.1837e+00]],
        
                  [[ 5.0373e-02, -3.2635e+00],
                   [-2.1386e+00, -3.7240e+00],
                   [ 1.4776e+00, -3.8233e+00],
                   ...,
                   [ 1.1425e+00, -1.8731e+00],
                   [ 7.6900e-01, -2.2674e+00],
                   [ 5.1408e-01, -1.6097e+00]],
        
                  [[ 3.0129e+00, -3.6718e-01],
                   [ 2.8157e+00, -2.1283e+00],
                   [ 1.4257e+00, -2.3667e+00],
                   ...,
                   [-1.3589e-02, -9.4063e-01],
                   [-1.1014e-01, -3.0156e+00],
                   [ 6.4843e+00, -5.8382e+00]]],
        
        
                 [[[ 8.4702e-01, -4.3607e-01],
                   [ 1.5967e+00,  7.8520e-01],
                   [ 1.6803e+00, -1.7687e+00],
                   ...,
                   [ 1.2267e+00, -3.9536e-01],
                   [ 1.1491e+00,  1.1001e+00],
                   [ 1.6285e+00, -1.0051e+00]],
        
                  [[ 2.5106e+00,  1.8155e+00],
                   [ 3.9521e+00, -4.5794e-01],
                   [ 3.8971e+00,  2.4780e+00],
                   ...,
                   [ 3.9414e+00,  7.9669e-01],
                   [ 3.0212e+00, -1.3067e+00],
                   [ 3.7526e+00,  2.0901e+00]],
        
                  [[-9.1440e-01,  1.2334e+00],
                   [ 2.6067e+00,  3.9110e+00],
                   [-7.9023e-02,  4.1029e+00],
                   ...,
                   [-7.3608e-02,  2.9596e+00],
                   [ 2.8120e+00,  3.7511e+00],
                   [-9.5722e-01,  3.0524e+00]],
        
                  ...,
        
                  [[-3.0343e+00, -7.8578e-01],
                   [-3.3878e+00, -2.2207e+00],
                   [-3.4036e+00, -3.3212e+00],
                   ...,
                   [-3.5384e-01, -1.7178e+00],
                   [-9.7481e-01, -2.2026e+00],
                   [-1.1872e+00, -1.0879e+00]],
        
                  [[ 3.1452e-01, -3.8772e+00],
                   [-1.0013e+00, -3.9658e+00],
                   [ 1.2621e+00, -3.9341e+00],
                   ...,
                   [ 2.0400e+00, -2.3238e+00],
                   [-1.3470e+00, -2.5756e+00],
                   [ 6.4108e-01, -2.8480e+00]],
        
                  [[ 3.3446e+00, -1.3616e+00],
                   [ 3.4902e+00, -2.2771e+00],
                   [ 3.2048e+00, -3.1084e+00],
                   ...,
                   [-3.6276e+00, -1.6038e+00],
                   [ 9.3202e-01, -2.5812e+00],
                   [ 4.8259e+00, -2.7178e+00]]],
        
        
                 ...,
        
        
                 [[[ 1.2395e+00,  4.1345e-01],
                   [ 2.0213e+00,  1.1960e+00],
                   [ 2.4461e+00, -8.9112e-01],
                   ...,
                   [ 4.6355e-01,  1.4206e-01],
                   [ 1.8286e+00,  1.8150e+00],
                   [ 1.8688e+00,  1.8287e-01]],
        
                  [[ 1.2162e+00,  2.3428e+00],
                   [ 4.1223e+00,  1.2540e+00],
                   [ 4.1351e+00,  3.0117e+00],
                   ...,
                   [ 3.0947e+00,  2.1540e+00],
                   [ 3.0297e+00,  1.2415e+00],
                   [ 3.7037e+00,  2.0812e+00]],
        
                  [[-7.2682e-01,  1.1491e+00],
                   [ 2.8918e+00,  4.0209e+00],
                   [-9.6383e-01,  4.0169e+00],
                   ...,
                   [-1.7757e-01,  3.1797e+00],
                   [ 1.5271e+00,  3.8930e+00],
                   [-1.3283e+00,  3.1096e+00]],
        
                  ...,
        
                  [[-3.3041e+00, -3.1045e-01],
                   [-3.4672e+00, -1.9771e+00],
                   [-3.5393e+00, -2.7796e+00],
                   ...,
                   [-1.0612e+00, -9.3831e-01],
                   [-1.3689e+00, -2.3790e+00],
                   [-1.5812e+00, -2.4498e-01]],
        
                  [[ 1.2117e+00, -3.7133e+00],
                   [-2.4701e+00, -3.8249e+00],
                   [ 3.6423e+00, -3.8554e+00],
                   ...,
                   [ 1.8935e+00, -2.3368e+00],
                   [-2.0856e+00, -2.4843e+00],
                   [ 8.1738e-01, -2.6743e+00]],
        
                  [[ 3.7656e+00, -7.1657e-01],
                   [ 3.5946e+00, -1.9811e+00],
                   [ 3.0353e+00, -3.2562e+00],
                   ...,
                   [-2.0125e+00, -2.1648e+00],
                   [ 9.5891e-01, -1.6325e+00],
                   [ 1.9428e+00, -2.1817e+00]]],
        
        
                 [[[ 6.0771e-01,  1.0262e-01],
                   [ 1.4831e+00,  1.1067e+00],
                   [ 1.0610e+00, -1.2201e+00],
                   ...,
                   [ 7.2343e-01,  2.4883e-01],
                   [ 2.2162e-01, -6.4252e-02],
                   [ 2.0676e+00, -3.9533e-03]],
        
                  [[ 1.3820e+00,  1.4330e+00],
                   [ 4.1732e+00, -9.7843e-01],
                   [ 3.5571e+00,  2.0292e+00],
                   ...,
                   [ 3.2113e+00,  2.6654e+00],
                   [ 1.9980e+00,  1.2863e+00],
                   [ 3.9677e+00,  3.9829e+00]],
        
                  [[-1.3117e-01, -5.9895e-01],
                   [ 1.8525e+00,  3.7325e+00],
                   [-1.6235e+00,  4.0922e+00],
                   ...,
                   [ 4.0785e-01,  2.5183e+00],
                   [ 2.8739e+00,  3.7995e+00],
                   [-5.6733e-01,  2.1583e+00]],
        
                  ...,
        
                  [[-2.7160e+00, -3.2235e-01],
                   [-2.3320e+00, -1.7031e+00],
                   [-2.1535e+00, -2.4451e+00],
                   ...,
                   [-3.5487e-01, -4.1812e-01],
                   [-1.1130e+00, -1.9623e+00],
                   [-1.1510e+00,  8.9924e-01]],
        
                  [[ 4.9821e-01, -3.1058e+00],
                   [-1.4791e+00, -3.7269e+00],
                   [ 1.4693e+00, -3.8320e+00],
                   ...,
                   [ 1.4400e+00, -1.8285e+00],
                   [-2.2986e-01, -2.1105e+00],
                   [ 8.0130e-01, -1.7942e+00]],
        
                  [[ 2.6409e+00, -9.5229e-01],
                   [ 2.7350e+00, -2.5601e+00],
                   [ 1.5331e+00, -2.6201e+00],
                   ...,
                   [-3.4041e+00,  1.6932e-01],
                   [ 9.6725e-02, -2.4743e+00],
                   [ 6.6096e+00, -4.1296e+00]]],
        
        
                 [[[ 6.2046e-01,  2.2938e-01],
                   [ 1.4172e+00,  9.8789e-01],
                   [ 1.1880e+00, -1.0214e+00],
                   ...,
                   [ 4.4593e-01, -1.3600e-01],
                   [ 6.2095e-01, -1.0169e+00],
                   [ 2.2083e+00,  3.7741e-01]],
        
                  [[ 1.0274e+00,  1.3362e+00],
                   [ 4.2800e+00, -1.1181e+00],
                   [ 3.5766e+00,  1.7940e+00],
                   ...,
                   [ 2.9678e+00,  2.4085e+00],
                   [ 2.0956e+00,  1.5165e+00],
                   [ 4.3328e+00,  3.7145e+00]],
        
                  [[ 1.2721e-01, -1.1718e+00],
                   [ 1.8150e+00,  3.7037e+00],
                   [-1.1822e+00,  4.1661e+00],
                   ...,
                   [ 6.0087e-01,  2.4921e+00],
                   [ 3.2088e+00,  3.8520e+00],
                   [ 1.3215e-01,  2.3907e+00]],
        
                  ...,
        
                  [[-2.6953e+00, -6.0336e-01],
                   [-2.1463e+00, -1.6653e+00],
                   [-2.1418e+00, -2.1711e+00],
                   ...,
                   [-5.2993e-01, -4.9240e-01],
                   [-1.5827e+00, -1.8434e+00],
                   [-1.3610e+00,  1.0862e+00]],
        
                  [[ 3.0338e-01, -3.0988e+00],
                   [-1.0183e+00, -3.7937e+00],
                   [ 1.1962e+00, -3.9335e+00],
                   ...,
                   [ 1.6064e+00, -1.9946e+00],
                   [ 1.2463e+00, -2.2435e+00],
                   [ 1.0126e+00, -1.5934e+00]],
        
                  [[ 2.6637e+00, -1.0263e+00],
                   [ 2.6858e+00, -2.4457e+00],
                   [ 1.6577e+00, -2.5366e+00],
                   ...,
                   [-1.5776e+00,  1.4631e-01],
                   [ 2.8820e-01, -2.6687e+00],
                   [ 6.3413e+00, -5.3056e+00]]]]]), '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0': tensor([[[[[ 6.5739e-01,  1.0883e+00],
                   [ 8.2806e-01, -1.0424e+00],
                   [ 1.7466e+00, -8.0331e-01],
                   ...,
                   [ 7.4283e-01,  6.0755e-01],
                   [ 2.7224e+00,  3.4009e-01],
                   [ 2.2032e+00, -3.3789e-01]],
        
                  [[ 2.6222e+00,  6.4460e-01],
                   [ 2.3778e+00,  2.4246e+00],
                   [ 2.5137e+00,  1.8394e+00],
                   ...,
                   [ 2.0093e+00,  1.1712e+00],
                   [ 2.2881e+00,  1.6911e+00],
                   [ 5.9008e+00,  6.7485e+00]],
        
                  [[-1.7750e+00, -2.3288e+00],
                   [-7.2683e-01,  3.2791e+00],
                   [-1.5511e+00,  3.8471e+00],
                   ...,
                   [ 1.1714e+00,  9.8654e-01],
                   [-4.4137e-01,  3.2210e+00],
                   [ 3.6412e-01,  3.5302e+00]],
        
                  ...,
        
                  [[-4.5011e+00,  6.8285e-01],
                   [-3.5966e+00, -4.5044e-01],
                   [-3.7993e+00, -1.3147e+00],
                   ...,
                   [-2.0109e+00, -3.2885e-01],
                   [-5.1884e-01, -4.3916e+00],
                   [-4.8551e+00, -3.6086e+00]],
        
                  [[ 1.9040e+00,  3.4585e+00],
                   [-2.5465e-01, -2.5248e+00],
                   [-4.5155e-01, -3.0928e+00],
                   ...,
                   [-7.0463e-01, -1.2255e+00],
                   [-4.7499e-01, -2.9874e+00],
                   [ 2.3300e-01, -2.7097e+00]],
        
                  [[ 3.7860e+00, -1.2398e+00],
                   [ 4.2269e+00,  9.0767e-01],
                   [ 3.3397e+00, -7.9448e-01],
                   ...,
                   [-1.9680e-01, -1.3941e+00],
                   [ 3.0561e+00, -1.8779e+00],
                   [ 2.2116e+00, -3.0860e+00]]],
        
        
                 [[[ 2.1510e+00,  1.7804e+00],
                   [ 1.6356e+00, -9.6380e-01],
                   [ 2.6788e+00,  2.4698e-01],
                   ...,
                   [ 8.9777e-01,  2.1794e+00],
                   [ 2.2380e+00,  1.8898e+00],
                   [ 1.5055e+00,  4.0926e-01]],
        
                  [[ 3.1203e+00,  2.2685e+00],
                   [ 2.0030e+00,  2.7980e+00],
                   [ 2.9386e+00,  2.1402e+00],
                   ...,
                   [ 1.5628e+00,  2.0165e+00],
                   [ 3.0892e+00,  1.6879e+00],
                   [ 7.0913e+00,  4.0157e+00]],
        
                  [[-1.1152e+00, -8.7121e-01],
                   [-1.2338e+00,  3.4290e+00],
                   [-2.0353e+00,  3.7881e+00],
                   ...,
                   [ 8.1966e-01,  1.5176e+00],
                   [-1.2843e+00,  3.2359e+00],
                   [-2.6138e-01,  3.1810e+00]],
        
                  ...,
        
                  [[-4.5001e+00,  1.8781e+00],
                   [-3.8906e+00,  5.1627e-01],
                   [-4.1603e+00, -7.2525e-01],
                   ...,
                   [-5.2775e-01, -3.0315e-01],
                   [ 7.9997e-01, -4.3354e+00],
                   [-2.1172e+00, -3.1760e+00]],
        
                  [[ 1.3510e+00,  2.2846e+00],
                   [-8.4979e-01, -2.5411e+00],
                   [-1.9194e+00, -3.1269e+00],
                   ...,
                   [-4.9989e-01, -1.2187e+00],
                   [-5.5228e-01, -2.8045e+00],
                   [-1.1339e-03, -1.5672e+00]],
        
                  [[ 3.3089e+00, -7.4703e-01],
                   [ 4.0632e+00,  1.7110e+00],
                   [ 3.5947e+00,  4.3159e-01],
                   ...,
                   [-9.3359e-01, -1.6864e+00],
                   [ 2.9746e+00, -4.7968e-01],
                   [ 2.5973e+00, -2.3041e+00]]],
        
        
                 [[[ 2.5166e+00,  1.3435e+00],
                   [ 2.0963e+00, -2.0124e+00],
                   [ 3.0302e+00, -1.3358e+00],
                   ...,
                   [ 1.2633e+00, -9.7110e-01],
                   [ 1.7567e+00,  9.7590e-02],
                   [ 7.2775e-01, -8.8415e-01]],
        
                  [[ 3.3904e+00,  1.0406e+00],
                   [ 2.5396e+00,  3.0087e+00],
                   [ 3.0274e+00,  2.1086e+00],
                   ...,
                   [ 8.7786e-01,  1.3041e+00],
                   [ 2.7439e+00,  1.4391e+00],
                   [ 6.6604e+00,  3.3852e+00]],
        
                  [[ 5.8358e-01,  2.1911e-02],
                   [-4.9549e-01,  3.7547e+00],
                   [-1.8729e+00,  3.6999e+00],
                   ...,
                   [ 6.1769e-01,  1.8878e+00],
                   [-2.7521e-01,  2.8923e+00],
                   [-9.6377e-02,  3.2404e+00]],
        
                  ...,
        
                  [[-3.8046e+00,  2.6622e-01],
                   [-3.7956e+00, -1.5085e+00],
                   [-3.8546e+00, -2.6863e+00],
                   ...,
                   [-1.6349e+00, -8.8092e-01],
                   [-7.5127e-01, -3.5906e+00],
                   [-2.2702e+00, -2.5099e+00]],
        
                  [[-3.6280e-01, -9.8353e-02],
                   [-6.5593e-01, -3.7751e+00],
                   [-1.8356e+00, -3.6350e+00],
                   ...,
                   [-3.5123e-01, -1.8441e+00],
                   [-2.7946e-01, -2.3063e+00],
                   [-5.0791e-03, -2.1279e+00]],
        
                  [[ 3.2053e+00, -1.8274e+00],
                   [ 3.7130e+00, -1.9607e-01],
                   [ 3.7208e+00, -1.2157e+00],
                   ...,
                   [-9.5014e-01, -1.6978e+00],
                   [ 2.0720e+00, -1.4547e+00],
                   [ 1.1467e+00, -1.8441e+00]]],
        
        
                 ...,
        
        
                 [[[ 3.3671e+00,  2.3563e+00],
                   [ 3.2236e+00, -2.1642e+00],
                   [ 3.6109e+00, -4.6150e-01],
                   ...,
                   [ 2.1834e+00,  4.8578e-01],
                   [ 2.3484e+00,  1.9835e+00],
                   [ 1.4502e+00, -3.7189e-01]],
        
                  [[ 3.7752e+00,  2.4368e+00],
                   [ 3.3799e+00,  3.3091e+00],
                   [ 3.9518e+00,  2.8891e+00],
                   ...,
                   [ 2.2446e+00,  2.0769e+00],
                   [ 2.6340e+00,  1.9792e+00],
                   [ 4.4921e+00,  8.3034e-01]],
        
                  [[ 1.0820e+00,  1.0011e+00],
                   [-7.4493e-01,  4.0727e+00],
                   [-3.1015e+00,  3.9790e+00],
                   ...,
                   [-2.1543e-01,  2.9387e+00],
                   [ 5.6303e-01,  3.3902e+00],
                   [ 2.1638e-01,  3.4134e+00]],
        
                  ...,
        
                  [[-4.3768e+00,  2.6116e+00],
                   [-4.0137e+00,  1.4096e+00],
                   [-3.9284e+00, -6.9659e-01],
                   ...,
                   [-1.9403e+00, -4.7104e-01],
                   [-9.8328e-01, -2.5470e+00],
                   [-2.8691e+00, -1.6740e+00]],
        
                  [[-6.3163e-01, -1.0429e+00],
                   [ 2.7599e-01, -3.6025e+00],
                   [-1.7056e+00, -3.4837e+00],
                   ...,
                   [ 9.6464e-01, -1.8594e+00],
                   [ 2.2095e-02, -2.5001e+00],
                   [-2.9557e-01, -1.7884e+00]],
        
                  [[ 4.3851e+00, -1.5660e+00],
                   [ 3.9040e+00,  1.1550e+00],
                   [ 3.9656e+00, -2.4138e-01],
                   ...,
                   [-1.0957e+00, -1.6508e+00],
                   [ 2.7113e+00, -8.7485e-01],
                   [ 1.1280e+00, -1.8341e+00]]],
        
        
                 [[[ 1.4733e+00,  6.6774e-01],
                   [ 1.5430e+00, -2.1069e+00],
                   [ 2.1853e+00, -1.1576e+00],
                   ...,
                   [ 1.2364e+00, -2.0540e-01],
                   [ 2.1876e+00,  8.4890e-02],
                   [ 1.2665e+00, -8.2904e-02]],
        
                  [[ 2.7641e+00,  1.1478e+00],
                   [ 2.5060e+00,  2.5502e+00],
                   [ 2.5644e+00,  1.5476e+00],
                   ...,
                   [ 9.4182e-01,  2.1180e+00],
                   [ 2.8329e+00,  1.5959e+00],
                   [ 8.1195e+00,  4.8258e+00]],
        
                  [[-2.4606e-01, -5.0298e-01],
                   [-6.3087e-02,  3.3698e+00],
                   [-1.1236e+00,  3.5841e+00],
                   ...,
                   [ 1.6464e+00,  8.6388e-01],
                   [-2.6919e-01,  2.9435e+00],
                   [ 6.9223e-01,  2.8663e+00]],
        
                  ...,
        
                  [[-4.1159e+00, -5.4012e-01],
                   [-3.6219e+00, -1.7354e+00],
                   [-3.7604e+00, -2.0506e+00],
                   ...,
                   [-1.6442e+00, -8.1849e-01],
                   [-8.5850e-01, -5.3801e+00],
                   [-3.3982e+00, -3.3966e+00]],
        
                  [[ 4.2219e-01,  1.8193e+00],
                   [-1.6095e-01, -3.0933e+00],
                   [-1.2672e+00, -3.2446e+00],
                   ...,
                   [-7.5868e-01, -1.4061e+00],
                   [ 2.8518e-01, -2.4764e+00],
                   [ 8.9751e-01, -2.3806e+00]],
        
                  [[ 3.5154e+00, -2.1870e+00],
                   [ 3.9560e+00, -2.8318e-01],
                   [ 3.4325e+00, -1.3633e+00],
                   ...,
                   [ 1.4787e-01, -1.6549e+00],
                   [ 3.6530e+00, -1.6876e+00],
                   [ 2.8583e+00, -3.4443e+00]]],
        
        
                 [[[ 2.0375e+00,  1.7558e+00],
                   [ 1.9177e+00, -1.2042e+00],
                   [ 2.4660e+00, -4.8845e-01],
                   ...,
                   [ 1.4141e+00,  1.0150e+00],
                   [ 2.3307e+00,  7.0787e-01],
                   [ 1.8301e+00,  5.8343e-01]],
        
                  [[ 2.9523e+00,  2.3946e+00],
                   [ 3.0132e+00,  2.6165e+00],
                   [ 3.0536e+00,  2.3304e+00],
                   ...,
                   [ 1.2350e+00,  2.2256e+00],
                   [ 3.3686e+00,  2.4792e+00],
                   [ 6.8504e+00,  5.0026e+00]],
        
                  [[-6.6688e-01, -1.3001e-01],
                   [-9.2719e-02,  3.3487e+00],
                   [-1.2211e+00,  3.6801e+00],
                   ...,
                   [ 2.0577e+00,  1.1145e+00],
                   [-4.5301e-01,  2.9939e+00],
                   [ 7.7996e-01,  3.1046e+00]],
        
                  ...,
        
                  [[-3.9568e+00,  9.1613e-01],
                   [-3.4633e+00, -1.6257e-01],
                   [-3.8687e+00, -7.3980e-01],
                   ...,
                   [-8.4120e-01,  7.5890e-01],
                   [-1.0685e+00, -5.0543e+00],
                   [-2.4148e+00, -2.6555e+00]],
        
                  [[-1.8354e-01,  2.8161e+00],
                   [ 2.7992e-01, -2.5643e+00],
                   [-5.4513e-01, -3.0886e+00],
                   ...,
                   [-4.8855e-01, -1.5227e+00],
                   [ 1.1092e+00, -2.5455e+00],
                   [ 1.5533e+00, -2.1705e+00]],
        
                  [[ 3.6467e+00, -1.5104e+00],
                   [ 3.9722e+00,  6.3705e-01],
                   [ 3.3812e+00, -8.1724e-01],
                   ...,
                   [ 1.7121e-01, -9.8417e-01],
                   [ 3.7802e+00, -7.3074e-01],
                   [ 3.4448e+00, -1.8215e+00]]]]]), '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0': tensor([[[[[-4.7352e-01,  3.3598e-02],
                   [-1.4070e-01,  1.2936e+00],
                   [ 4.9469e-01, -7.6794e-03],
                   ...,
                   [ 6.6284e-01,  1.1107e+00],
                   [ 4.4285e-01,  7.5280e-01],
                   [ 9.6479e-01,  7.7465e-01]],
        
                  [[-3.5531e+00, -6.5854e-02],
                   [ 4.3461e+00,  1.3772e-01],
                   [ 4.1213e+00,  2.2297e+00],
                   ...,
                   [ 2.4992e+00,  9.2022e-01],
                   [ 2.2905e+00,  1.9439e+00],
                   [ 3.5082e+00,  3.4121e+00]],
        
                  [[-2.2690e-01, -1.9739e+00],
                   [ 2.7823e-01,  4.5327e+00],
                   [-3.0656e+00,  4.1107e+00],
                   ...,
                   [-7.4419e-01,  1.5044e+00],
                   [ 3.6999e-01,  4.1330e+00],
                   [-7.8784e-02,  4.5757e+00]],
        
                  ...,
        
                  [[-1.8460e+00,  1.6472e+00],
                   [-9.6404e-01, -2.1842e+00],
                   [-1.7174e+00, -1.1928e+00],
                   ...,
                   [-3.1100e-01,  8.6714e-01],
                   [-5.6231e-01, -1.1950e+00],
                   [-1.3386e+00,  5.7670e-01]],
        
                  [[-1.5801e+00,  1.4103e+00],
                   [-4.0227e-01, -3.7514e+00],
                   [-1.0922e+00, -3.7747e+00],
                   ...,
                   [-1.9982e+00, -9.7746e-01],
                   [ 1.2506e+00, -1.4575e+00],
                   [ 1.2981e-01, -3.7588e+00]],
        
                  [[-7.6281e-01, -3.3634e+00],
                   [ 1.4628e+00, -1.8583e+00],
                   [ 2.1825e+00,  6.2508e-01],
                   ...,
                   [ 3.2999e+00, -1.9366e+00],
                   [ 9.5722e-01, -1.7204e+00],
                   [ 4.3141e+00, -4.5292e+00]]],
        
        
                 [[[-1.5946e+00,  1.0562e-01],
                   [-2.1967e-01,  1.1837e+00],
                   [ 7.6737e-01,  4.9628e-01],
                   ...,
                   [ 3.0471e-01,  1.3565e+00],
                   [ 3.2521e-01,  1.3825e+00],
                   [ 4.0888e-01,  9.6820e-01]],
        
                  [[-1.1914e+00,  5.6363e-01],
                   [ 3.8904e+00,  1.6706e+00],
                   [ 3.9571e+00,  2.1609e+00],
                   ...,
                   [ 1.8348e+00,  9.7345e-02],
                   [ 2.5554e+00,  3.4160e+00],
                   [ 3.1501e+00,  4.3630e+00]],
        
                  [[-1.6444e+00,  1.9500e-01],
                   [-2.0051e-01,  4.5046e+00],
                   [-3.5116e+00,  4.0129e+00],
                   ...,
                   [-8.0531e-01,  8.6508e-01],
                   [-1.7855e-02,  3.8872e+00],
                   [-5.1509e-03,  4.3373e+00]],
        
                  ...,
        
                  [[-2.8189e+00,  2.1720e+00],
                   [-9.9726e-01, -2.3285e+00],
                   [-2.6595e+00, -1.5050e+00],
                   ...,
                   [-1.9221e-01,  6.9497e-01],
                   [-2.0656e+00, -6.6519e-01],
                   [-1.8901e+00,  1.1272e+00]],
        
                  [[-1.0728e+00,  1.4028e+00],
                   [-1.0876e+00, -3.7444e+00],
                   [-2.3806e+00, -3.7723e+00],
                   ...,
                   [-2.1112e+00, -1.3050e+00],
                   [ 8.1377e-01, -1.5707e+00],
                   [-1.0978e+00, -3.5312e+00]],
        
                  [[ 9.6047e-01, -2.8990e+00],
                   [ 1.4905e+00, -1.7308e+00],
                   [ 2.9601e+00,  2.7750e+00],
                   ...,
                   [ 2.1394e+00, -2.3007e+00],
                   [ 6.5247e-01, -3.4163e+00],
                   [ 3.0090e+00, -4.3650e+00]]],
        
        
                 [[[-5.1478e-01, -1.6492e+00],
                   [ 3.6770e-01,  4.7703e-02],
                   [ 1.3655e+00, -1.5458e+00],
                   ...,
                   [ 4.1290e-01,  5.7652e-02],
                   [ 6.3678e-01, -1.9731e-01],
                   [ 4.2777e-01, -8.6644e-01]],
        
                  [[-8.1976e-01, -9.7179e-01],
                   [ 3.8002e+00, -1.1559e+00],
                   [ 3.9451e+00,  2.0552e+00],
                   ...,
                   [ 2.1475e-01,  2.5299e-01],
                   [ 2.8337e+00, -3.9164e-01],
                   [ 3.1142e+00,  3.0502e-01]],
        
                  [[ 1.6287e+00,  8.0700e-01],
                   [-5.2824e-02,  4.2021e+00],
                   [-1.2180e+00,  3.9516e+00],
                   ...,
                   [-3.9314e-01,  1.2119e+00],
                   [ 3.4604e-01,  4.0021e+00],
                   [ 2.5375e-01,  3.7740e+00]],
        
                  ...,
        
                  [[-3.0914e+00,  9.5688e-02],
                   [-1.4856e+00, -3.3280e+00],
                   [-2.9852e+00, -2.4904e+00],
                   ...,
                   [ 7.0262e-02, -1.4160e+00],
                   [-6.6277e-01, -2.4420e+00],
                   [-6.8223e-01, -1.2217e+00]],
        
                  [[-3.1684e-02, -1.4123e+00],
                   [-8.3316e-01, -4.0731e+00],
                   [-1.2797e+00, -3.8913e+00],
                   ...,
                   [-9.5416e-01, -2.1275e+00],
                   [ 9.3690e-01, -2.0956e+00],
                   [-1.0430e-01, -3.0138e+00]],
        
                  [[ 1.2610e+00, -3.2419e+00],
                   [ 2.6335e+00, -2.8255e+00],
                   [ 3.4196e+00, -4.4079e-01],
                   ...,
                   [ 2.1424e+00, -2.0518e+00],
                   [ 6.2761e-01, -2.4444e+00],
                   [ 3.6438e+00, -3.7387e+00]]],
        
        
                 ...,
        
        
                 [[[-5.1500e-01, -6.7004e-01],
                   [ 4.4843e-01,  1.1026e+00],
                   [ 2.0584e+00, -1.1486e+00],
                   ...,
                   [-1.2150e-01,  2.7012e+00],
                   [ 1.4657e+00,  1.0575e+00],
                   [ 3.5539e-01,  6.0421e-02]],
        
                  [[-1.7156e-01,  7.6936e-02],
                   [ 4.1054e+00,  2.0699e-01],
                   [ 3.9616e+00,  2.7943e+00],
                   ...,
                   [ 1.2344e+00,  1.2860e+00],
                   [ 2.8740e+00,  1.2247e+00],
                   [ 2.9821e+00,  2.7093e+00]],
        
                  [[ 6.3843e-01,  1.0487e+00],
                   [ 7.5470e-02,  4.1958e+00],
                   [-2.7262e+00,  4.0948e+00],
                   ...,
                   [ 2.6192e-01,  1.7842e+00],
                   [ 1.3518e+00,  4.1160e+00],
                   [-9.2485e-01,  4.4075e+00]],
        
                  ...,
        
                  [[-3.6725e+00,  2.5786e+00],
                   [-1.4117e+00, -2.8565e+00],
                   [-3.3055e+00, -1.7759e+00],
                   ...,
                   [ 1.4634e+00, -6.5520e-01],
                   [-3.2880e-01, -8.7664e-01],
                   [-1.6835e+00, -5.6301e-01]],
        
                  [[ 5.5111e-01, -3.9213e-01],
                   [ 2.2552e-02, -3.8481e+00],
                   [-7.2458e-01, -3.8959e+00],
                   ...,
                   [-1.3182e+00, -2.2221e+00],
                   [ 1.8592e+00, -2.3242e+00],
                   [-1.7961e-01, -3.0282e+00]],
        
                  [[ 1.9184e+00, -3.1528e+00],
                   [ 3.0906e+00, -3.1710e+00],
                   [ 3.7199e+00,  1.3179e+00],
                   ...,
                   [ 1.2686e+00, -1.3781e+00],
                   [-1.3213e-01, -2.2795e+00],
                   [ 3.0152e+00, -4.1140e+00]]],
        
        
                 [[[-6.3012e-01, -8.6077e-01],
                   [ 4.2006e-01,  3.4247e-01],
                   [ 8.1376e-01, -7.9634e-01],
                   ...,
                   [ 6.6952e-01, -6.3572e-01],
                   [ 8.6912e-01, -5.8667e-02],
                   [ 1.1809e+00, -3.3473e-02]],
        
                  [[-2.2424e+00, -3.4792e-01],
                   [ 4.3823e+00, -6.3835e-01],
                   [ 4.1333e+00,  1.5611e+00],
                   ...,
                   [ 7.1723e-02, -9.0433e-01],
                   [ 2.7694e+00,  1.7506e+00],
                   [ 3.9892e+00,  2.2348e+00]],
        
                  [[ 4.4744e-01, -9.5495e-01],
                   [-8.1057e-02,  3.5762e+00],
                   [-1.6723e+00,  3.9380e+00],
                   ...,
                   [-1.4411e+00, -5.1147e-01],
                   [ 2.3654e-01,  3.6756e+00],
                   [-7.4180e-01,  4.0160e+00]],
        
                  ...,
        
                  [[-2.0353e+00, -3.6569e-01],
                   [-6.2269e-01, -2.5728e+00],
                   [-1.6717e+00, -1.9910e+00],
                   ...,
                   [-5.9524e-01, -9.4032e-01],
                   [-1.1887e+00, -1.6813e+00],
                   [-1.1287e+00, -1.0847e+00]],
        
                  [[-3.7576e-01,  1.8961e+00],
                   [-2.3290e-01, -3.7608e+00],
                   [-6.1545e-01, -3.7136e+00],
                   ...,
                   [-1.9703e+00, -1.2780e+00],
                   [ 1.5136e+00, -1.7880e+00],
                   [ 7.8546e-01, -3.6414e+00]],
        
                  [[-1.5182e+00, -2.9454e+00],
                   [ 2.1548e+00, -2.4746e+00],
                   [ 2.5551e+00, -3.4069e-01],
                   ...,
                   [ 3.8776e+00, -1.1784e+00],
                   [ 3.0255e-01, -1.7217e+00],
                   [ 3.7461e+00, -4.2863e+00]]],
        
        
                 [[[ 1.0898e-01,  9.4199e-01],
                   [ 1.4409e+00,  1.4677e+00],
                   [ 1.5107e+00,  1.2889e+00],
                   ...,
                   [ 1.9393e+00,  1.6154e+00],
                   [ 1.8870e+00,  1.8296e+00],
                   [ 2.0576e+00,  1.3799e+00]],
        
                  [[-1.6038e+00,  2.9070e-01],
                   [ 4.3101e+00,  1.2638e+00],
                   [ 4.2163e+00,  2.6919e+00],
                   ...,
                   [ 6.9087e-01, -3.9337e-03],
                   [ 2.9377e+00,  2.9127e+00],
                   [ 3.6282e+00,  3.8093e+00]],
        
                  [[-1.0497e+00, -1.0438e+00],
                   [ 1.5487e+00,  4.6970e+00],
                   [-1.9864e+00,  4.0547e+00],
                   ...,
                   [-2.2244e-01,  1.0702e-01],
                   [ 7.9862e-01,  3.6338e+00],
                   [ 4.2461e-01,  4.3791e+00]],
        
                  ...,
        
                  [[-2.0941e+00,  1.8278e+00],
                   [-6.6709e-02, -1.4333e+00],
                   [-1.4069e+00, -1.1470e+00],
                   ...,
                   [ 3.0359e-01,  8.4350e-01],
                   [-6.5395e-01, -1.4155e-03],
                   [-6.4852e-01,  1.1953e+00]],
        
                  [[-1.7882e+00,  2.6435e+00],
                   [ 4.0609e-01, -3.5706e+00],
                   [-2.3343e-01, -3.6865e+00],
                   ...,
                   [-6.8734e-01, -1.4038e+00],
                   [ 1.7449e+00, -1.8522e+00],
                   [ 1.2562e+00, -3.5072e+00]],
        
                  [[ 3.7348e-01, -2.7042e+00],
                   [ 2.4422e+00, -1.5350e+00],
                   [ 3.0338e+00,  1.9708e+00],
                   ...,
                   [ 1.3783e+00,  1.7176e-01],
                   [ 1.4641e+00, -2.2724e+00],
                   [ 3.3272e+00, -3.5136e+00]]]]]), '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0': tensor([[[[ 3.8864e+00,  2.6560e+00,  3.0121e+00,  ..., -2.1953e+00,
                   -3.0089e+00, -3.1312e+00],
                  [ 1.6113e+00,  2.4142e+00,  2.6142e+00,  ..., -5.0109e+00,
                   -2.7125e+00, -2.5894e+00],
                  [-9.3098e-01,  3.1053e+00,  3.0030e+00,  ..., -2.6049e+00,
                   -2.4928e+00, -2.7077e+00],
                  ...,
                  [ 2.7089e+00,  2.9258e+00,  3.2356e+00,  ...,  1.8530e-01,
                   -3.3936e+00,  1.4033e+00],
                  [ 3.3739e+00,  3.2462e+00,  3.5344e+00,  ...,  9.9018e-01,
                   -3.7602e+00, -3.5003e+00],
                  [ 8.7974e-01,  1.2086e+00,  1.2997e+00,  ...,  1.4150e+00,
                   -1.3747e+00,  5.1302e-01]],
        
                 [[ 2.9682e+00,  1.3912e+00,  1.6830e+00,  ..., -1.9778e-01,
                   -2.9512e+00, -1.9104e+00],
                  [-2.9359e-01,  1.9063e+00,  2.1364e+00,  ..., -2.8679e+00,
                   -1.6543e+00, -1.7252e+00],
                  [-2.1586e+00,  2.2035e+00,  2.3946e+00,  ..., -2.2343e+00,
                   -1.4859e+00, -1.5427e+00],
                  ...,
                  [ 2.4410e+00,  1.9994e+00,  2.5094e+00,  ...,  3.3492e-01,
                   -2.9186e+00,  1.1084e+00],
                  [ 1.8997e+00,  2.1721e+00,  2.4938e+00,  ..., -1.0955e-01,
                   -3.7008e+00, -1.8711e+00],
                  [ 9.0318e-01,  8.6024e-01,  8.2960e-01,  ...,  1.2557e+00,
                   -1.0860e+00,  5.8152e-01]],
        
                 [[-1.0341e+00, -1.6683e-01,  1.5413e-01,  ...,  1.6568e+00,
                    5.5727e-01,  6.7211e-01],
                  [-1.8131e+00,  5.4374e-02,  4.4451e-01,  ..., -2.6599e-01,
                    2.9458e-01, -6.5159e-01],
                  [-1.2081e+00,  4.3939e-01,  1.5954e-01,  ...,  1.7240e-03,
                    2.4412e-01, -1.1490e-01],
                  ...,
                  [-6.2450e-01,  7.3257e-01,  5.9018e-01,  ...,  1.0572e+00,
                   -5.4897e-01, -5.1677e-02],
                  [ 1.7284e-01,  7.8768e-01,  6.8880e-01,  ...,  4.1344e-01,
                   -1.1270e+00, -7.0857e-01],
                  [-2.5763e-01,  3.1391e-01, -4.0031e-01,  ...,  1.3031e-01,
                   -7.8524e-02, -3.6164e-01]],
        
                 ...,
        
                 [[-1.1155e+00, -1.3151e+00, -1.1665e+00,  ...,  1.6249e+00,
                    1.5542e+00,  1.0035e+00],
                  [-1.9593e+00,  1.8104e-01,  6.0639e-01,  ..., -8.7885e-01,
                   -6.4888e-01,  1.5454e-01],
                  [-2.9724e+00,  7.3425e-01,  1.3069e-01,  ..., -2.7351e-01,
                    5.7789e-01, -8.9149e-01],
                  ...,
                  [-1.0052e-01,  1.0048e-01,  7.1309e-01,  ...,  7.6466e-01,
                   -1.4934e+00,  1.0132e+00],
                  [ 1.4174e-01,  1.4977e-01, -1.8534e-01,  ...,  7.5514e-01,
                   -1.8774e-01, -7.3620e-01],
                  [-1.8478e-01, -8.3342e-03, -5.4790e-01,  ...,  3.6886e-01,
                   -8.3651e-01, -3.3297e-01]],
        
                 [[ 3.7093e+00,  2.3833e+00,  2.7045e+00,  ..., -2.1001e+00,
                   -2.5054e+00, -3.2245e+00],
                  [ 6.4102e-01,  2.0790e+00,  2.9145e+00,  ..., -3.6915e+00,
                   -2.5528e+00, -2.7272e+00],
                  [ 3.4890e-01,  3.3598e+00,  2.7466e+00,  ..., -2.7637e+00,
                   -1.8221e+00, -2.4976e+00],
                  ...,
                  [ 2.3157e+00,  2.4551e+00,  3.5161e+00,  ...,  2.1817e-01,
                   -3.1533e+00,  1.0691e+00],
                  [ 3.2675e+00,  3.7400e+00,  3.3350e+00,  ...,  4.8311e-01,
                   -4.8542e+00, -3.4087e+00],
                  [ 1.0330e+00,  9.8612e-01,  1.0605e+00,  ...,  1.2936e+00,
                   -1.3532e+00,  7.3365e-01]],
        
                 [[ 4.5051e+00,  2.3447e+00,  2.8511e+00,  ..., -2.3870e+00,
                   -3.5071e+00, -3.3199e+00],
                  [ 6.4207e-01,  2.1776e+00,  2.5633e+00,  ..., -3.6744e+00,
                   -2.1550e+00, -2.4500e+00],
                  [ 3.4490e-01,  3.3515e+00,  2.9548e+00,  ..., -3.2627e+00,
                   -1.6678e+00, -2.5257e+00],
                  ...,
                  [ 2.4017e+00,  2.5294e+00,  3.2760e+00,  ...,  2.0372e-01,
                   -3.1030e+00,  1.2690e+00],
                  [ 3.2247e+00,  3.4253e+00,  3.3944e+00,  ...,  7.8766e-02,
                   -5.2547e+00, -3.0413e+00],
                  [ 9.8988e-01,  1.3154e+00,  1.0932e+00,  ...,  1.5855e+00,
                   -1.3272e+00,  8.0677e-01]]]]), '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0': tensor([[[[ 2.6173,  2.4923,  1.8669,  ..., -3.5363, -3.4766, -2.6207],
                  [ 1.5001,  2.1129,  2.4533,  ..., -2.0339, -2.2282,  0.9332],
                  [ 0.7101,  3.0754,  3.3411,  ..., -3.8219, -4.0025, -1.6716],
                  ...,
                  [ 2.3858,  1.6422,  0.9790,  ..., -0.7496,  1.1663, -3.8354],
                  [ 1.0473,  3.7199,  2.7989,  ..., -3.4424, -4.9100, -2.5540],
                  [ 1.7258,  1.2984,  0.7174,  ...,  0.5285, -3.2315,  0.2703]],
        
                 [[ 2.3529,  1.0416,  1.9073,  ..., -2.5955, -2.7914, -2.5528],
                  [ 1.3533,  1.0567,  2.1470,  ..., -1.1430, -2.0085,  0.6657],
                  [-1.0832,  2.7968,  2.7571,  ..., -2.4352, -3.7165, -2.2471],
                  ...,
                  [ 1.1894,  1.8599,  1.9270,  ..., -0.3508,  0.5695, -3.2059],
                  [-0.7137,  3.0389,  2.2639,  ..., -3.2947, -3.4605, -1.5399],
                  [ 0.6152,  1.7135,  1.4253,  ...,  0.9839, -2.8940,  1.2901]],
        
                 [[ 0.6215,  0.4753,  0.6065,  ..., -0.4052, -1.3917, -0.2338],
                  [ 0.6264,  0.5480,  1.0207,  ...,  0.2241, -2.0427, -0.4415],
                  [-1.8538,  1.0131,  0.4768,  ..., -0.1581, -0.6637, -1.9499],
                  ...,
                  [ 0.1402,  0.6538,  0.5636,  ..., -0.2381, -0.1857, -1.7170],
                  [-1.8888,  0.8931,  0.8292,  ..., -1.1609, -1.2398, -0.5222],
                  [-0.1818,  0.5529,  0.6991,  ...,  1.0875, -0.6423,  0.5404]],
        
                 ...,
        
                 [[-0.1215, -0.7094,  0.4629,  ...,  0.5710,  0.4645, -0.1105],
                  [-0.0377, -0.1068,  0.1866,  ...,  0.9044, -0.5807, -0.4958],
                  [-2.6755,  0.3787,  0.5123,  ...,  0.3671,  0.0590, -1.0650],
                  ...,
                  [-0.5134,  0.2979,  0.2152,  ...,  0.6018,  0.1271, -0.5756],
                  [-2.3381,  0.6499,  0.0848,  ...,  0.3056, -0.7057,  0.1302],
                  [-1.2395,  0.3969,  0.7082,  ...,  1.1607, -0.8799,  0.4742]],
        
                 [[ 2.4958,  2.7087,  1.6094,  ..., -4.1675, -4.0522, -3.0272],
                  [ 1.5459,  2.2509,  2.3699,  ..., -1.2259, -2.9062,  1.1884],
                  [-0.4718,  3.2835,  2.1105,  ..., -2.7594, -3.8213, -3.3643],
                  ...,
                  [ 1.6648,  1.5913,  1.1224,  ..., -1.1653,  0.0171, -3.2398],
                  [-0.3361,  3.3210,  2.8777,  ..., -3.8188, -4.5605, -2.2729],
                  [ 1.5497,  1.4222,  1.0335,  ...,  1.0292, -2.2524,  0.7039]],
        
                 [[ 2.8415,  2.3623,  1.8292,  ..., -3.4406, -3.2440, -2.5911],
                  [ 0.9362,  1.5937,  2.2280,  ..., -1.3789, -2.3629,  0.9474],
                  [-0.7605,  2.3421,  2.1674,  ..., -2.8195, -3.0883, -3.0568],
                  ...,
                  [ 1.4676,  1.7857,  0.9614,  ..., -0.8216,  0.5061, -3.0744],
                  [-0.7089,  3.2738,  2.2222,  ..., -3.4601, -4.3843, -2.7069],
                  [ 1.4399,  1.8931,  1.4082,  ...,  1.1610, -2.3385,  1.4377]]]]), '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0': tensor([[[[ 3.9151,  4.2549,  3.1362,  ..., -2.4364, -3.1740, -3.7321],
                  [ 1.3221,  3.7641,  3.1905,  ...,  1.3933, -3.5835, -5.0356],
                  [-0.4713,  2.7925,  3.0537,  ...,  0.9258, -1.6190, -0.3539],
                  ...,
                  [ 2.0376,  2.6577,  3.2978,  ...,  0.9834, -2.5509,  1.5784],
                  [ 0.7834,  3.4902,  3.1609,  ..., -3.4826, -3.1710,  0.8098],
                  [ 0.5707,  2.5263,  1.8921,  ...,  0.1553,  0.9190, -0.7568]],
        
                 [[ 2.7675,  2.5564,  2.1182,  ..., -0.8290, -2.1331, -2.8877],
                  [ 0.5220,  1.7180,  4.0806,  ...,  0.1133, -3.8336, -3.6960],
                  [ 0.1771,  2.2643,  2.4974,  ...,  0.7391, -2.1801, -0.4616],
                  ...,
                  [ 2.0779,  1.8550,  1.4699,  ...,  0.6786, -2.2316,  1.2662],
                  [ 0.0426,  2.7867,  2.3261,  ..., -2.4527, -3.1097, -0.1767],
                  [ 0.9949,  0.8221,  2.1220,  ...,  0.9519,  0.2675, -1.2333]],
        
                 [[ 1.4859,  1.6941,  1.1975,  ..., -0.4795, -0.0805, -0.5503],
                  [-0.7114,  1.3918,  1.0382,  ...,  0.8492, -0.1340, -2.7277],
                  [-2.5990,  0.0690,  1.3065,  ...,  0.7695, -0.1505,  0.5618],
                  ...,
                  [ 0.6071,  0.6872,  1.0442,  ...,  0.6443, -0.0131,  0.7085],
                  [-2.7211,  0.2247,  1.4769,  ..., -1.2173, -1.4020,  0.7598],
                  [ 0.3562,  0.9152,  1.2578,  ...,  0.0775,  0.5086, -1.4401]],
        
                 ...,
        
                 [[ 0.4342, -0.0196,  0.7254,  ...,  1.2087,  0.9078,  0.4840],
                  [-2.4008, -0.1790,  0.8937,  ..., -0.1929,  0.0683,  0.0204],
                  [-3.0332, -0.5475,  1.0886,  ...,  0.5707,  1.3899,  0.8868],
                  ...,
                  [ 0.3103,  0.7728, -0.3897,  ...,  1.3435,  1.3636,  0.7046],
                  [-2.1904,  0.3110,  0.4803,  ...,  0.5611,  0.3131, -0.0674],
                  [-1.1805, -0.3424,  0.6639,  ...,  0.0951,  0.9235, -0.7154]],
        
                 [[ 3.5248,  3.6790,  3.1350,  ..., -2.0765, -2.8139, -3.5629],
                  [ 1.9080,  3.7540,  3.1247,  ...,  1.2883, -3.2482, -5.3389],
                  [-0.2693,  2.8896,  2.2241,  ...,  0.7428, -2.2699, -1.3738],
                  ...,
                  [ 1.8455,  2.6315,  2.5821,  ...,  0.9347, -2.9383,  0.6796],
                  [-0.6148,  2.8612,  2.5634,  ..., -2.9427, -2.7382,  0.4858],
                  [ 0.2511,  2.7839,  2.1011,  ..., -1.5470,  1.4541, -1.1836]],
        
                 [[ 2.8229,  3.3151,  1.8541,  ..., -1.4270, -1.7512, -3.8045],
                  [ 0.8376,  2.2058,  4.1382,  ...,  0.4517, -3.9220, -4.7886],
                  [-0.2909,  2.4804,  1.9853,  ...,  0.4840, -1.2333, -0.9911],
                  ...,
                  [ 2.2202,  2.3059,  1.5454,  ...,  0.6374, -2.4326,  0.4265],
                  [-0.3500,  3.3962,  1.6780,  ..., -2.9704, -2.2736,  0.5250],
                  [ 0.8357,  1.9073,  1.8608,  ...,  2.3757,  0.4869, -1.0527]]]]), '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0': tensor([[[ 3.8864,  2.6560,  3.0121,  ...,  1.4150, -1.3747,  0.5130],
                 [ 2.9682,  1.3912,  1.6830,  ...,  1.2557, -1.0860,  0.5815],
                 [-1.0341, -0.1668,  0.1541,  ...,  0.1303, -0.0785, -0.3616],
                 ...,
                 [-1.1155, -1.3151, -1.1665,  ...,  0.3689, -0.8365, -0.3330],
                 [ 3.7093,  2.3833,  2.7045,  ...,  1.2936, -1.3532,  0.7336],
                 [ 4.5051,  2.3447,  2.8511,  ...,  1.5855, -1.3272,  0.8068]]]), '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0': tensor([[[ 2.6173,  2.4923,  1.8669,  ...,  0.5285, -3.2315,  0.2703],
                 [ 2.3529,  1.0416,  1.9073,  ...,  0.9839, -2.8940,  1.2901],
                 [ 0.6215,  0.4753,  0.6065,  ...,  1.0875, -0.6423,  0.5404],
                 ...,
                 [-0.1215, -0.7094,  0.4629,  ...,  1.1607, -0.8799,  0.4742],
                 [ 2.4958,  2.7087,  1.6094,  ...,  1.0292, -2.2524,  0.7039],
                 [ 2.8415,  2.3623,  1.8292,  ...,  1.1610, -2.3385,  1.4377]]]), '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0': tensor([[[ 3.9151,  4.2549,  3.1362,  ...,  0.1553,  0.9190, -0.7568],
                 [ 2.7675,  2.5564,  2.1182,  ...,  0.9519,  0.2675, -1.2333],
                 [ 1.4859,  1.6941,  1.1975,  ...,  0.0775,  0.5086, -1.4401],
                 ...,
                 [ 0.4342, -0.0196,  0.7254,  ...,  0.0951,  0.9235, -0.7154],
                 [ 3.5248,  3.6790,  3.1350,  ..., -1.5470,  1.4541, -1.1836],
                 [ 2.8229,  3.3151,  1.8541,  ...,  2.3757,  0.4869, -1.0527]]]), '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0': tensor([[[-0.0296,  0.5031,  1.3804,  ..., -1.4008,  2.7708, -5.5250],
                 [-0.0953,  0.4430,  1.3873,  ..., -3.0156,  6.4843, -5.8382],
                 [ 0.8470, -0.4361,  1.5967,  ..., -2.5812,  4.8259, -2.7178],
                 ...,
                 [ 1.2395,  0.4135,  2.0213,  ..., -1.6325,  1.9428, -2.1817],
                 [ 0.6077,  0.1026,  1.4831,  ..., -2.4743,  6.6096, -4.1296],
                 [ 0.6205,  0.2294,  1.4172,  ..., -2.6687,  6.3413, -5.3056]]]), '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0': tensor([[[ 0.6574,  1.0883,  0.8281,  ..., -1.8779,  2.2116, -3.0860],
                 [ 2.1510,  1.7804,  1.6356,  ..., -0.4797,  2.5973, -2.3041],
                 [ 2.5166,  1.3435,  2.0963,  ..., -1.4547,  1.1467, -1.8441],
                 ...,
                 [ 3.3671,  2.3563,  3.2236,  ..., -0.8749,  1.1280, -1.8341],
                 [ 1.4733,  0.6677,  1.5430,  ..., -1.6876,  2.8583, -3.4443],
                 [ 2.0375,  1.7558,  1.9177,  ..., -0.7307,  3.4448, -1.8215]]]), '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0': tensor([[[-0.4735,  0.0336, -0.1407,  ..., -1.7204,  4.3141, -4.5292],
                 [-1.5946,  0.1056, -0.2197,  ..., -3.4163,  3.0090, -4.3650],
                 [-0.5148, -1.6492,  0.3677,  ..., -2.4444,  3.6438, -3.7387],
                 ...,
                 [-0.5150, -0.6700,  0.4484,  ..., -2.2795,  3.0152, -4.1140],
                 [-0.6301, -0.8608,  0.4201,  ..., -1.7217,  3.7461, -4.2863],
                 [ 0.1090,  0.9420,  1.4409,  ..., -2.2724,  3.3272, -3.5136]]]), 'labels': tensor([[ 0, 67,  8,  8, 26, 67,  8,  0,  0,  0, 28,  8,  8,  0,  8,  8,  0,  0,
                 67,  8,  8,  8,  8,  8,  8,  8,  8,  0, 24,  8, 67, 24, 13,  8,  0,  8,
                  8, 67,  0,  8,  8,  8, 67, 26,  8,  0,  0, 26,  8,  0, 67,  8,  8, 26,
                  0,  0,  8,  0,  8, 13,  8,  8,  8, 28, 13,  8,  0, 28, 24,  8,  0, 26,
                 28, 26,  8, 24,  0, 26,  0,  8, 24,  0, 24,  0,  0, 67,  0, 67, 67,  8,
                 24,  0, 26, 67, 24,  0, 67, 79, 58,  8,  8, 24,  0,  0,  0,  8, 28,  8,
                  8, 56,  0, 58, 28, 67, 67,  0, 76,  0, 26, 24, 73,  8, 60, 24, 28,  8,
                 24,  8, 13,  0,  0, 24,  0,  8, 28, 24, 11,  8, 43,  2, 26,  0, 26,  8,
                  8, 28,  8,  8, 24,  0, 24, 28, 24,  2, 24, 26, 26,  0, 26, 28, 39, 26,
                 26, 13, 56, 24,  8,  0, 26, 13, 67, 24,  8, 24, 24, 26,  8, 28,  0, 28,
                  2, 24, 24, 13,  8, 56, 67, 26, 24,  0, 26,  0, 76, 24,  9,  8,  8, 34,
                  0, 58, 67, 28, 13, 24, 13,  8,  8, 24, 56, 79, 56, 24,  0, 12, 25, 28,
                 43, 26,  3,  0, 27, 58, 74,  3,  8,  8,  2, 26, 58,  0,  8, 74, 60,  9,
                  0, 24,  0, 26, 24, 24,  0, 11,  8,  8,  0, 13,  0,  0,  0,  1, 79,  8,
                 28, 24,  0,  1,  2, 59,  0, 58, 24, 28,  0,  9, 26, 24,  8, 26, 79,  8,
                 26, 26, 24, 24,  8, 24, 13, 67,  2,  8,  0, 26,  0, 13, 26, 24,  0, 58,
                  0,  8, 67,  8, 11, 25,  2,  0, 56,  2, 28, 24]]), 'boxes': tensor([[[262.9576,  71.5327, 547.7870, 476.1678],
                 [361.2971, 137.0405, 373.3970, 169.7942],
                 [ -1.0678, 216.7084,  27.7456, 234.6881],
                 ...,
                 [100.0009, 178.9919, 123.2679, 196.2153],
                 [175.3711, 198.3185, 231.4542, 280.3434],
                 [368.4505, 398.3175, 518.1219, 478.9788]]]), 'scores': tensor([[0.6030, 0.5362, 0.5060, 0.4716, 0.4550, 0.4172, 0.4056, 0.3967, 0.3598,
                 0.3229, 0.2707, 0.2680, 0.2555, 0.2454, 0.2324, 0.2316, 0.2067, 0.2027,
                 0.2022, 0.2010, 0.1952, 0.1934, 0.1921, 0.1915, 0.1900, 0.1865, 0.1848,
                 0.1837, 0.1807, 0.1792, 0.1760, 0.1732, 0.1698, 0.1648, 0.1636, 0.1600,
                 0.1595, 0.1585, 0.1555, 0.1547, 0.1529, 0.1523, 0.1518, 0.1502, 0.1478,
                 0.1432, 0.1393, 0.1369, 0.1360, 0.1356, 0.1341, 0.1318, 0.1289, 0.1285,
                 0.1260, 0.1236, 0.1187, 0.1185, 0.1182, 0.1180, 0.1179, 0.1175, 0.1174,
                 0.1169, 0.1167, 0.1138, 0.1123, 0.1119, 0.1114, 0.1112, 0.1103, 0.1103,
                 0.1102, 0.1101, 0.1092, 0.1092, 0.1083, 0.1081, 0.1059, 0.1050, 0.1040,
                 0.1031, 0.1025, 0.1021, 0.1009, 0.0988, 0.0972, 0.0961, 0.0952, 0.0948,
                 0.0943, 0.0942, 0.0942, 0.0942, 0.0940, 0.0934, 0.0934, 0.0932, 0.0930,
                 0.0925, 0.0924, 0.0922, 0.0921, 0.0915, 0.0913, 0.0910, 0.0907, 0.0899,
                 0.0898, 0.0883, 0.0872, 0.0868, 0.0864, 0.0864, 0.0859, 0.0849, 0.0835,
                 0.0833, 0.0830, 0.0819, 0.0814, 0.0810, 0.0804, 0.0800, 0.0790, 0.0788,
                 0.0786, 0.0780, 0.0779, 0.0776, 0.0776, 0.0773, 0.0771, 0.0770, 0.0766,
                 0.0765, 0.0760, 0.0759, 0.0752, 0.0751, 0.0749, 0.0748, 0.0744, 0.0742,
                 0.0740, 0.0736, 0.0732, 0.0732, 0.0731, 0.0730, 0.0722, 0.0718, 0.0717,
                 0.0717, 0.0710, 0.0706, 0.0705, 0.0693, 0.0692, 0.0691, 0.0686, 0.0684,
                 0.0680, 0.0678, 0.0677, 0.0677, 0.0672, 0.0670, 0.0656, 0.0656, 0.0655,
                 0.0655, 0.0654, 0.0654, 0.0652, 0.0645, 0.0640, 0.0639, 0.0638, 0.0631,
                 0.0628, 0.0626, 0.0619, 0.0617, 0.0612, 0.0612, 0.0611, 0.0610, 0.0609,
                 0.0606, 0.0606, 0.0604, 0.0604, 0.0604, 0.0604, 0.0601, 0.0597, 0.0597,
                 0.0597, 0.0596, 0.0595, 0.0593, 0.0589, 0.0586, 0.0586, 0.0586, 0.0585,
                 0.0578, 0.0574, 0.0573, 0.0573, 0.0571, 0.0570, 0.0568, 0.0567, 0.0562,
                 0.0557, 0.0556, 0.0555, 0.0555, 0.0555, 0.0544, 0.0542, 0.0535, 0.0533,
                 0.0533, 0.0531, 0.0530, 0.0527, 0.0525, 0.0522, 0.0521, 0.0519, 0.0518,
                 0.0517, 0.0515, 0.0514, 0.0512, 0.0507, 0.0507, 0.0507, 0.0505, 0.0504,
                 0.0503, 0.0502, 0.0502, 0.0499, 0.0496, 0.0495, 0.0487, 0.0487, 0.0485,
                 0.0484, 0.0482, 0.0480, 0.0480, 0.0480, 0.0476, 0.0472, 0.0467, 0.0466,
                 0.0462, 0.0460, 0.0460, 0.0460, 0.0459, 0.0456, 0.0455, 0.0455, 0.0455,
                 0.0454, 0.0451, 0.0451, 0.0445, 0.0444, 0.0444, 0.0443, 0.0441, 0.0441,
                 0.0440, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437, 0.0436, 0.0435,
                 0.0432, 0.0430, 0.0429, 0.0426, 0.0426, 0.0425, 0.0423, 0.0419, 0.0419,
                 0.0419, 0.0417, 0.0417]])}[0m
[38;5;10m[I] onnxrt-runner-N2-05/19/25-15:43:09  | Completed 1 iteration(s) in 138.5 ms | Average inference time: 138.5 ms.[0m
[38;5;13m[V] Successfully ran: ['trt-runner-N2-05/19/25-15:43:09', 'onnxrt-runner-N2-05/19/25-15:43:09'][0m
[38;5;14m[I] Accuracy Comparison | trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) with '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 0.1683,  0.7307,  1.4870,  ..., -1.3125,  5.5169, -2.7646])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([-0.0296,  0.5031,  1.3804,  ..., -2.6687,  6.3413, -5.3056])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 | Stats: mean=0.1925, std-dev=2.0345, var=4.139, median=0.18268, min=-8.1297 at (0, 118, 7, 11, 1), max=7.9326 at (0, 60, 7, 11, 0), avg-magnitude=1.6522, p90=2.8262, p95=3.6648, p99=4.3738
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-8.13  , -6.52  ) |       18.0 | 
                (-6.52  , -4.92  ) |      162.0 | 
                (-4.92  , -3.31  ) |     2321.0 | #####
                (-3.31  , -1.7   ) |     8243.0 | ###################
                (-1.7   , -0.0986) |    14792.0 | ###################################
                (-0.0986, 1.51   ) |    16737.0 | ########################################
                (1.51   , 3.11   ) |    10622.0 | #########################
                (3.11   , 4.72   ) |     4309.0 | ##########
                (4.72   , 6.33   ) |      272.0 | 
                (6.33   , 7.93   ) |      124.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 | Stats: mean=0.15685, std-dev=2.0738, var=4.3007, median=0.14499, min=-7.8389 at (0, 123, 4, 0, 0), max=7.7039 at (0, 215, 7, 11, 0), avg-magnitude=1.6873, p90=2.8771, p95=3.6862, p99=4.361
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-8.13  , -6.52  ) |       25.0 | 
                (-6.52  , -4.92  ) |      190.0 | 
                (-4.92  , -3.31  ) |     2528.0 | ######
                (-3.31  , -1.7   ) |     8706.0 | #####################
                (-1.7   , -0.0986) |    14590.0 | ###################################
                (-0.0986, 1.51   ) |    16308.0 | ########################################
                (1.51   , 3.11   ) |    10312.0 | #########################
                (3.11   , 4.72   ) |     4556.0 | ###########
                (4.72   , 6.33   ) |      259.0 | 
                (6.33   , 7.93   ) |      126.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=7.7626] OR [rel=1.1769e+05] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.62783, std-dev=0.68223, var=0.46544, median=0.40287, min=2.3842e-07 at (0, 5, 6, 2, 1), max=7.7626 at (0, 123, 1, 0, 0), avg-magnitude=0.62783, p90=1.4997, p95=1.9867, p99=3.2191
[I]                 ---- Histogram ----
                    Bin Range         |  Num Elems | Visualization
                    (2.38e-07, 0.776) |    41598.0 | ########################################
                    (0.776   , 1.55 ) |    10659.0 | ##########
                    (1.55    , 2.33 ) |     3557.0 | ###
                    (2.33    , 3.11 ) |     1128.0 | #
                    (3.11    , 3.88 ) |      413.0 | 
                    (3.88    , 4.66 ) |      158.0 | 
                    (4.66    , 5.43 ) |       56.0 | 
                    (5.43    , 6.21 ) |       23.0 | 
                    (6.21    , 6.99 ) |        4.0 | 
                    (6.99    , 7.76 ) |        4.0 | 
[I]             Relative Difference | Stats: mean=4.5228, std-dev=493.41, var=2.4345e+05, median=0.32636, min=6.1956e-08 at (0, 15, 2, 1, 1), max=1.1769e+05 at (0, 120, 4, 4, 0), avg-magnitude=4.5228, p90=2.1291, p95=4.2775, p99=22.582
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (6.2e-08 , 1.18e+04) |    57599.0 | ########################################
                    (1.18e+04, 2.35e+04) |        0.0 | 
                    (2.35e+04, 3.53e+04) |        0.0 | 
                    (3.53e+04, 4.71e+04) |        0.0 | 
                    (4.71e+04, 5.88e+04) |        0.0 | 
                    (5.88e+04, 7.06e+04) |        0.0 | 
                    (7.06e+04, 8.24e+04) |        0.0 | 
                    (8.24e+04, 9.42e+04) |        0.0 | 
                    (9.42e+04, 1.06e+05) |        0.0 | 
                    (1.06e+05, 1.18e+05) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) with '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 0.5720,  1.4100,  0.9941,  ..., -1.3169,  1.6684, -1.7301])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 0.6574,  1.0883,  0.8281,  ..., -0.7307,  3.4448, -1.8215])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 | Stats: mean=0.077656, std-dev=2.5473, var=6.4888, median=0.15283, min=-35.302 at (0, 67, 7, 7, 1), max=9.1329 at (0, 46, 1, 11, 1), avg-magnitude=1.826, p90=3.0505, p95=3.6218, p99=4.8338
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-35.3, -30.8) |        8.0 | 
                (-30.8, -26.2) |       26.0 | 
                (-26.2, -21.7) |       40.0 | 
                (-21.7, -17.1) |       79.0 | 
                (-17.1, -12.6) |       88.0 | 
                (-12.6, -8.04) |       30.0 | 
                (-8.04, -3.49) |     2608.0 | ##
                (-3.49, 1.05 ) |    35095.0 | ########################################
                (1.05 , 5.59 ) |    19224.0 | #####################
                (5.59 , 10.1 ) |      402.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 | Stats: mean=0.062898, std-dev=2.5249, var=6.3749, median=0.12922, min=-33.608 at (0, 49, 7, 7, 1), max=10.137 at (0, 272, 1, 11, 1), avg-magnitude=1.8338, p90=3.0449, p95=3.621, p99=4.9222
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-35.3, -30.8) |        7.0 | 
                (-30.8, -26.2) |       19.0 | 
                (-26.2, -21.7) |       43.0 | 
                (-21.7, -17.1) |       70.0 | 
                (-17.1, -12.6) |       81.0 | 
                (-12.6, -8.04) |       49.0 | 
                (-8.04, -3.49) |     2568.0 | ##
                (-3.49, 1.05 ) |    35205.0 | ########################################
                (1.05 , 5.59 ) |    19152.0 | #####################
                (5.59 , 10.1 ) |      406.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=26.6] OR [rel=1.9526e+05] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.78773, std-dev=0.97639, var=0.95334, median=0.54182, min=5.4002e-05 at (0, 0, 7, 5, 0), max=26.6 at (0, 91, 7, 7, 1), avg-magnitude=0.78773, p90=1.7296, p95=2.2378, p99=3.5586
[I]                 ---- Histogram ----
                    Bin Range       |  Num Elems | Visualization
                    (5.4e-05, 2.66) |    55974.0 | ########################################
                    (2.66   , 5.32) |     1414.0 | #
                    (5.32   , 7.98) |       92.0 | 
                    (7.98   , 10.6) |       37.0 | 
                    (10.6   , 13.3) |       32.0 | 
                    (13.3   , 16  ) |       20.0 | 
                    (16     , 18.6) |       12.0 | 
                    (18.6   , 21.3) |        8.0 | 
                    (21.3   , 23.9) |        4.0 | 
                    (23.9   , 26.6) |        7.0 | 
[I]             Relative Difference | Stats: mean=7.5528, std-dev=854.09, var=7.2946e+05, median=0.40182, min=3.3131e-05 at (0, 12, 1, 6, 0), max=1.9526e+05 at (0, 149, 4, 9, 0), avg-magnitude=7.5528, p90=2.7934, p95=5.61, p99=27.564
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (3.31e-05, 1.95e+04) |    57598.0 | ########################################
                    (1.95e+04, 3.91e+04) |        0.0 | 
                    (3.91e+04, 5.86e+04) |        0.0 | 
                    (5.86e+04, 7.81e+04) |        1.0 | 
                    (7.81e+04, 9.76e+04) |        0.0 | 
                    (9.76e+04, 1.17e+05) |        0.0 | 
                    (1.17e+05, 1.37e+05) |        0.0 | 
                    (1.37e+05, 1.56e+05) |        0.0 | 
                    (1.56e+05, 1.76e+05) |        0.0 | 
                    (1.76e+05, 1.95e+05) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) with '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([-2.4460e-01,  5.9259e-02, -1.1141e-03,  ..., -1.8162e+00,
                         4.1330e+00, -4.0583e+00])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([-0.4735,  0.0336, -0.1407,  ..., -2.2724,  3.3272, -3.5136])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 | Stats: mean=0.15435, std-dev=2.2185, var=4.9217, median=0.23945, min=-9.3616 at (0, 258, 7, 7, 1), max=8.4497 at (0, 45, 4, 4, 0), avg-magnitude=1.7862, p90=3.0584, p95=3.8427, p99=4.5012
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.36 , -7.58 ) |       95.0 | 
                (-7.58 , -5.8  ) |      178.0 | 
                (-5.8  , -4.02 ) |      700.0 | #
                (-4.02 , -2.24 ) |     7964.0 | ################
                (-2.24 , -0.456) |    12551.0 | ##########################
                (-0.456, 1.33  ) |    18766.0 | ########################################
                (1.33  , 3.11  ) |    11826.0 | #########################
                (3.11  , 4.89  ) |     5194.0 | ###########
                (4.89  , 6.67  ) |      309.0 | 
                (6.67  , 8.45  ) |       17.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 | Stats: mean=0.11493, std-dev=2.2279, var=4.9633, median=0.1752, min=-9.3242 at (0, 208, 7, 7, 1), max=7.8344 at (0, 100, 4, 4, 0), avg-magnitude=1.7968, p90=3.0641, p95=3.8392, p99=4.463
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.36 , -7.58 ) |      104.0 | 
                (-7.58 , -5.8  ) |      150.0 | 
                (-5.8  , -4.02 ) |      703.0 | #
                (-4.02 , -2.24 ) |     8295.0 | ##################
                (-2.24 , -0.456) |    12926.0 | ############################
                (-0.456, 1.33  ) |    18268.0 | ########################################
                (1.33  , 3.11  ) |    11576.0 | #########################
                (3.11  , 4.89  ) |     5266.0 | ###########
                (4.89  , 6.67  ) |      295.0 | 
                (6.67  , 8.45  ) |       17.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=9.5232] OR [rel=62685] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.77918, std-dev=0.73952, var=0.5469, median=0.56411, min=0 at (0, 212, 2, 7, 1), max=9.5232 at (0, 137, 7, 8, 0), avg-magnitude=0.77918, p90=1.7818, p95=2.2654, p99=3.31
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (0    , 0.952) |    39845.0 | ########################################
                    (0.952, 1.9  ) |    12889.0 | ############
                    (1.9  , 2.86 ) |     3697.0 | ###
                    (2.86 , 3.81 ) |      944.0 | 
                    (3.81 , 4.76 ) |      186.0 | 
                    (4.76 , 5.71 ) |       28.0 | 
                    (5.71 , 6.67 ) |        6.0 | 
                    (6.67 , 7.62 ) |        2.0 | 
                    (7.62 , 8.57 ) |        1.0 | 
                    (8.57 , 9.52 ) |        2.0 | 
[I]             Relative Difference | Stats: mean=5.0311, std-dev=321.37, var=1.0328e+05, median=0.42494, min=0 at (0, 212, 2, 7, 1), max=62685 at (0, 195, 4, 3, 1), avg-magnitude=5.0311, p90=3.173, p95=6.3615, p99=33.115
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0       , 6.27e+03) |    57598.0 | ########################################
                    (6.27e+03, 1.25e+04) |        0.0 | 
                    (1.25e+04, 1.88e+04) |        0.0 | 
                    (1.88e+04, 2.51e+04) |        0.0 | 
                    (2.51e+04, 3.13e+04) |        0.0 | 
                    (3.13e+04, 3.76e+04) |        0.0 | 
                    (3.76e+04, 4.39e+04) |        1.0 | 
                    (4.39e+04, 5.01e+04) |        0.0 | 
                    (5.01e+04, 5.64e+04) |        0.0 | 
                    (5.64e+04, 6.27e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12, 2])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) with '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 4.2611,  2.5208,  3.0048,  ...,  1.0716, -1.0450,  0.3493])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 3.8864,  2.6560,  3.0121,  ...,  1.5855, -1.3272,  0.8068])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 | Stats: mean=0.18873, std-dev=1.9092, var=3.6451, median=0.44864, min=-6.3041 at (0, 131, 6, 10), max=6.304 at (0, 25, 0, 0), avg-magnitude=1.5184, p90=2.5213, p95=3.047, p99=3.8394
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.69 , -5.33 ) |       52.0 | 
                (-5.33 , -3.98 ) |      624.0 | ##
                (-3.98 , -2.62 ) |     2374.0 | #########
                (-2.62 , -1.27 ) |     3199.0 | ############
                (-1.27 , 0.0886) |     5197.0 | ###################
                (0.0886, 1.44  ) |    10457.0 | ########################################
                (1.44  , 2.8   ) |     4851.0 | ##################
                (2.8   , 4.15  ) |     1893.0 | #######
                (4.15  , 5.51  ) |      129.0 | 
                (5.51  , 6.86  ) |       24.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 | Stats: mean=0.1808, std-dev=1.8751, var=3.5161, median=0.42876, min=-6.6851 at (0, 123, 6, 10), max=6.8622 at (0, 170, 0, 0), avg-magnitude=1.4761, p90=2.4666, p95=3.0216, p99=3.855
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.69 , -5.33 ) |       61.0 | 
                (-5.33 , -3.98 ) |      580.0 | ##
                (-3.98 , -2.62 ) |     2320.0 | ########
                (-2.62 , -1.27 ) |     3046.0 | ###########
                (-1.27 , 0.0886) |     5462.0 | ####################
                (0.0886, 1.44  ) |    10882.0 | ########################################
                (1.44  , 2.8   ) |     4514.0 | ################
                (2.8   , 4.15  ) |     1781.0 | ######
                (4.15  , 5.51  ) |      124.0 | 
                (5.51  , 6.86  ) |       30.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=8.7196] OR [rel=15913] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.9372, std-dev=1.0493, var=1.1011, median=0.54716, min=6.6996e-05 at (0, 148, 1, 2), max=8.7196 at (0, 11, 0, 0), avg-magnitude=0.9372, p90=2.4148, p95=3.2659, p99=4.6964
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (6.7e-05, 0.872) |    18743.0 | ########################################
                    (0.872  , 1.74 ) |     5252.0 | ###########
                    (1.74   , 2.62 ) |     2326.0 | ####
                    (2.62   , 3.49 ) |     1299.0 | ##
                    (3.49   , 4.36 ) |      737.0 | #
                    (4.36   , 5.23 ) |      294.0 | 
                    (5.23   , 6.1  ) |      106.0 | 
                    (6.1    , 6.98 ) |       34.0 | 
                    (6.98   , 7.85 ) |        7.0 | 
                    (7.85   , 8.72 ) |        2.0 | 
[I]             Relative Difference | Stats: mean=4.3792, std-dev=119.7, var=14328, median=0.51118, min=2.5296e-05 at (0, 148, 1, 2), max=15913 at (0, 76, 1, 2), avg-magnitude=4.3792, p90=4.216, p95=8.5001, p99=43.137
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (2.53e-05, 1.59e+03) |    28795.0 | ########################################
                    (1.59e+03, 3.18e+03) |        3.0 | 
                    (3.18e+03, 4.77e+03) |        0.0 | 
                    (4.77e+03, 6.37e+03) |        0.0 | 
                    (6.37e+03, 7.96e+03) |        0.0 | 
                    (7.96e+03, 9.55e+03) |        0.0 | 
                    (9.55e+03, 1.11e+04) |        0.0 | 
                    (1.11e+04, 1.27e+04) |        1.0 | 
                    (1.27e+04, 1.43e+04) |        0.0 | 
                    (1.43e+04, 1.59e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) with '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 2.6295,  2.5487,  1.8629,  ...,  1.8816, -2.1333,  0.3129])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 2.6173,  2.4923,  1.8669,  ...,  1.1610, -2.3385,  1.4377])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 | Stats: mean=-0.020501, std-dev=1.9162, var=3.672, median=0.2974, min=-6.434 at (0, 56, 4, 11), max=5.0485 at (0, 160, 6, 1), avg-magnitude=1.524, p90=2.2583, p95=2.7651, p99=3.6775
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.75 , -5.56 ) |       44.0 | 
                (-5.56 , -4.37 ) |      602.0 | ##
                (-4.37 , -3.18 ) |     1615.0 | #######
                (-3.18 , -1.98 ) |     2577.0 | ############
                (-1.98 , -0.792) |     3804.0 | ##################
                (-0.792, 0.4   ) |     6455.0 | ###############################
                (0.4   , 1.59  ) |     8256.0 | ########################################
                (1.59  , 2.78  ) |     4044.0 | ###################
                (2.78  , 3.98  ) |     1267.0 | ######
                (3.98  , 5.17  ) |      136.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 | Stats: mean=-0.01428, std-dev=1.8962, var=3.5957, median=0.30371, min=-6.7511 at (0, 50, 4, 11), max=5.1675 at (0, 222, 6, 1), avg-magnitude=1.5028, p90=2.2252, p95=2.7625, p99=3.6673
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.75 , -5.56 ) |       51.0 | 
                (-5.56 , -4.37 ) |      560.0 | ##
                (-4.37 , -3.18 ) |     1598.0 | #######
                (-3.18 , -1.98 ) |     2492.0 | ###########
                (-1.98 , -0.792) |     3843.0 | ##################
                (-0.792, 0.4   ) |     6583.0 | ###############################
                (0.4   , 1.59  ) |     8407.0 | ########################################
                (1.59  , 2.78  ) |     3876.0 | ##################
                (2.78  , 3.98  ) |     1244.0 | #####
                (3.98  , 5.17  ) |      146.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=7.8445] OR [rel=15575] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.98457, std-dev=0.95025, var=0.90297, median=0.68537, min=7.7605e-05 at (0, 1, 5, 0), max=7.8445 at (0, 11, 6, 10), avg-magnitude=0.98457, p90=2.3045, p95=2.9352, p99=4.336
[I]                 ---- Histogram ----
                    Bin Range         |  Num Elems | Visualization
                    (7.76e-05, 0.785) |    15806.0 | ########################################
                    (0.785   , 1.57 ) |     7100.0 | #################
                    (1.57    , 2.35 ) |     3163.0 | ########
                    (2.35    , 3.14 ) |     1575.0 | ###
                    (3.14    , 3.92 ) |      683.0 | #
                    (3.92    , 4.71 ) |      305.0 | 
                    (4.71    , 5.49 ) |      125.0 | 
                    (5.49    , 6.28 ) |       33.0 | 
                    (6.28    , 7.06 ) |        7.0 | 
                    (7.06    , 7.84 ) |        3.0 | 
[I]             Relative Difference | Stats: mean=4.2658, std-dev=105.49, var=11128, median=0.57877, min=4.3134e-05 at (0, 287, 4, 8), max=15575 at (0, 188, 3, 6), avg-magnitude=4.2658, p90=4.0217, p95=8.2541, p99=37.863
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (4.31e-05, 1.56e+03) |    28791.0 | ########################################
                    (1.56e+03, 3.11e+03) |        6.0 | 
                    (3.11e+03, 4.67e+03) |        2.0 | 
                    (4.67e+03, 6.23e+03) |        0.0 | 
                    (6.23e+03, 7.79e+03) |        0.0 | 
                    (7.79e+03, 9.34e+03) |        0.0 | 
                    (9.34e+03, 1.09e+04) |        0.0 | 
                    (1.09e+04, 1.25e+04) |        0.0 | 
                    (1.25e+04, 1.4e+04 ) |        0.0 | 
                    (1.4e+04 , 1.56e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) with '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 4.0432,  4.0170,  3.3249,  ...,  0.1316,  0.8250, -0.2724])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 3.9151,  4.2549,  3.1362,  ...,  2.3757,  0.4869, -1.0527])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 | Stats: mean=0.30826, std-dev=1.7842, var=3.1835, median=0.4835, min=-6.7018 at (0, 149, 1, 11), max=7.4945 at (0, 32, 7, 8), avg-magnitude=1.4494, p90=2.5044, p95=3.0792, p99=4.0189
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-7.23  , -5.42  ) |       80.0 | 
                (-5.42  , -3.61  ) |      520.0 | #
                (-3.61  , -1.8   ) |     3193.0 | ##########
                (-1.8   , 0.00667) |     7345.0 | ########################
                (0.00667, 1.82   ) |    12131.0 | ########################################
                (1.82   , 3.63   ) |     4889.0 | ################
                (3.63   , 5.44   ) |      634.0 | ##
                (5.44   , 7.25   ) |        5.0 | 
                (7.25   , 9.06   ) |        3.0 | 
                (9.06   , 10.9   ) |        0.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 | Stats: mean=0.29836, std-dev=1.7936, var=3.2169, median=0.486, min=-7.2331 at (0, 151, 1, 11), max=10.866 at (0, 104, 7, 9), avg-magnitude=1.4456, p90=2.4812, p95=3.0639, p99=4.11
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-7.23  , -5.42  ) |       90.0 | 
                (-5.42  , -3.61  ) |      540.0 | #
                (-3.61  , -1.8   ) |     3218.0 | ##########
                (-1.8   , 0.00667) |     7181.0 | #######################
                (0.00667, 1.82   ) |    12469.0 | ########################################
                (1.82   , 3.63   ) |     4614.0 | ##############
                (3.63   , 5.44   ) |      671.0 | ##
                (5.44   , 7.25   ) |        9.0 | 
                (7.25   , 9.06   ) |        6.0 | 
                (9.06   , 10.9   ) |        2.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=11.265] OR [rel=2408.1] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=1.0334, std-dev=0.99135, var=0.98277, median=0.73552, min=8.9765e-05 at (0, 202, 6, 7), max=11.265 at (0, 8, 7, 8), avg-magnitude=1.0334, p90=2.3557, p95=3.0789, p99=4.4731
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (8.98e-05, 1.13) |    19141.0 | ########################################
                    (1.13    , 2.25) |     6451.0 | #############
                    (2.25    , 3.38) |     2166.0 | ####
                    (3.38    , 4.51) |      770.0 | #
                    (4.51    , 5.63) |      192.0 | 
                    (5.63    , 6.76) |       54.0 | 
                    (6.76    , 7.89) |       13.0 | 
                    (7.89    , 9.01) |        8.0 | 
                    (9.01    , 10.1) |        3.0 | 
                    (10.1    , 11.3) |        2.0 | 
[I]             Relative Difference | Stats: mean=3.2835, std-dev=28.337, var=802.97, median=0.64228, min=7.8131e-05 at (0, 172, 7, 2), max=2408.1 at (0, 54, 5, 10), avg-magnitude=3.2835, p90=4.0706, p95=8.4256, p99=41.56
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (7.81e-05, 241     ) |    28759.0 | ########################################
                    (241     , 482     ) |       26.0 | 
                    (482     , 722     ) |        7.0 | 
                    (722     , 963     ) |        2.0 | 
                    (963     , 1.2e+03 ) |        4.0 | 
                    (1.2e+03 , 1.44e+03) |        0.0 | 
                    (1.44e+03, 1.69e+03) |        0.0 | 
                    (1.69e+03, 1.93e+03) |        1.0 | 
                    (1.93e+03, 2.17e+03) |        0.0 | 
                    (2.17e+03, 2.41e+03) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' (dtype=float32, shape=torch.Size([1, 300, 8, 12])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) with '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 4.2611,  2.5208,  3.0048,  ...,  1.0716, -1.0450,  0.3493])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 3.8864,  2.6560,  3.0121,  ...,  1.5855, -1.3272,  0.8068])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 | Stats: mean=0.18873, std-dev=1.9092, var=3.6451, median=0.44864, min=-6.3041 at (0, 131, 82), max=6.304 at (0, 25, 0), avg-magnitude=1.5184, p90=2.5213, p95=3.047, p99=3.8394
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.69 , -5.33 ) |       52.0 | 
                (-5.33 , -3.98 ) |      624.0 | ##
                (-3.98 , -2.62 ) |     2374.0 | #########
                (-2.62 , -1.27 ) |     3199.0 | ############
                (-1.27 , 0.0886) |     5197.0 | ###################
                (0.0886, 1.44  ) |    10457.0 | ########################################
                (1.44  , 2.8   ) |     4851.0 | ##################
                (2.8   , 4.15  ) |     1893.0 | #######
                (4.15  , 5.51  ) |      129.0 | 
                (5.51  , 6.86  ) |       24.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 | Stats: mean=0.1808, std-dev=1.8751, var=3.5161, median=0.42876, min=-6.6851 at (0, 123, 82), max=6.8622 at (0, 170, 0), avg-magnitude=1.4761, p90=2.4666, p95=3.0216, p99=3.855
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.69 , -5.33 ) |       61.0 | 
                (-5.33 , -3.98 ) |      580.0 | ##
                (-3.98 , -2.62 ) |     2320.0 | ########
                (-2.62 , -1.27 ) |     3046.0 | ###########
                (-1.27 , 0.0886) |     5462.0 | ####################
                (0.0886, 1.44  ) |    10882.0 | ########################################
                (1.44  , 2.8   ) |     4514.0 | ################
                (2.8   , 4.15  ) |     1781.0 | ######
                (4.15  , 5.51  ) |      124.0 | 
                (5.51  , 6.86  ) |       30.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=8.7196] OR [rel=15913] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.9372, std-dev=1.0493, var=1.1011, median=0.54716, min=6.6996e-05 at (0, 148, 14), max=8.7196 at (0, 11, 0), avg-magnitude=0.9372, p90=2.4148, p95=3.2659, p99=4.6964
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (6.7e-05, 0.872) |    18743.0 | ########################################
                    (0.872  , 1.74 ) |     5252.0 | ###########
                    (1.74   , 2.62 ) |     2326.0 | ####
                    (2.62   , 3.49 ) |     1299.0 | ##
                    (3.49   , 4.36 ) |      737.0 | #
                    (4.36   , 5.23 ) |      294.0 | 
                    (5.23   , 6.1  ) |      106.0 | 
                    (6.1    , 6.98 ) |       34.0 | 
                    (6.98   , 7.85 ) |        7.0 | 
                    (7.85   , 8.72 ) |        2.0 | 
[I]             Relative Difference | Stats: mean=4.3792, std-dev=119.7, var=14328, median=0.51118, min=2.5296e-05 at (0, 148, 14), max=15913 at (0, 76, 14), avg-magnitude=4.3792, p90=4.216, p95=8.5001, p99=43.137
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (2.53e-05, 1.59e+03) |    28795.0 | ########################################
                    (1.59e+03, 3.18e+03) |        3.0 | 
                    (3.18e+03, 4.77e+03) |        0.0 | 
                    (4.77e+03, 6.37e+03) |        0.0 | 
                    (6.37e+03, 7.96e+03) |        0.0 | 
                    (7.96e+03, 9.55e+03) |        0.0 | 
                    (9.55e+03, 1.11e+04) |        0.0 | 
                    (1.11e+04, 1.27e+04) |        1.0 | 
                    (1.27e+04, 1.43e+04) |        0.0 | 
                    (1.43e+04, 1.59e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) with '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 2.6295,  2.5487,  1.8629,  ...,  1.8816, -2.1333,  0.3129])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 2.6173,  2.4923,  1.8669,  ...,  1.1610, -2.3385,  1.4377])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 | Stats: mean=-0.020501, std-dev=1.9162, var=3.672, median=0.2974, min=-6.434 at (0, 56, 59), max=5.0485 at (0, 160, 73), avg-magnitude=1.524, p90=2.2583, p95=2.7651, p99=3.6775
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.75 , -5.56 ) |       44.0 | 
                (-5.56 , -4.37 ) |      602.0 | ##
                (-4.37 , -3.18 ) |     1615.0 | #######
                (-3.18 , -1.98 ) |     2577.0 | ############
                (-1.98 , -0.792) |     3804.0 | ##################
                (-0.792, 0.4   ) |     6455.0 | ###############################
                (0.4   , 1.59  ) |     8256.0 | ########################################
                (1.59  , 2.78  ) |     4044.0 | ###################
                (2.78  , 3.98  ) |     1267.0 | ######
                (3.98  , 5.17  ) |      136.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 | Stats: mean=-0.01428, std-dev=1.8962, var=3.5957, median=0.30371, min=-6.7511 at (0, 50, 59), max=5.1675 at (0, 222, 73), avg-magnitude=1.5028, p90=2.2252, p95=2.7625, p99=3.6673
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-6.75 , -5.56 ) |       51.0 | 
                (-5.56 , -4.37 ) |      560.0 | ##
                (-4.37 , -3.18 ) |     1598.0 | #######
                (-3.18 , -1.98 ) |     2492.0 | ###########
                (-1.98 , -0.792) |     3843.0 | ##################
                (-0.792, 0.4   ) |     6583.0 | ###############################
                (0.4   , 1.59  ) |     8407.0 | ########################################
                (1.59  , 2.78  ) |     3876.0 | ##################
                (2.78  , 3.98  ) |     1244.0 | #####
                (3.98  , 5.17  ) |      146.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=7.8445] OR [rel=15575] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.98457, std-dev=0.95025, var=0.90297, median=0.68537, min=7.7605e-05 at (0, 1, 60), max=7.8445 at (0, 11, 82), avg-magnitude=0.98457, p90=2.3045, p95=2.9352, p99=4.336
[I]                 ---- Histogram ----
                    Bin Range         |  Num Elems | Visualization
                    (7.76e-05, 0.785) |    15806.0 | ########################################
                    (0.785   , 1.57 ) |     7100.0 | #################
                    (1.57    , 2.35 ) |     3163.0 | ########
                    (2.35    , 3.14 ) |     1575.0 | ###
                    (3.14    , 3.92 ) |      683.0 | #
                    (3.92    , 4.71 ) |      305.0 | 
                    (4.71    , 5.49 ) |      125.0 | 
                    (5.49    , 6.28 ) |       33.0 | 
                    (6.28    , 7.06 ) |        7.0 | 
                    (7.06    , 7.84 ) |        3.0 | 
[I]             Relative Difference | Stats: mean=4.2658, std-dev=105.49, var=11128, median=0.57877, min=4.3134e-05 at (0, 287, 56), max=15575 at (0, 188, 42), avg-magnitude=4.2658, p90=4.0217, p95=8.2541, p99=37.863
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (4.31e-05, 1.56e+03) |    28791.0 | ########################################
                    (1.56e+03, 3.11e+03) |        6.0 | 
                    (3.11e+03, 4.67e+03) |        2.0 | 
                    (4.67e+03, 6.23e+03) |        0.0 | 
                    (6.23e+03, 7.79e+03) |        0.0 | 
                    (7.79e+03, 9.34e+03) |        0.0 | 
                    (9.34e+03, 1.09e+04) |        0.0 | 
                    (1.09e+04, 1.25e+04) |        0.0 | 
                    (1.25e+04, 1.4e+04 ) |        0.0 | 
                    (1.4e+04 , 1.56e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) with '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 4.0432,  4.0170,  3.3249,  ...,  0.1316,  0.8250, -0.2724])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 3.9151,  4.2549,  3.1362,  ...,  2.3757,  0.4869, -1.0527])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 | Stats: mean=0.30826, std-dev=1.7842, var=3.1835, median=0.4835, min=-6.7018 at (0, 149, 23), max=7.4945 at (0, 32, 92), avg-magnitude=1.4494, p90=2.5044, p95=3.0792, p99=4.0189
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-7.23  , -5.42  ) |       80.0 | 
                (-5.42  , -3.61  ) |      520.0 | #
                (-3.61  , -1.8   ) |     3193.0 | ##########
                (-1.8   , 0.00667) |     7345.0 | ########################
                (0.00667, 1.82   ) |    12131.0 | ########################################
                (1.82   , 3.63   ) |     4889.0 | ################
                (3.63   , 5.44   ) |      634.0 | ##
                (5.44   , 7.25   ) |        5.0 | 
                (7.25   , 9.06   ) |        3.0 | 
                (9.06   , 10.9   ) |        0.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 | Stats: mean=0.29836, std-dev=1.7936, var=3.2169, median=0.486, min=-7.2331 at (0, 151, 23), max=10.866 at (0, 104, 93), avg-magnitude=1.4456, p90=2.4812, p95=3.0639, p99=4.11
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-7.23  , -5.42  ) |       90.0 | 
                (-5.42  , -3.61  ) |      540.0 | #
                (-3.61  , -1.8   ) |     3218.0 | ##########
                (-1.8   , 0.00667) |     7181.0 | #######################
                (0.00667, 1.82   ) |    12469.0 | ########################################
                (1.82   , 3.63   ) |     4614.0 | ##############
                (3.63   , 5.44   ) |      671.0 | ##
                (5.44   , 7.25   ) |        9.0 | 
                (7.25   , 9.06   ) |        6.0 | 
                (9.06   , 10.9   ) |        2.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=11.265] OR [rel=2408.1] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=1.0334, std-dev=0.99135, var=0.98277, median=0.73552, min=8.9765e-05 at (0, 202, 79), max=11.265 at (0, 8, 92), avg-magnitude=1.0334, p90=2.3557, p95=3.0789, p99=4.4731
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (8.98e-05, 1.13) |    19141.0 | ########################################
                    (1.13    , 2.25) |     6451.0 | #############
                    (2.25    , 3.38) |     2166.0 | ####
                    (3.38    , 4.51) |      770.0 | #
                    (4.51    , 5.63) |      192.0 | 
                    (5.63    , 6.76) |       54.0 | 
                    (6.76    , 7.89) |       13.0 | 
                    (7.89    , 9.01) |        8.0 | 
                    (9.01    , 10.1) |        3.0 | 
                    (10.1    , 11.3) |        2.0 | 
[I]             Relative Difference | Stats: mean=3.2835, std-dev=28.337, var=802.97, median=0.64228, min=7.8131e-05 at (0, 172, 86), max=2408.1 at (0, 54, 70), avg-magnitude=3.2835, p90=4.0706, p95=8.4256, p99=41.56
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (7.81e-05, 241     ) |    28759.0 | ########################################
                    (241     , 482     ) |       26.0 | 
                    (482     , 722     ) |        7.0 | 
                    (722     , 963     ) |        2.0 | 
                    (963     , 1.2e+03 ) |        4.0 | 
                    (1.2e+03 , 1.44e+03) |        0.0 | 
                    (1.44e+03, 1.69e+03) |        0.0 | 
                    (1.69e+03, 1.93e+03) |        1.0 | 
                    (1.93e+03, 2.17e+03) |        0.0 | 
                    (2.17e+03, 2.41e+03) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 96])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 0.1683,  0.7307,  1.4870,  ..., -1.3125,  5.5169, -2.7646])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([-0.0296,  0.5031,  1.3804,  ..., -2.6687,  6.3413, -5.3056])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.1925, std-dev=2.0345, var=4.139, median=0.18268, min=-8.1297 at (0, 118, 191), max=7.9326 at (0, 60, 190), avg-magnitude=1.6522, p90=2.8262, p95=3.6648, p99=4.3738
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-8.13  , -6.52  ) |       18.0 | 
                (-6.52  , -4.92  ) |      162.0 | 
                (-4.92  , -3.31  ) |     2321.0 | #####
                (-3.31  , -1.7   ) |     8243.0 | ###################
                (-1.7   , -0.0986) |    14792.0 | ###################################
                (-0.0986, 1.51   ) |    16737.0 | ########################################
                (1.51   , 3.11   ) |    10622.0 | #########################
                (3.11   , 4.72   ) |     4309.0 | ##########
                (4.72   , 6.33   ) |      272.0 | 
                (6.33   , 7.93   ) |      124.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.15685, std-dev=2.0738, var=4.3007, median=0.14499, min=-7.8389 at (0, 123, 96), max=7.7039 at (0, 215, 190), avg-magnitude=1.6873, p90=2.8771, p95=3.6862, p99=4.361
[I]             ---- Histogram ----
                Bin Range          |  Num Elems | Visualization
                (-8.13  , -6.52  ) |       25.0 | 
                (-6.52  , -4.92  ) |      190.0 | 
                (-4.92  , -3.31  ) |     2528.0 | ######
                (-3.31  , -1.7   ) |     8706.0 | #####################
                (-1.7   , -0.0986) |    14590.0 | ###################################
                (-0.0986, 1.51   ) |    16308.0 | ########################################
                (1.51   , 3.11   ) |    10312.0 | #########################
                (3.11   , 4.72   ) |     4556.0 | ###########
                (4.72   , 6.33   ) |      259.0 | 
                (6.33   , 7.93   ) |      126.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=7.7626] OR [rel=1.1769e+05] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.62783, std-dev=0.68223, var=0.46544, median=0.40287, min=2.3842e-07 at (0, 5, 149), max=7.7626 at (0, 123, 24), avg-magnitude=0.62783, p90=1.4997, p95=1.9867, p99=3.2191
[I]                 ---- Histogram ----
                    Bin Range         |  Num Elems | Visualization
                    (2.38e-07, 0.776) |    41598.0 | ########################################
                    (0.776   , 1.55 ) |    10659.0 | ##########
                    (1.55    , 2.33 ) |     3557.0 | ###
                    (2.33    , 3.11 ) |     1128.0 | #
                    (3.11    , 3.88 ) |      413.0 | 
                    (3.88    , 4.66 ) |      158.0 | 
                    (4.66    , 5.43 ) |       56.0 | 
                    (5.43    , 6.21 ) |       23.0 | 
                    (6.21    , 6.99 ) |        4.0 | 
                    (6.99    , 7.76 ) |        4.0 | 
[I]             Relative Difference | Stats: mean=4.5228, std-dev=493.41, var=2.4345e+05, median=0.32636, min=6.1956e-08 at (0, 15, 51), max=1.1769e+05 at (0, 120, 104), avg-magnitude=4.5228, p90=2.1291, p95=4.2775, p99=22.582
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (6.2e-08 , 1.18e+04) |    57599.0 | ########################################
                    (1.18e+04, 2.35e+04) |        0.0 | 
                    (2.35e+04, 3.53e+04) |        0.0 | 
                    (3.53e+04, 4.71e+04) |        0.0 | 
                    (4.71e+04, 5.88e+04) |        0.0 | 
                    (5.88e+04, 7.06e+04) |        0.0 | 
                    (7.06e+04, 8.24e+04) |        0.0 | 
                    (8.24e+04, 9.42e+04) |        0.0 | 
                    (9.42e+04, 1.06e+05) |        0.0 | 
                    (1.06e+05, 1.18e+05) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([ 0.5720,  1.4100,  0.9941,  ..., -1.3169,  1.6684, -1.7301])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 0.6574,  1.0883,  0.8281,  ..., -0.7307,  3.4448, -1.8215])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.077656, std-dev=2.5473, var=6.4888, median=0.15283, min=-35.302 at (0, 67, 183), max=9.1329 at (0, 46, 47), avg-magnitude=1.826, p90=3.0505, p95=3.6218, p99=4.8338
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-35.3, -30.8) |        8.0 | 
                (-30.8, -26.2) |       26.0 | 
                (-26.2, -21.7) |       40.0 | 
                (-21.7, -17.1) |       79.0 | 
                (-17.1, -12.6) |       88.0 | 
                (-12.6, -8.04) |       30.0 | 
                (-8.04, -3.49) |     2608.0 | ##
                (-3.49, 1.05 ) |    35095.0 | ########################################
                (1.05 , 5.59 ) |    19224.0 | #####################
                (5.59 , 10.1 ) |      402.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.062898, std-dev=2.5249, var=6.3749, median=0.12922, min=-33.608 at (0, 49, 183), max=10.137 at (0, 272, 47), avg-magnitude=1.8338, p90=3.0449, p95=3.621, p99=4.9222
[I]             ---- Histogram ----
                Bin Range      |  Num Elems | Visualization
                (-35.3, -30.8) |        7.0 | 
                (-30.8, -26.2) |       19.0 | 
                (-26.2, -21.7) |       43.0 | 
                (-21.7, -17.1) |       70.0 | 
                (-17.1, -12.6) |       81.0 | 
                (-12.6, -8.04) |       49.0 | 
                (-8.04, -3.49) |     2568.0 | ##
                (-3.49, 1.05 ) |    35205.0 | ########################################
                (1.05 , 5.59 ) |    19152.0 | #####################
                (5.59 , 10.1 ) |      406.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=26.6] OR [rel=1.9526e+05] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.78773, std-dev=0.97639, var=0.95334, median=0.54182, min=5.4002e-05 at (0, 0, 178), max=26.6 at (0, 91, 183), avg-magnitude=0.78773, p90=1.7296, p95=2.2378, p99=3.5586
[I]                 ---- Histogram ----
                    Bin Range       |  Num Elems | Visualization
                    (5.4e-05, 2.66) |    55974.0 | ########################################
                    (2.66   , 5.32) |     1414.0 | #
                    (5.32   , 7.98) |       92.0 | 
                    (7.98   , 10.6) |       37.0 | 
                    (10.6   , 13.3) |       32.0 | 
                    (13.3   , 16  ) |       20.0 | 
                    (16     , 18.6) |       12.0 | 
                    (18.6   , 21.3) |        8.0 | 
                    (21.3   , 23.9) |        4.0 | 
                    (23.9   , 26.6) |        7.0 | 
[I]             Relative Difference | Stats: mean=7.5528, std-dev=854.09, var=7.2946e+05, median=0.40182, min=3.3131e-05 at (0, 12, 36), max=1.9526e+05 at (0, 149, 114), avg-magnitude=7.5528, p90=2.7934, p95=5.61, p99=27.564
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (3.31e-05, 1.95e+04) |    57598.0 | ########################################
                    (1.95e+04, 3.91e+04) |        0.0 | 
                    (3.91e+04, 5.86e+04) |        0.0 | 
                    (5.86e+04, 7.81e+04) |        1.0 | 
                    (7.81e+04, 9.76e+04) |        0.0 | 
                    (9.76e+04, 1.17e+05) |        0.0 | 
                    (1.17e+05, 1.37e+05) |        0.0 | 
                    (1.37e+05, 1.56e+05) |        0.0 | 
                    (1.56e+05, 1.76e+05) |        0.0 | 
                    (1.76e+05, 1.95e+05) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) with '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([-2.4460e-01,  5.9259e-02, -1.1141e-03,  ..., -1.8162e+00,
                         4.1330e+00, -4.0583e+00])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([-0.4735,  0.0336, -0.1407,  ..., -2.2724,  3.3272, -3.5136])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.15435, std-dev=2.2185, var=4.9217, median=0.23945, min=-9.3616 at (0, 258, 183), max=8.4497 at (0, 45, 104), avg-magnitude=1.7862, p90=3.0584, p95=3.8427, p99=4.5012
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.36 , -7.58 ) |       95.0 | 
                (-7.58 , -5.8  ) |      178.0 | 
                (-5.8  , -4.02 ) |      700.0 | #
                (-4.02 , -2.24 ) |     7964.0 | ################
                (-2.24 , -0.456) |    12551.0 | ##########################
                (-0.456, 1.33  ) |    18766.0 | ########################################
                (1.33  , 3.11  ) |    11826.0 | #########################
                (3.11  , 4.89  ) |     5194.0 | ###########
                (4.89  , 6.67  ) |      309.0 | 
                (6.67  , 8.45  ) |       17.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 | Stats: mean=0.11493, std-dev=2.2279, var=4.9633, median=0.1752, min=-9.3242 at (0, 208, 183), max=7.8344 at (0, 100, 104), avg-magnitude=1.7968, p90=3.0641, p95=3.8392, p99=4.463
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (-9.36 , -7.58 ) |      104.0 | 
                (-7.58 , -5.8  ) |      150.0 | 
                (-5.8  , -4.02 ) |      703.0 | #
                (-4.02 , -2.24 ) |     8295.0 | ##################
                (-2.24 , -0.456) |    12926.0 | ############################
                (-0.456, 1.33  ) |    18268.0 | ########################################
                (1.33  , 3.11  ) |    11576.0 | #########################
                (3.11  , 4.89  ) |     5266.0 | ###########
                (4.89  , 6.67  ) |      295.0 | 
                (6.67  , 8.45  ) |       17.0 | 
[I]         Error Metrics: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0
[I]             Minimum Required Tolerance: elemwise error | [abs=9.5232] OR [rel=62685] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.77918, std-dev=0.73952, var=0.5469, median=0.56411, min=0 at (0, 212, 63), max=9.5232 at (0, 137, 184), avg-magnitude=0.77918, p90=1.7818, p95=2.2654, p99=3.31
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (0    , 0.952) |    39845.0 | ########################################
                    (0.952, 1.9  ) |    12889.0 | ############
                    (1.9  , 2.86 ) |     3697.0 | ###
                    (2.86 , 3.81 ) |      944.0 | 
                    (3.81 , 4.76 ) |      186.0 | 
                    (4.76 , 5.71 ) |       28.0 | 
                    (5.71 , 6.67 ) |        6.0 | 
                    (6.67 , 7.62 ) |        2.0 | 
                    (7.62 , 8.57 ) |        1.0 | 
                    (8.57 , 9.52 ) |        2.0 | 
[I]             Relative Difference | Stats: mean=5.0311, std-dev=321.37, var=1.0328e+05, median=0.42494, min=0 at (0, 212, 63), max=62685 at (0, 195, 103), avg-magnitude=5.0311, p90=3.173, p95=6.3615, p99=33.115
[I]                 ---- Histogram ----
                    Bin Range            |  Num Elems | Visualization
                    (0       , 6.27e+03) |    57598.0 | ########################################
                    (6.27e+03, 1.25e+04) |        0.0 | 
                    (1.25e+04, 1.88e+04) |        0.0 | 
                    (1.88e+04, 2.51e+04) |        0.0 | 
                    (2.51e+04, 3.13e+04) |        0.0 | 
                    (3.13e+04, 3.76e+04) |        0.0 | 
                    (3.76e+04, 4.39e+04) |        1.0 | 
                    (4.39e+04, 5.01e+04) |        0.0 | 
                    (5.01e+04, 5.64e+04) |        0.0 | 
                    (5.64e+04, 6.27e+04) |        1.0 | 
[38;5;104m[X]         Finished comparing: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [trt-runner-N2-05/19/25-15:43:09] and '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' (dtype=float32, shape=torch.Size([1, 300, 192])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: 'labels' (dtype=int64, shape=torch.Size([1, 300])) with 'labels' (dtype=int64, shape=torch.Size([1, 300]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([67, 26,  8,  0,  8,  8,  8,  0,  0,  8,  0, 67, 24,  8, 24, 67,  0,  0,
                        67,  0, 24,  8,  8, 28,  0,  8, 24,  8, 24,  0,  0, 67, 26,  8,  8,  0,
                         0,  8,  0, 67,  8,  8,  0, 67, 24,  0, 26, 28,  8,  0, 67, 24,  8,  0,
                         0, 24,  8,  8,  2, 67, 67,  8,  0, 13,  8,  8, 26, 24,  8, 26, 28, 67,
                         8,  8,  8,  8, 24,  0, 26, 26,  2, 67,  8,  0, 26, 24, 26, 76, 56, 24,
                        13,  8,  0, 26, 28, 26, 67,  0, 79, 28,  8,  0,  0, 28, 67,  8,  8, 24,
                         8,  8, 26, 26, 58, 13, 58, 24, 67, 24, 26, 13,  0, 24,  8,  8,  0,  8,
                         8, 24, 24,  0, 24, 76, 26, 26, 28, 13, 24,  0,  0,  8,  0, 26, 13, 26,
                        67,  8,  8, 13,  0,  2,  8,  0,  0,  8, 28, 28, 28, 26,  0,  8,  0,  0,
                        39, 58, 26, 26, 13,  8,  8, 58, 24,  0, 67,  0,  0,  8,  2, 26, 13,  8,
                         8, 26,  8,  8,  8,  2, 24,  0, 26,  0,  8,  8, 28,  1, 28, 13, 26, 28,
                        24,  0, 25, 26, 26, 56,  0, 79,  1, 67,  1, 67, 28, 67, 79, 24, 67,  8,
                        59, 28,  3, 74, 27, 74,  0, 24, 43,  9, 43, 10, 73,  3,  8, 79,  0,  0,
                        24, 56, 13, 28, 73, 79, 24, 13, 39, 13, 13, 26, 26,  8, 28])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([ 8,  8, 26, 67,  0,  0, 28,  8,  8,  0,  8,  0,  0, 67,  8,  8,  8,  8,
                         0, 24,  8, 67, 24, 13,  8,  0,  8, 67,  8,  8, 67, 26,  8,  0,  0, 26,
                         8, 67,  8, 26,  0,  0,  8,  8, 13,  8,  8,  8, 28, 13,  8,  0, 28, 24,
                         8,  0, 26, 28, 26,  8, 24,  0, 26, 24,  0,  0, 67,  0, 67,  8, 24,  0,
                        26,  0, 67, 79, 58,  8, 24,  0,  0,  8, 28,  8,  8, 56,  0, 58, 28, 67,
                        67,  0, 76,  0, 26, 24, 73,  8, 60, 24, 28,  8, 13,  0,  0, 24,  0,  8,
                        28, 24, 11,  8, 43,  2, 26,  0, 26,  8, 28,  8,  8,  0, 24, 28,  2, 24,
                        26, 26,  0, 26, 28, 39, 13, 56, 24,  8,  0, 26, 13, 67, 24, 24, 24,  8,
                        28,  0, 28,  2, 24, 24, 13,  8, 56, 67, 26, 24,  0,  0, 76, 24,  9,  8,
                         8, 34,  0, 58, 67, 28, 13, 24, 13,  8, 24, 56, 79, 56, 12, 25, 28, 43,
                        26,  3,  0, 27, 58, 74,  3,  8,  8,  2, 26, 58,  0,  8, 74, 60,  9, 24,
                         0, 26, 24, 24,  0, 11,  8,  0, 13,  0,  0, 79,  8, 28,  0,  1,  2, 59,
                        58, 24, 28,  0,  9, 26, 24,  8, 26, 79,  8, 26, 26, 24, 24,  8, 13, 67,
                         8, 26,  0, 13, 26, 24, 58,  8, 67,  8, 11, 25,  2,  0,  2])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: labels | Stats: mean=20.71, std-dev=22.017, var=484.76, median=13, min=0 at (0, 0), max=79 at (0, 122), avg-magnitude=20.71, p90=67, p95=67, p99=79
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         74 | #################################
                (7 , 15) |         89 | ########################################
                (15, 23) |          0 | 
                (23, 31) |         89 | ########################################
                (31, 39) |          2 | 
                (39, 47) |          2 | 
                (47, 55) |          0 | 
                (55, 63) |          9 | ####
                (63, 71) |         24 | ##########
                (71, 79) |         11 | ####
[I]         onnxrt-runner-N2-05/19/25-15:43:09: labels | Stats: mean=20.497, std-dev=21.41, var=458.4, median=12, min=0 at (0, 0), max=79 at (0, 97), avg-magnitude=20.497, p90=59, p95=67, p99=79
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         73 | ################################
                (7 , 15) |         89 | #######################################
                (15, 23) |          0 | 
                (23, 31) |         90 | ########################################
                (31, 39) |          2 | 
                (39, 47) |          2 | 
                (47, 55) |          0 | 
                (55, 63) |         16 | #######
                (63, 71) |         19 | ########
                (71, 79) |          9 | ####
[I]         Error Metrics: labels
[I]             Minimum Required Tolerance: elemwise error | [abs=79] OR [rel=nan] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=20.733, std-dev=20.724, var=429.5, median=16, min=0 at (0, 0), max=79 at (0, 209), avg-magnitude=20.733, p90=59, p95=67, p99=76
[I]                 ---- Histogram ----
                    Bin Range|  Num Elems | Visualization
                    (0 , 7 ) |         83 | ########################################
                    (7 , 15) |         60 | ############################
                    (15, 23) |         53 | #########################
                    (23, 31) |         38 | ##################
                    (31, 39) |         11 | #####
                    (39, 47) |         12 | #####
                    (47, 55) |          8 | ###
                    (55, 63) |         16 | #######
                    (63, 71) |         12 | #####
                    (71, 79) |          7 | ###
[I]             Relative Difference | Stats: mean=nan, std-dev=nan, var=nan, median=nan, min=nan at (0, 0), max=nan at (0, 0), avg-magnitude=nan, p90=nan, p95=nan, p99=nan
[38;5;13m[V]                 Could not generate histogram. Note: Error was: torch.histogramdd: dimension 0's range [-nan, -nan] is not finite[0m
[I]                 
[38;5;104m[X]         Finished comparing: 'labels' (dtype=int64, shape=torch.Size([1, 300])) [trt-runner-N2-05/19/25-15:43:09] and 'labels' (dtype=int64, shape=torch.Size([1, 300])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: 'labels' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) with 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([262.9834,  66.3367, 549.6310,  ..., 226.3198, 223.4867, 258.2858])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([262.9576,  71.5327, 547.7870,  ..., 398.3175, 518.1219, 478.9788])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: boxes | Stats: mean=265.49, std-dev=137.9, var=19016, median=226.12, min=-1.1929 at (0, 8, 0), max=640.07 at (0, 200, 2), avg-magnitude=265.5, p90=479.61, p95=585.29, p99=610.27
[I]             ---- Histogram ----
                Bin Range     |  Num Elems | Visualization
                (-1.37, 63.6) |       59.0 | ######
                (63.6 , 129 ) |       62.0 | ######
                (129  , 194 ) |      245.0 | #########################
                (194  , 259 ) |      379.0 | ########################################
                (259  , 324 ) |      132.0 | #############
                (324  , 389 ) |      107.0 | ###########
                (389  , 454 ) |       63.0 | ######
                (454  , 519 ) |       67.0 | #######
                (519  , 584 ) |       23.0 | ##
                (584  , 649 ) |       63.0 | ######
[I]         onnxrt-runner-N2-05/19/25-15:43:09: boxes | Stats: mean=260.78, std-dev=140.6, var=19769, median=224.33, min=-1.3738 at (0, 105, 0), max=648.52 at (0, 276, 2), avg-magnitude=260.79, p90=479.46, p95=594.98, p99=622.91
[I]             ---- Histogram ----
                Bin Range     |  Num Elems | Visualization
                (-1.37, 63.6) |       63.0 | ######
                (63.6 , 129 ) |       82.0 | ########
                (129  , 194 ) |      242.0 | ##########################
                (194  , 259 ) |      372.0 | ########################################
                (259  , 324 ) |      133.0 | ##############
                (324  , 389 ) |      121.0 | #############
                (389  , 454 ) |       39.0 | ####
                (454  , 519 ) |       53.0 | #####
                (519  , 584 ) |       25.0 | ##
                (584  , 649 ) |       70.0 | #######
[I]         Error Metrics: boxes
[I]             Minimum Required Tolerance: elemwise error | [abs=609.9] OR [rel=460.72] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=137.18, std-dev=135.31, var=18308, median=83.684, min=0.025726 at (0, 0, 0), max=609.9 at (0, 7, 0), avg-magnitude=137.18, p90=334.33, p95=410.6, p99=549.54
[I]                 ---- Histogram ----
                    Bin Range     |  Num Elems | Visualization
                    (0.0257, 61 ) |      522.0 | ########################################
                    (61    , 122) |      179.0 | #############
                    (122   , 183) |      115.0 | ########
                    (183   , 244) |      124.0 | #########
                    (244   , 305) |      113.0 | ########
                    (305   , 366) |       54.0 | ####
                    (366   , 427) |       40.0 | ###
                    (427   , 488) |       20.0 | #
                    (488   , 549) |       19.0 | #
                    (549   , 610) |       14.0 | #
[I]             Relative Difference | Stats: mean=3.8402, std-dev=30.748, var=945.46, median=0.36742, min=8.4262e-05 at (0, 152, 3), max=460.72 at (0, 127, 0), avg-magnitude=3.8402, p90=1.6486, p95=5.0111, p99=40.753
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (8.43e-05, 46.1) |     1188.0 | ########################################
                    (46.1    , 92.1) |        1.0 | 
                    (92.1    , 138 ) |        0.0 | 
                    (138     , 184 ) |        2.0 | 
                    (184     , 230 ) |        0.0 | 
                    (230     , 276 ) |        2.0 | 
                    (276     , 323 ) |        2.0 | 
                    (323     , 369 ) |        1.0 | 
                    (369     , 415 ) |        3.0 | 
                    (415     , 461 ) |        1.0 | 
[38;5;104m[X]         Finished comparing: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [trt-runner-N2-05/19/25-15:43:09] and 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: 'boxes' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: 'scores' (dtype=float32, shape=torch.Size([1, 300])) with 'scores' (dtype=float32, shape=torch.Size([1, 300]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;104m[X]             trt-runner-N2-05/19/25-15:43:09     | Mismatched values:
                tensor([0.5844, 0.4758, 0.4500, 0.4185, 0.4173, 0.4012, 0.3364, 0.3303, 0.3272,
                        0.3045, 0.2979, 0.2899, 0.2799, 0.2709, 0.2675, 0.2627, 0.2626, 0.2591,
                        0.2471, 0.2452, 0.2332, 0.2287, 0.2236, 0.2140, 0.2139, 0.2122, 0.2074,
                        0.2015, 0.1970, 0.1943, 0.1862, 0.1773, 0.1768, 0.1759, 0.1750, 0.1707,
                        0.1706, 0.1681, 0.1668, 0.1648, 0.1640, 0.1620, 0.1609, 0.1607, 0.1597,
                        0.1495, 0.1461, 0.1455, 0.1447, 0.1428, 0.1413, 0.1411, 0.1410, 0.1353,
                        0.1353, 0.1334, 0.1321, 0.1316, 0.1316, 0.1303, 0.1270, 0.1243, 0.1229,
                        0.1227, 0.1223, 0.1221, 0.1219, 0.1215, 0.1213, 0.1200, 0.1198, 0.1197,
                        0.1169, 0.1163, 0.1159, 0.1154, 0.1148, 0.1144, 0.1141, 0.1116, 0.1111,
                        0.1108, 0.1106, 0.1097, 0.1082, 0.1076, 0.1074, 0.1069, 0.1063, 0.1062,
                        0.1046, 0.1041, 0.1040, 0.1039, 0.1022, 0.1016, 0.1013, 0.1012, 0.1007,
                        0.1002, 0.0994, 0.0990, 0.0988, 0.0987, 0.0987, 0.0985, 0.0984, 0.0983,
                        0.0983, 0.0980, 0.0972, 0.0970, 0.0964, 0.0952, 0.0941, 0.0930, 0.0926,
                        0.0916, 0.0915, 0.0915, 0.0897, 0.0889, 0.0885, 0.0879, 0.0873, 0.0873,
                        0.0864, 0.0863, 0.0861, 0.0851, 0.0849, 0.0846, 0.0845, 0.0844, 0.0833,
                        0.0829, 0.0827, 0.0824, 0.0817, 0.0812, 0.0810, 0.0807, 0.0806, 0.0786,
                        0.0785, 0.0776, 0.0773, 0.0767, 0.0760, 0.0758, 0.0756, 0.0756, 0.0746,
                        0.0735, 0.0731, 0.0728, 0.0720, 0.0719, 0.0713, 0.0711, 0.0700, 0.0697,
                        0.0695, 0.0695, 0.0691, 0.0687, 0.0683, 0.0679, 0.0677, 0.0675, 0.0664,
                        0.0662, 0.0659, 0.0646, 0.0645, 0.0644, 0.0636, 0.0634, 0.0631, 0.0630,
                        0.0630, 0.0629, 0.0627, 0.0622, 0.0616, 0.0614, 0.0610, 0.0608, 0.0600,
                        0.0593, 0.0591, 0.0590, 0.0590, 0.0584, 0.0584, 0.0582, 0.0581, 0.0581,
                        0.0580, 0.0579, 0.0577, 0.0570, 0.0568, 0.0567, 0.0567, 0.0566, 0.0566,
                        0.0564, 0.0564, 0.0561, 0.0561, 0.0560, 0.0556, 0.0555, 0.0555, 0.0552,
                        0.0548, 0.0546, 0.0545, 0.0545, 0.0541, 0.0538, 0.0538, 0.0527, 0.0526,
                        0.0525, 0.0523, 0.0519, 0.0517, 0.0513, 0.0513, 0.0513, 0.0510, 0.0507,
                        0.0507, 0.0505, 0.0503, 0.0503, 0.0498, 0.0498, 0.0496, 0.0495, 0.0493,
                        0.0492, 0.0491, 0.0490, 0.0489, 0.0488, 0.0486, 0.0485, 0.0484, 0.0483,
                        0.0482, 0.0481, 0.0478, 0.0477, 0.0476, 0.0474, 0.0473, 0.0471, 0.0469,
                        0.0468, 0.0463, 0.0462, 0.0462, 0.0460, 0.0458, 0.0450, 0.0447, 0.0446,
                        0.0443, 0.0443, 0.0443, 0.0440, 0.0438, 0.0438, 0.0437, 0.0436, 0.0436,
                        0.0434, 0.0431, 0.0429, 0.0428, 0.0427, 0.0427, 0.0427, 0.0426, 0.0426,
                        0.0425, 0.0425, 0.0423, 0.0423, 0.0422, 0.0419, 0.0417, 0.0417, 0.0415,
                        0.0412, 0.0410, 0.0409])[0m
[38;5;104m[X]             onnxrt-runner-N2-05/19/25-15:43:09  | Mismatched values:
                tensor([0.6030, 0.5362, 0.5060, 0.4716, 0.4550, 0.4172, 0.4056, 0.3967, 0.3598,
                        0.3229, 0.2707, 0.2680, 0.2555, 0.2454, 0.2324, 0.2316, 0.2067, 0.2027,
                        0.2022, 0.2010, 0.1952, 0.1934, 0.1921, 0.1915, 0.1900, 0.1865, 0.1848,
                        0.1837, 0.1807, 0.1792, 0.1760, 0.1732, 0.1698, 0.1648, 0.1636, 0.1600,
                        0.1595, 0.1585, 0.1555, 0.1547, 0.1529, 0.1523, 0.1518, 0.1502, 0.1478,
                        0.1432, 0.1393, 0.1369, 0.1360, 0.1356, 0.1341, 0.1318, 0.1289, 0.1285,
                        0.1260, 0.1236, 0.1187, 0.1185, 0.1182, 0.1180, 0.1179, 0.1175, 0.1174,
                        0.1169, 0.1167, 0.1138, 0.1123, 0.1119, 0.1114, 0.1112, 0.1103, 0.1103,
                        0.1102, 0.1101, 0.1092, 0.1092, 0.1083, 0.1081, 0.1059, 0.1050, 0.1040,
                        0.1031, 0.1025, 0.1021, 0.1009, 0.0988, 0.0972, 0.0961, 0.0952, 0.0948,
                        0.0943, 0.0942, 0.0942, 0.0942, 0.0940, 0.0934, 0.0934, 0.0932, 0.0930,
                        0.0925, 0.0924, 0.0922, 0.0921, 0.0915, 0.0913, 0.0910, 0.0907, 0.0899,
                        0.0898, 0.0883, 0.0872, 0.0868, 0.0864, 0.0864, 0.0859, 0.0849, 0.0835,
                        0.0833, 0.0830, 0.0819, 0.0814, 0.0810, 0.0804, 0.0800, 0.0790, 0.0788,
                        0.0786, 0.0780, 0.0779, 0.0776, 0.0776, 0.0773, 0.0771, 0.0770, 0.0766,
                        0.0765, 0.0760, 0.0759, 0.0752, 0.0751, 0.0749, 0.0748, 0.0744, 0.0742,
                        0.0740, 0.0736, 0.0732, 0.0732, 0.0731, 0.0730, 0.0722, 0.0718, 0.0717,
                        0.0717, 0.0710, 0.0706, 0.0705, 0.0693, 0.0692, 0.0691, 0.0686, 0.0684,
                        0.0680, 0.0678, 0.0677, 0.0677, 0.0672, 0.0670, 0.0656, 0.0656, 0.0655,
                        0.0655, 0.0654, 0.0654, 0.0652, 0.0645, 0.0640, 0.0639, 0.0638, 0.0631,
                        0.0628, 0.0626, 0.0619, 0.0617, 0.0612, 0.0612, 0.0611, 0.0610, 0.0609,
                        0.0606, 0.0606, 0.0604, 0.0604, 0.0604, 0.0604, 0.0601, 0.0597, 0.0597,
                        0.0597, 0.0596, 0.0595, 0.0593, 0.0589, 0.0586, 0.0586, 0.0586, 0.0585,
                        0.0578, 0.0574, 0.0573, 0.0573, 0.0571, 0.0570, 0.0568, 0.0567, 0.0562,
                        0.0557, 0.0556, 0.0555, 0.0555, 0.0555, 0.0544, 0.0542, 0.0535, 0.0533,
                        0.0533, 0.0531, 0.0530, 0.0527, 0.0525, 0.0522, 0.0521, 0.0519, 0.0518,
                        0.0517, 0.0515, 0.0514, 0.0512, 0.0507, 0.0507, 0.0507, 0.0505, 0.0504,
                        0.0503, 0.0502, 0.0502, 0.0499, 0.0496, 0.0495, 0.0487, 0.0487, 0.0485,
                        0.0484, 0.0482, 0.0480, 0.0480, 0.0480, 0.0476, 0.0472, 0.0467, 0.0466,
                        0.0462, 0.0460, 0.0460, 0.0460, 0.0459, 0.0456, 0.0455, 0.0455, 0.0455,
                        0.0454, 0.0451, 0.0451, 0.0445, 0.0444, 0.0444, 0.0443, 0.0441, 0.0441,
                        0.0440, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437, 0.0436, 0.0435,
                        0.0432, 0.0430, 0.0429, 0.0426, 0.0426, 0.0425, 0.0423, 0.0419, 0.0419,
                        0.0419, 0.0417, 0.0417])[0m
[I]         trt-runner-N2-05/19/25-15:43:09: scores | Stats: mean=0.1017, std-dev=0.078513, var=0.0061642, median=0.07574, min=0.040893 at (0, 299), max=0.58439 at (0, 0), avg-magnitude=0.1017, p90=0.18697, p95=0.26297, p99=0.41883
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (0.0409, 0.0971) |      189.0 | ########################################
                (0.0971, 0.153 ) |       66.0 | #############
                (0.153 , 0.21  ) |       19.0 | ####
                (0.21  , 0.266 ) |       11.0 | ##
                (0.266 , 0.322 ) |        6.0 | #
                (0.322 , 0.378 ) |        3.0 | 
                (0.378 , 0.434 ) |        3.0 | 
                (0.434 , 0.491 ) |        2.0 | 
                (0.491 , 0.547 ) |        0.0 | 
                (0.547 , 0.603 ) |        1.0 | 
[I]         onnxrt-runner-N2-05/19/25-15:43:09: scores | Stats: mean=0.097993, std-dev=0.080413, var=0.0064662, median=0.072622, min=0.041714 at (0, 299), max=0.60297 at (0, 0), avg-magnitude=0.097993, p90=0.17633, p95=0.23168, p99=0.47193
[I]             ---- Histogram ----
                Bin Range        |  Num Elems | Visualization
                (0.0409, 0.0971) |      213.0 | ########################################
                (0.0971, 0.153 ) |       47.0 | ########
                (0.153 , 0.21  ) |       24.0 | ####
                (0.21  , 0.266 ) |        4.0 | 
                (0.266 , 0.322 ) |        2.0 | 
                (0.322 , 0.378 ) |        2.0 | 
                (0.378 , 0.434 ) |        3.0 | 
                (0.434 , 0.491 ) |        2.0 | 
                (0.491 , 0.547 ) |        2.0 | 
                (0.547 , 0.603 ) |        1.0 | 
[I]         Error Metrics: scores
[I]             Minimum Required Tolerance: elemwise error | [abs=0.069165] OR [rel=0.27847] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.0072539, std-dev=0.01109, var=0.00012299, median=0.0031735, min=5.5034e-05 at (0, 258), max=0.069165 at (0, 6), avg-magnitude=0.0072539, p90=0.013611, p95=0.031109, p99=0.056481
[I]                 ---- Histogram ----
                    Bin Range          |  Num Elems | Visualization
                    (5.5e-05, 0.00697) |      183.0 | ########################################
                    (0.00697, 0.0139 ) |       87.0 | ###################
                    (0.0139 , 0.0208 ) |        6.0 | #
                    (0.0208 , 0.0277 ) |        8.0 | #
                    (0.0277 , 0.0346 ) |        3.0 | 
                    (0.0346 , 0.0415 ) |        4.0 | 
                    (0.0415 , 0.0484 ) |        2.0 | 
                    (0.0484 , 0.0553 ) |        1.0 | 
                    (0.0553 , 0.0623 ) |        4.0 | 
                    (0.0623 , 0.0692 ) |        2.0 | 
[I]             Relative Difference | Stats: mean=0.054368, std-dev=0.046179, var=0.0021325, median=0.039071, min=0.001042 at (0, 186), max=0.27847 at (0, 17), avg-magnitude=0.054368, p90=0.10837, p95=0.1179, p99=0.21984
[I]                 ---- Histogram ----
                    Bin Range         |  Num Elems | Visualization
                    (0.00104, 0.0288) |      132.0 | ########################################
                    (0.0288 , 0.0565) |       36.0 | ##########
                    (0.0565 , 0.0843) |       53.0 | ################
                    (0.0843 , 0.112 ) |       53.0 | ################
                    (0.112  , 0.14  ) |       16.0 | ####
                    (0.14   , 0.167 ) |        3.0 | 
                    (0.167  , 0.195 ) |        3.0 | 
                    (0.195  , 0.223 ) |        2.0 | 
                    (0.223  , 0.251 ) |        0.0 | 
                    (0.251  , 0.278 ) |        2.0 | 
[38;5;104m[X]         Finished comparing: 'scores' (dtype=float32, shape=torch.Size([1, 300])) [trt-runner-N2-05/19/25-15:43:09] and 'scores' (dtype=float32, shape=torch.Size([1, 300])) [onnxrt-runner-N2-05/19/25-15:43:09][0m
[38;5;9m[E]         FAILED | Output: 'scores' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;9m[E]     FAILED | Mismatched outputs: ['/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0', '/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0', '/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0', '/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0', '/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0', '/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0', '/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0', '/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0', '/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0', '/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0', '/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0', '/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0', 'labels', 'boxes', 'scores'][0m
[38;5;104m[X]     Finished comparing trt-runner-N2-05/19/25-15:43:09 with onnxrt-runner-N2-05/19/25-15:43:09[0m
[38;5;9m[E] Accuracy Summary | trt-runner-N2-05/19/25-15:43:09 vs. onnxrt-runner-N2-05/19/25-15:43:09 | Passed: 0/1 iterations | Pass Rate: 0.0%[0m
